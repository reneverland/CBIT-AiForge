"""
æ–‡æ¡£ç®¡ç† API
"""

from fastapi import APIRouter, UploadFile, File, Form, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List, Optional
from pathlib import Path
import shutil
from datetime import datetime
from loguru import logger

from app.models.database import (
    get_db, 
    Document as DocumentModel,
    EmbeddingProvider,
    VectorDBProvider,
    KnowledgeBase as KnowledgeBaseModel
)
from app.core.document_processor import DocumentProcessor
from app.core.config import settings

router = APIRouter()


@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    knowledge_base_id: Optional[int] = Form(None),
    auto_process: bool = Form(True),
    use_qa_format: bool = Form(False),
    chunk_mode: str = Form("auto"),  # æ–°å¢ï¼šåˆ†å—æ¨¡å¼ (auto, faq, standard)
    db: Session = Depends(get_db)
):
    """
    ä¸Šä¼ æ–‡æ¡£
    
    - æ”¯æŒæ ¼å¼: PDF, DOCX, XLSX, TXT, MD
    - è‡ªåŠ¨è§£æå’Œæ¸…æ´—
    - å¦‚æœæŒ‡å®šknowledge_base_idï¼Œè‡ªåŠ¨æ·»åŠ åˆ°çŸ¥è¯†åº“å¹¶å‘é‡åŒ–
    """
    try:
        logger.info(f"æ”¶åˆ°æ–‡æ¡£ä¸Šä¼ è¯·æ±‚: æ–‡ä»¶={file.filename}, knowledge_base_id={knowledge_base_id}, auto_process={auto_process}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > settings.MAX_UPLOAD_SIZE_MB * 1024 * 1024:
            raise HTTPException(
                status_code=400,
                detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({settings.MAX_UPLOAD_SIZE_MB}MB)"
            )
        
        # ä¿å­˜æ–‡ä»¶
        file_path = settings.UPLOAD_DIR / f"{datetime.now().timestamp()}_{file.filename}"
        with open(file_path, "wb") as f:
            f.write(content)
        
        # è·å–æ–‡ä»¶ç±»å‹
        file_type = file_path.suffix.lower()
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        doc = DocumentModel(
            filename=file.filename,
            original_path=str(file_path),
            file_type=file_type,
            file_size=file_size,
            knowledge_base_id=knowledge_base_id,
            status="uploaded"
        )
        db.add(doc)
        db.commit()
        db.refresh(doc)
        
        logger.info(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {file.filename}")
        
        result = {
            "id": doc.id,
            "filename": doc.filename,
            "file_type": doc.file_type,
            "file_size": doc.file_size,
            "status": doc.status,
            "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸ"
        }
        
        # å¦‚æœæŒ‡å®šäº†çŸ¥è¯†åº“ä¸”auto_process=Trueï¼Œè‡ªåŠ¨å¤„ç†å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“
        if knowledge_base_id and auto_process:
            logger.info(f"å¼€å§‹è‡ªåŠ¨å¤„ç†: knowledge_base_id={knowledge_base_id}, auto_process={auto_process}")
            try:
                from app.core.rag_engine import RAGEngine
                
                # è·å–çŸ¥è¯†åº“
                kb = db.query(KnowledgeBaseModel).filter(
                    KnowledgeBaseModel.id == knowledge_base_id
                ).first()
                
                if kb:
                    logger.info(f"æ‰¾åˆ°çŸ¥è¯†åº“: {kb.name}, collection_name={kb.collection_name}, vector_db_provider_id={kb.vector_db_provider_id}")
                    # å¤„ç†æ–‡æ¡£
                    doc.status = "processing"
                    db.commit()
                    logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}, åˆ†å—æ¨¡å¼: {chunk_mode}")
                    
                    processed_result = DocumentProcessor.process_document(Path(file_path), chunk_mode=chunk_mode)
                    chunks = processed_result["chunks"]
                    actual_mode = processed_result["metadata"].get("chunk_mode", chunk_mode)
                    logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼ˆæ¨¡å¼: {actual_mode}ï¼‰")
                    
                    # è·å–é»˜è®¤ Embedding æä¾›å•†é…ç½®ï¼ˆç”¨äºå‘é‡åŒ–å’ŒQAç”Ÿæˆï¼‰
                    embedding_config = None
                    try:
                        provider = db.query(EmbeddingProvider).filter(
                            EmbeddingProvider.is_default == True
                        ).first()
                        if provider:
                            embedding_config = {
                                "name": provider.name,
                                "provider_type": provider.provider_type,
                                "model_name": provider.model_name,
                                "api_key": provider.api_key,
                                "base_url": provider.base_url,
                                "dimension": provider.dimension
                            }
                            logger.info(f"ä½¿ç”¨é»˜è®¤ Embedding æä¾›å•†: {provider.name} ({provider.provider_type})")
                        else:
                            logger.warning("æœªæ‰¾åˆ°é»˜è®¤ Embedding æä¾›å•†ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                    except Exception as e:
                        logger.warning(f"è·å–é»˜è®¤ Embedding æä¾›å•†å¤±è´¥: {e}")
                    
                    # å¦‚æœå¯ç”¨QAæ ¼å¼ï¼Œè½¬æ¢æ–‡æœ¬å—ä¸ºé—®ç­”å¯¹
                    if use_qa_format:
                        try:
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯FAQæ ¼å¼ï¼ˆé€šè¿‡actual_modeåˆ¤æ–­ï¼‰
                            if actual_mode == "faq":
                                logger.info(f"ğŸ“‹ æ–‡æ¡£å·²æ˜¯FAQæ ¼å¼ï¼ŒåŒ…å« {len(chunks)} ä¸ªé—®ç­”å¯¹ï¼Œç›´æ¥ä½¿ç”¨æ— éœ€AIè½¬æ¢")
                                # FAQæ ¼å¼çš„chunkå·²ç»æ˜¯Q&Aæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€OpenAIè½¬æ¢
                            else:
                                # æ™®é€šæ–‡æ¡£ï¼Œéœ€è¦ç”¨OpenAIç”ŸæˆQAå¯¹
                                from app.core.qa_generator import QAGenerator
                                
                                # ä½¿ç”¨ Embedding é…ç½®ä¸­çš„ OpenAI API è¿›è¡Œ QA è½¬æ¢
                                qa_api_key = None
                                qa_base_url = None
                                qa_model = "gpt-3.5-turbo"
                                
                                if embedding_config and embedding_config.get('api_key'):
                                    qa_api_key = embedding_config['api_key']
                                    qa_base_url = embedding_config.get('base_url', 'https://api.openai.com/v1')
                                    logger.info(f"ğŸ¤– ä½¿ç”¨ OpenAI API è¿›è¡Œ QA æ ¼å¼è½¬æ¢")
                                else:
                                    logger.warning("âš ï¸ æœªé…ç½® OpenAI APIï¼Œæ— æ³•è¿›è¡Œ QA è½¬æ¢")
                                
                                qa_gen = QAGenerator(
                                    api_key=qa_api_key,
                                    base_url=qa_base_url,
                                    model=qa_model
                                )
                                
                                # ä¸ºæ¯ä¸ªchunkç”Ÿæˆé—®ç­”å¯¹
                                qa_chunks = []
                                # ç§»é™¤20ä¸ªé™åˆ¶ï¼Œä½†å¯¹è¶…å¤§æ–‡æ¡£åšé€‚å½“æ§åˆ¶
                                total_chunks = len(chunks)
                                if total_chunks > 100:
                                    logger.warning(f"âš ï¸ æ–‡æ¡£åŒ…å« {total_chunks} ä¸ªæ–‡æœ¬å—ï¼Œå°†å¤„ç†å‰100ä¸ª")
                                    total_chunks = 100
                                logger.info(f"ğŸ“š å¼€å§‹ä¸º {total_chunks} ä¸ªæ–‡æœ¬å—ç”Ÿæˆé—®ç­”å¯¹...")
                                
                                for i, chunk in enumerate(chunks[:total_chunks]):
                                    chunk_words = len(chunk.split())
                                    # æ ¹æ®chunkå¤§å°åŠ¨æ€è°ƒæ•´é—®é¢˜æ•°é‡
                                    num_questions = min(8, max(3, chunk_words // 100))  # æ¯100è¯ç”Ÿæˆ1ä¸ªé—®é¢˜ï¼Œæœ€å°‘3ä¸ªï¼Œæœ€å¤š8ä¸ª
                                    
                                    logger.info(f"  [{i+1}/{total_chunks}] å¤„ç†æ–‡æœ¬å—ï¼ˆ{chunk_words}è¯ï¼‰â†’ ç”Ÿæˆ{num_questions}ä¸ªé—®ç­”å¯¹...")
                                    qa_pairs = await qa_gen.generate_qa_pairs(
                                        chunk,
                                        num_questions=num_questions,
                                        domain="æ•™è‚²æ‹›ç”Ÿ"
                                    )
                                    
                                    # å°†é—®ç­”å¯¹è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼å­˜å‚¨
                                    if qa_pairs:
                                        for qa in qa_pairs:
                                            qa_text = f"Q: {qa['question']}\nA: {qa['answer']}"
                                            qa_chunks.append(qa_text)
                                        logger.info(f"  âœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                                    else:
                                        logger.warning(f"  âš ï¸ ç¬¬ {i+1} ä¸ªæ–‡æœ¬å—æœªç”Ÿæˆé—®ç­”å¯¹")
                                
                                if qa_chunks:
                                    original_count = len(chunks)
                                    chunks = qa_chunks
                                    logger.info(f"âœ… QAæ ¼å¼è½¬æ¢å®Œæˆï¼š{original_count} ä¸ªæ–‡æœ¬å— â†’ {len(chunks)} ä¸ªé—®ç­”å¯¹")
                                else:
                                    logger.warning("âš ï¸ QAæ ¼å¼è½¬æ¢æœªç”Ÿæˆä»»ä½•é—®ç­”å¯¹ï¼Œä½¿ç”¨åŸå§‹chunks")
                        except Exception as e:
                            logger.error(f"âŒ QAæ ¼å¼è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹chunks: {e}", exc_info=True)
                    
                    # è·å–å‘é‡æ•°æ®åº“é…ç½®
                    vector_db_config = None
                    try:
                        if kb.vector_db_provider_id:
                            vdb_provider = db.query(VectorDBProvider).filter(
                                VectorDBProvider.id == kb.vector_db_provider_id
                            ).first()
                            if vdb_provider:
                                vector_db_config = {
                                    "name": vdb_provider.name,
                                    "provider_type": vdb_provider.provider_type,
                                    "host": vdb_provider.host,
                                    "port": vdb_provider.port,
                                    "api_key": vdb_provider.api_key
                                }
                                logger.info(f"ä½¿ç”¨å‘é‡æ•°æ®åº“: {vdb_provider.name} ({vdb_provider.provider_type})")
                    except Exception as e:
                        logger.warning(f"è·å–å‘é‡æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
                    
                    # å‘é‡åŒ–å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“
                    logger.info(f"å¼€å§‹å‘é‡åŒ–å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“...")
                    rag_engine = RAGEngine(embedding_config, vector_db_config)
                    metadatas = [
                        {
                            "document_id": doc.id,
                            "filename": doc.filename,
                            "chunk_index": i
                        }
                        for i in range(len(chunks))
                    ]
                    
                    await rag_engine.add_documents(
                        collection_name=kb.collection_name,
                        documents=chunks,
                        metadatas=metadatas
                    )
                    
                    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
                    doc.status = "completed"
                    doc.chunk_count = len(chunks)
                    doc.doc_metadata = processed_result["metadata"]
                    doc.processed_at = datetime.utcnow()
                    db.commit()
                    logger.info(f"âœ… æ–‡æ¡£çŠ¶æ€å·²æ›´æ–°: {file.filename} -> completed ({len(chunks)} chunks)")
                    
                    # é‡æ–°æŸ¥è¯¢çŸ¥è¯†åº“å¹¶æ›´æ–°æ–‡æ¡£è®¡æ•°
                    kb_refresh = db.query(KnowledgeBaseModel).filter(
                        KnowledgeBaseModel.id == knowledge_base_id
                    ).first()
                    if kb_refresh:
                        kb_refresh.document_count = db.query(DocumentModel).filter(
                            DocumentModel.knowledge_base_id == knowledge_base_id,
                            DocumentModel.status == "completed"
                        ).count()
                        db.commit()
                        logger.info(f"âœ… çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡å·²æ›´æ–°: {kb_refresh.name} -> {kb_refresh.document_count} ä¸ªæ–‡æ¡£")
                    
                    result["status"] = "completed"
                    result["chunk_count"] = len(chunks)
                    result["chunk_mode"] = actual_mode
                    result["qa_format_used"] = use_qa_format
                    
                    mode_desc = {
                        "faq": "ğŸ¯ FAQé—®ç­”æ ¼å¼",
                        "standard": "ğŸ“„ æ ‡å‡†åˆ†å—",
                        "auto": "ğŸ¤– è‡ªåŠ¨è¯†åˆ«"
                    }.get(actual_mode, actual_mode)
                    
                    result["message"] = f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼å·²è‡ªåŠ¨æ·»åŠ åˆ°çŸ¥è¯†åº“å¹¶å‘é‡åŒ–\n\nğŸ“Š åˆ†å—ä¿¡æ¯ï¼š\n- æ–‡æœ¬å—æ•°é‡ï¼š{len(chunks)} ä¸ª\n- åˆ†å—æ¨¡å¼ï¼š{mode_desc}\n- QAè½¬æ¢ï¼š{'å·²å¯ç”¨' if use_qa_format else 'æœªå¯ç”¨'}"
                    
                    logger.info(f"âœ… æ–‡æ¡£å·²è‡ªåŠ¨æ·»åŠ åˆ°çŸ¥è¯†åº“: {file.filename} -> {kb.name}")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ ID: {knowledge_base_id}")
                    
            except Exception as e:
                error_msg = str(e)
                # å°è¯•æå–æ›´æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
                if 'status' in error_msg and 'error' in error_msg:
                    try:
                        import json
                        # å°è¯•è§£æ Qdrant é”™è¯¯å“åº”
                        if 'Raw response content' in error_msg:
                            json_str = error_msg.split('Raw response content:')[1].strip()
                            error_data = json.loads(json_str.replace("b'", "").replace("'", ""))
                            if 'status' in error_data and 'error' in error_data['status']:
                                error_msg = error_data['status']['error']
                    except:
                        pass
                
                logger.error(f"è‡ªåŠ¨æ·»åŠ åˆ°çŸ¥è¯†åº“å¤±è´¥: {error_msg}", exc_info=True)
                doc.status = "failed"
                doc.error_message = error_msg
                db.commit()
                result["message"] = f"æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œä½†è‡ªåŠ¨æ·»åŠ åˆ°çŸ¥è¯†åº“å¤±è´¥: {error_msg}"
        else:
            logger.info(f"è·³è¿‡è‡ªåŠ¨å¤„ç†: knowledge_base_id={knowledge_base_id}, auto_process={auto_process}")
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        # å°è¯•æå–æ›´æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
        if 'status' in error_msg and 'error' in error_msg:
            try:
                import json
                # å°è¯•è§£æ Qdrant é”™è¯¯å“åº”
                if 'Raw response content' in error_msg:
                    json_str = error_msg.split('Raw response content:')[1].strip()
                    error_data = json.loads(json_str.replace("b'", "").replace("'", ""))
                    if 'status' in error_data and 'error' in error_data['status']:
                        error_msg = error_data['status']['error']
            except:
                pass
        
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {error_msg}", exc_info=True)
        raise HTTPException(status_code=500, detail=error_msg)


@router.post("/{document_id}/process")
async def process_document(
    document_id: int,
    db: Session = Depends(get_db)
):
    """
    å¤„ç†æ–‡æ¡£ï¼ˆè§£æã€æ¸…æ´—ã€åˆ†å—ï¼‰
    """
    try:
        # è·å–æ–‡æ¡£è®°å½•
        doc = db.query(DocumentModel).filter(DocumentModel.id == document_id).first()
        if not doc:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        # æ›´æ–°çŠ¶æ€
        doc.status = "processing"
        db.commit()
        
        # å¤„ç†æ–‡æ¡£
        file_path = Path(doc.original_path)
        result = DocumentProcessor.process_document(file_path)
        
        # æ›´æ–°è®°å½•
        doc.status = "completed"
        doc.chunk_count = len(result["chunks"])
        doc.metadata = result["metadata"]
        doc.processed_at = datetime.utcnow()
        db.commit()
        
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: {doc.filename}")
        
        return {
            "id": doc.id,
            "filename": doc.filename,
            "status": doc.status,
            "chunk_count": doc.chunk_count,
            "chunks": result["chunks"],
            "metadata": result["metadata"]
        }
        
    except Exception as e:
        # æ›´æ–°å¤±è´¥çŠ¶æ€
        doc.status = "failed"
        doc.error_message = str(e)
        db.commit()
        
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/")
@router.get("")
async def list_documents(
    skip: int = 0,
    limit: int = 100,  # å¢åŠ é»˜è®¤é™åˆ¶
    knowledge_base_id: int = None,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    query = db.query(DocumentModel)
    
    if knowledge_base_id:
        query = query.filter(DocumentModel.knowledge_base_id == knowledge_base_id)
    
    # å…ˆè·å–æ€»æ•°ï¼Œå†åˆ†é¡µ
    total = query.count()
    documents = query.order_by(DocumentModel.created_at.desc()).offset(skip).limit(limit).all()
    
    logger.info(f"ğŸ“‹ æŸ¥è¯¢æ–‡æ¡£åˆ—è¡¨: knowledge_base_id={knowledge_base_id}, æ€»æ•°={total}, è¿”å›={len(documents)}")
    
    return {
        "total": total,
        "documents": [
            {
                "id": doc.id,
                "filename": doc.filename,
                "file_type": doc.file_type,
                "file_size": doc.file_size,
                "status": doc.status,
                "chunk_count": doc.chunk_count,
                "created_at": doc.created_at.isoformat() if doc.created_at else None,
                "error_message": doc.error_message if doc.status == "failed" else None,
            }
            for doc in documents
        ]
    }


@router.post("/generate-qa-preview")
async def generate_qa_preview(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    ç”Ÿæˆé—®ç­”æ ¼å¼é¢„è§ˆï¼ˆç”¨äºä¸Šä¼ å‰é¢„è§ˆï¼‰
    """
    try:
        # ä¸´æ—¶ä¿å­˜æ–‡ä»¶
        temp_path = settings.UPLOAD_DIR / f"temp_{datetime.now().timestamp()}_{file.filename}"
        content = await file.read()
        with open(temp_path, "wb") as f:
            f.write(content)
        
        # å¤„ç†æ–‡æ¡£è·å–chunks
        processed_result = DocumentProcessor.process_document(Path(temp_path))
        chunks = processed_result["chunks"][:3]  # åªå¤„ç†å‰3ä¸ªchunkä½œä¸ºé¢„è§ˆ
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        temp_path.unlink(missing_ok=True)
        
        if not chunks:
            return {"success": False, "message": "æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œæœªæå–åˆ°æ–‡æœ¬å†…å®¹"}
        
        # è·å–é»˜è®¤ Embedding æä¾›å•†ï¼ˆç”¨äºè·å–OpenAI APIé…ç½®ï¼‰
        embedding_provider = db.query(EmbeddingProvider).filter(
            EmbeddingProvider.is_default == True
        ).first()
        
        logger.info(f"ğŸ” æ£€æŸ¥Embeddingæä¾›å•†: provider={embedding_provider.name if embedding_provider else 'None'}, has_key={bool(embedding_provider and embedding_provider.api_key and embedding_provider.api_key.strip()) if embedding_provider else False}")
        
        if not embedding_provider or not embedding_provider.api_key or not embedding_provider.api_key.strip():
            # è¿”å›ç¤ºä¾‹é¢„è§ˆ
            logger.warning("âš ï¸ æœªé…ç½®æœ‰æ•ˆçš„OpenAI APIå¯†é’¥ï¼Œè¿”å›ç¤ºä¾‹é¢„è§ˆ")
            preview = []
            for i, chunk in enumerate(chunks):
                preview.append({
                    "chunk": chunk[:200] + "..." if len(chunk) > 200 else chunk,
                    "qa_pairs": [
                        {
                            "question": f"ç¤ºä¾‹é—®é¢˜ {i+1}-1ï¼ˆéœ€é…ç½®OpenAI APIæ‰èƒ½ç”ŸæˆçœŸå®é—®ç­”å¯¹ï¼‰",
                            "answer": "ç¤ºä¾‹ç­”æ¡ˆ..."
                        },
                        {
                            "question": f"ç¤ºä¾‹é—®é¢˜ {i+1}-2ï¼ˆéœ€é…ç½®OpenAI APIæ‰èƒ½ç”ŸæˆçœŸå®é—®ç­”å¯¹ï¼‰",
                            "answer": "ç¤ºä¾‹ç­”æ¡ˆ..."
                        }
                    ]
                })
            return {
                "success": True,
                "preview": preview,
                "is_mock": True,
                "message": "è¿™æ˜¯ç¤ºä¾‹é¢„è§ˆï¼Œé…ç½®OpenAI APIåå¯ç”ŸæˆçœŸå®é—®ç­”å¯¹"
            }
        
        # ä½¿ç”¨OpenAI APIç”ŸæˆçœŸå®é—®ç­”å¯¹
        from app.core.qa_generator import QAGenerator
        
        qa_generator = QAGenerator(
            api_key=embedding_provider.api_key,
            base_url=embedding_provider.base_url or "https://api.openai.com/v1",
            model="gpt-3.5-turbo"
        )
        preview = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"QAé¢„è§ˆï¼šæ­£åœ¨ä¸ºç¬¬ {i+1}/{len(chunks)} ä¸ªæ–‡æœ¬å—ç”Ÿæˆé—®ç­”å¯¹...")
            qa_pairs = await qa_generator.generate_qa_pairs(
                chunk,
                num_questions=2,
                domain="é€šç”¨"
            )
            
            preview.append({
                "chunk": chunk[:200] + "..." if len(chunk) > 200 else chunk,
                "qa_pairs": qa_pairs if qa_pairs else [
                    {"question": f"AIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç¤ºä¾‹é—®é¢˜ {i+1}", "answer": "ç¤ºä¾‹ç­”æ¡ˆ..."}
                ]
            })
            logger.info(f"  ç”Ÿæˆäº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
        
        return {
            "success": True,
            "preview": preview,
            "is_mock": False,
            "message": f"æˆåŠŸç”Ÿæˆ{len(preview)}ä¸ªæ–‡æœ¬å—çš„é—®ç­”å¯¹é¢„è§ˆ"
        }
        
    except Exception as e:
        logger.error(f"ç”ŸæˆQAé¢„è§ˆå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{document_id}")
async def get_document_detail(
    document_id: int,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯"""
    try:
        doc = db.query(DocumentModel).filter(DocumentModel.id == document_id).first()
        if not doc:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        return {
            "id": doc.id,
            "filename": doc.filename,
            "file_type": doc.file_type,
            "file_size": doc.file_size,
            "status": doc.status,
            "chunk_count": doc.chunk_count,
            "error_message": doc.error_message,
            "doc_metadata": doc.doc_metadata,
            "knowledge_base_id": doc.knowledge_base_id,
            "created_at": doc.created_at.isoformat() if doc.created_at else None,
            "processed_at": doc.processed_at.isoformat() if doc.processed_at else None,
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£è¯¦æƒ…å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{document_id}/chunks")
async def get_document_chunks(
    document_id: int,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡æ¡£çš„åˆ‡åˆ†ç‰‡æ®µ"""
    try:
        doc = db.query(DocumentModel).filter(DocumentModel.id == document_id).first()
        if not doc:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        # é‡æ–°å¤„ç†æ–‡æ¡£ä»¥è·å–chunks
        file_path = Path(doc.original_path)
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨")
        
        logger.info(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡æ¡£chunks: {doc.filename}")
        # ä½¿ç”¨autoæ¨¡å¼é‡æ–°å¤„ç†ï¼Œè‡ªåŠ¨è¯†åˆ«FAQæ ¼å¼
        processed_result = DocumentProcessor.process_document(file_path, chunk_mode="auto")
        chunks = processed_result["chunks"]
        actual_mode = processed_result["metadata"].get("chunk_mode", "standard")
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(chunks)} ä¸ªchunksï¼ˆæ¨¡å¼: {actual_mode}ï¼‰")
        
        return {
            "document_id": doc.id,
            "filename": doc.filename,
            "total_chars": processed_result["metadata"]["char_count"],
            "total_chunks": len(chunks),
            "chunks": [
                {
                    "index": i,
                    "content": chunk,
                    "word_count": len(chunk.split()),
                    "char_count": len(chunk)
                }
                for i, chunk in enumerate(chunks)
            ]
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ‡åˆ†ç‰‡æ®µå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/{document_id}")
async def delete_document(
    document_id: int,
    db: Session = Depends(get_db)
):
    """åˆ é™¤æ–‡æ¡£"""
    doc = db.query(DocumentModel).filter(DocumentModel.id == document_id).first()
    if not doc:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    # åˆ é™¤æ–‡ä»¶
    try:
        Path(doc.original_path).unlink(missing_ok=True)
    except Exception as e:
        logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
    
    # åˆ é™¤æ•°æ®åº“è®°å½•
    db.delete(doc)
    db.commit()
    
    return {"message": "æ–‡æ¡£åˆ é™¤æˆåŠŸ"}

