"""
AIæä¾›å•†é…ç½®ç®¡ç† API
"""

from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from loguru import logger

from app.core.multi_model_engine import multi_model_engine
from app.models.database import get_db, EmbeddingProvider

router = APIRouter()


class ProviderConfig(BaseModel):
    """æä¾›å•†é…ç½®"""
    provider: str
    api_key: str
    base_url: Optional[str] = None


class ProviderVerifyRequest(BaseModel):
    """æä¾›å•†éªŒè¯è¯·æ±‚"""
    provider: str
    api_key: str
    base_url: Optional[str] = None


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    provider: str
    model: str
    messages: List[Dict[str, str]]
    temperature: float = 0.7
    max_tokens: int = 2000
    stream: bool = False


@router.get("/providers")
async def list_providers():
    """
    è·å–æ‰€æœ‰æ”¯æŒçš„AIæä¾›å•†åˆ—è¡¨
    """
    try:
        providers = multi_model_engine.list_providers()
        return {
            "providers": providers,
            "total": len(providers)
        }
    except Exception as e:
        logger.error(f"è·å–æä¾›å•†åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/providers/{provider}")
async def get_provider_info(provider: str):
    """
    è·å–æŒ‡å®šæä¾›å•†çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        info = multi_model_engine.get_provider_info(provider)
        if not info:
            raise HTTPException(status_code=404, detail="æä¾›å•†ä¸å­˜åœ¨")
        
        return {
            "provider": provider,
            **info,
            "configured": provider in multi_model_engine.api_keys,
            "has_api_key": provider in multi_model_engine.api_keys  # æ·»åŠ æ˜ç¡®çš„APIå¯†é’¥çŠ¶æ€
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æä¾›å•†ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/providers/{provider}/check-config")
async def check_provider_config(provider: str):
    """
    æ£€æŸ¥æŒ‡å®šæä¾›å•†æ˜¯å¦å·²é…ç½®APIå¯†é’¥
    
    è¿”å›æ ¼å¼ï¼š
    {
        "provider": "openai",
        "configured": true,
        "has_api_key": true,
        "message": "å·²é…ç½®"
    }
    """
    try:
        has_key = provider in multi_model_engine.api_keys
        return {
            "provider": provider,
            "configured": has_key,
            "has_api_key": has_key,
            "message": "å·²é…ç½®" if has_key else "æœªé…ç½®"
        }
    except Exception as e:
        logger.error(f"æ£€æŸ¥æä¾›å•†é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/providers/verify")
@router.post("/providers/verify/")
async def verify_provider(request: ProviderVerifyRequest):
    """
    éªŒè¯AIæä¾›å•†APIå¯†é’¥
    """
    try:
        # éªŒè¯æä¾›å•†æ˜¯å¦å­˜åœ¨
        provider_info = multi_model_engine.get_provider_info(request.provider)
        if not provider_info:
            raise HTTPException(status_code=404, detail="ä¸æ”¯æŒçš„æä¾›å•†")
        
        # éªŒè¯APIå¯†é’¥
        result = await multi_model_engine.verify_api_key(
            request.provider,
            request.api_key,
            request.base_url
        )
        
        if result["valid"]:
            logger.info(f"âœ… APIéªŒè¯æˆåŠŸ: {request.provider}")
            return {
                "valid": True,
                "message": result["message"],
                "models": result["models"],
                "provider": request.provider
            }
        else:
            logger.warning(f"âš ï¸  APIéªŒè¯å¤±è´¥: {request.provider} - {result['message']}")
            return {
                "valid": False,
                "message": result["message"],
                "models": [],
                "provider": request.provider
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"éªŒè¯æä¾›å•†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/providers/fetch-models")
@router.post("/providers/fetch-models/")
async def fetch_provider_models(request: ProviderVerifyRequest):
    """
    è·å–æŒ‡å®šAIæä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆæ— éœ€é¢„å…ˆé…ç½®ï¼Œä»…ç”¨äºæ¢ç´¢ï¼‰
    
    è¿”å›æ ¼å¼ï¼š
    {
        "valid": true/false,
        "provider": "openai",
        "models": ["gpt-4", "gpt-3.5-turbo", ...],
        "message": "æˆåŠŸ/å¤±è´¥ä¿¡æ¯"
    }
    """
    try:
        logger.info(f"ğŸ” å°è¯•è·å– {request.provider} çš„æ¨¡å‹åˆ—è¡¨...")
        
        # é¦–å…ˆå°è¯•éªŒè¯APIå¯†é’¥å¹¶è·å–æ¨¡å‹
        result = await multi_model_engine.verify_api_key(
            request.provider,
            request.api_key,
            request.base_url
        )
        
        if result["valid"] and result.get("models"):
            logger.info(f"âœ… æˆåŠŸè·å– {request.provider} çš„ {len(result['models'])} ä¸ªæ¨¡å‹")
            return {
                "valid": True,
                "provider": request.provider,
                "models": result["models"],
                "message": "æˆåŠŸè·å–æ¨¡å‹åˆ—è¡¨"
            }
        else:
            logger.warning(f"âš ï¸ APIéªŒè¯å¤±è´¥ï¼Œè¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨")
            # å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›ä¸€äº›å¸¸è§çš„é»˜è®¤æ¨¡å‹ä¾›ç”¨æˆ·é€‰æ‹©
            default_models = _get_default_models(request.provider)
            return {
                "valid": False,
                "provider": request.provider,
                "models": default_models,
                "message": result.get("message", "APIéªŒè¯å¤±è´¥ï¼Œæ˜¾ç¤ºé»˜è®¤æ¨¡å‹åˆ—è¡¨ï¼ˆå¯æ‰‹åŠ¨è¾“å…¥å…¶ä»–æ¨¡å‹ï¼‰")
            }
            
    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        default_models = _get_default_models(request.provider)
        return {
            "valid": False,
            "provider": request.provider,
            "models": default_models,
            "message": f"è·å–å¤±è´¥: {str(e)}ï¼Œæ˜¾ç¤ºé»˜è®¤æ¨¡å‹åˆ—è¡¨ï¼ˆå¯æ‰‹åŠ¨è¾“å…¥å…¶ä»–æ¨¡å‹ï¼‰"
        }


def _get_default_models(provider: str) -> List[str]:
    """è·å–é»˜è®¤æ¨¡å‹åˆ—è¡¨"""
    default_models = {
        "openai": [
            "gpt-4-turbo-preview",
            "gpt-4",
            "gpt-3.5-turbo",
            "gpt-4-1106-preview",
            "gpt-4-0125-preview",
            "gpt-3.5-turbo-16k"
        ],
        "anthropic": [
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "claude-3-haiku-20240307",
            "claude-2.1",
            "claude-2.0"
        ],
        "openrouter": [
            "anthropic/claude-3-opus",
            "anthropic/claude-3-sonnet",
            "openai/gpt-4-turbo-preview",
            "google/gemini-pro",
            "meta-llama/llama-3-70b-instruct"
        ],
        "gemini": [
            "gemini-pro",
            "gemini-pro-vision",
            "gemini-1.5-pro"
        ],
        "deepseek": [
            "deepseek-chat",
            "deepseek-coder"
        ],
        "qwen": [
            "qwen-turbo",
            "qwen-plus",
            "qwen-max",
            "qwen-max-longcontext"
        ],
        "local": [
            "local-model"
        ]
    }
    return default_models.get(provider, [])


@router.post("/providers/configure")
@router.post("/providers/configure/")
async def configure_provider(config: ProviderConfig):
    """
    é…ç½®AIæä¾›å•†ï¼ˆè®¾ç½®APIå¯†é’¥ç­‰ï¼‰
    
    æ³¨æ„ï¼šé…ç½®å‰å»ºè®®å…ˆè°ƒç”¨ /providers/verify éªŒè¯APIå¯†é’¥
    """
    try:
        # éªŒè¯æä¾›å•†æ˜¯å¦å­˜åœ¨
        provider_info = multi_model_engine.get_provider_info(config.provider)
        if not provider_info:
            raise HTTPException(status_code=404, detail="ä¸æ”¯æŒçš„æä¾›å•†")
        
        # éªŒè¯APIå¯†é’¥
        verification = await multi_model_engine.verify_api_key(
            config.provider,
            config.api_key,
            config.base_url
        )
        
        if not verification["valid"]:
            raise HTTPException(
                status_code=400,
                detail=f"APIå¯†é’¥éªŒè¯å¤±è´¥: {verification['message']}"
            )
        
        # è®¾ç½®APIå¯†é’¥
        multi_model_engine.set_api_key(config.provider, config.api_key)
        
        # ä¿å­˜å¯ç”¨æ¨¡å‹åˆ—è¡¨
        if verification["models"]:
            multi_model_engine.set_available_models(config.provider, verification["models"])
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰base_url
        if config.base_url:
            multi_model_engine.set_custom_config(
                config.provider,
                {"base_url": config.base_url}
            )
        
        logger.info(f"âœ… å·²é…ç½®æä¾›å•†: {config.provider}")
        
        return {
            "message": "é…ç½®æˆåŠŸ",
            "provider": config.provider,
            "configured": True,
            "models": verification["models"]
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é…ç½®æä¾›å•†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/completions")
@router.post("/chat/completions/")
async def chat_completions(request: ChatRequest):
    """
    å¤šæä¾›å•†ç»Ÿä¸€èŠå¤©è¡¥å…¨æ¥å£
    
    å…¼å®¹OpenAIæ ¼å¼ï¼Œæ”¯æŒå¤šä¸ªæä¾›å•†
    """
    try:
        # æ£€æŸ¥æä¾›å•†æ˜¯å¦å·²é…ç½®
        if request.provider not in multi_model_engine.api_keys and request.provider != "local":
            raise HTTPException(
                status_code=400,
                detail=f"æä¾›å•† {request.provider} æœªé…ç½®ï¼Œè¯·å…ˆè®¾ç½®APIå¯†é’¥"
            )
        
        # è°ƒç”¨æ¨ç†å¼•æ“
        response = await multi_model_engine.chat_completion(
            provider=request.provider,
            model=request.model,
            messages=request.messages,
            stream=request.stream,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        logger.info(f"âœ… æ¨ç†å®Œæˆ: {request.provider}/{request.model}")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"èŠå¤©è¡¥å…¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/providers/{provider}/models")
async def get_provider_models(provider: str, db: Session = Depends(get_db)):
    """
    è·å–æŒ‡å®šæä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨
    
    Args:
        provider: æä¾›å•†åç§° (openai, anthropic, geminiç­‰)
    
    Returns:
        {
            "has_config": bool,  # æ˜¯å¦å·²é…ç½®APIå¯†é’¥
            "models": List[str],  # å¯ç”¨æ¨¡å‹åˆ—è¡¨
            "provider": str
        }
    """
    try:
        # ä»æ•°æ®åº“æŸ¥æ‰¾è¯¥æä¾›å•†çš„é…ç½®
        provider_config = db.query(EmbeddingProvider).filter(
            EmbeddingProvider.provider_type == provider,
            EmbeddingProvider.api_key.isnot(None)
        ).first()
        
        if not provider_config or not provider_config.api_key:
            # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
            default_models = _get_default_models(provider)
            return {
                "has_config": False,
                "models": default_models,
                "provider": provider,
                "message": f"æœªé…ç½® {provider} çš„APIå¯†é’¥ï¼Œæ˜¾ç¤ºé»˜è®¤æ¨¡å‹åˆ—è¡¨"
            }
        
        # å·²é…ç½®ï¼Œå°è¯•ä»APIè·å–çœŸå®æ¨¡å‹åˆ—è¡¨
        try:
            multi_model_engine.set_api_key(provider, provider_config.api_key)
            if provider_config.base_url:
                multi_model_engine.set_custom_config(provider, {
                    "base_url": provider_config.base_url
                })
            
            # è·å–ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨
            models = multi_model_engine.available_models.get(provider, [])
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä»APIè·å–
            if not models:
                logger.info(f"ğŸ”„ ä» {provider} API è·å–æ¨¡å‹åˆ—è¡¨...")
                verification = await multi_model_engine.verify_api_key(provider, provider_config.api_key)
                if verification["valid"] and verification.get("models"):
                    models = verification["models"]
                    multi_model_engine.set_available_models(provider, models)
                    logger.info(f"âœ… æˆåŠŸè·å– {provider} çš„ {len(models)} ä¸ªæ¨¡å‹")
                else:
                    # APIéªŒè¯å¤±è´¥ï¼Œè¿”å›é»˜è®¤æ¨¡å‹
                    models = _get_default_models(provider)
                    logger.warning(f"âš ï¸ {provider} APIéªŒè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨")
            
            return {
                "has_config": True,
                "models": models,
                "provider": provider,
                "message": f"å·²ä» {provider} API è·å–æ¨¡å‹åˆ—è¡¨"
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å– {provider} æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            default_models = _get_default_models(provider)
            return {
                "has_config": True,
                "models": default_models,
                "provider": provider,
                "error": str(e),
                "message": f"è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨"
            }
    
    except Exception as e:
        logger.error(f"âŒ æŸ¥è¯¢ {provider} é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/providers/models/available")
async def get_available_models(db: Session = Depends(get_db)):
    """
    è·å–æ‰€æœ‰å·²é…ç½®æä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨
    
    è¿”å›æ ¼å¼ï¼š
    {
        "models": [
            {
                "id": "openai/gpt-4-turbo-preview",
                "name": "gpt-4-turbo-preview", 
                "display_name": "GPT-4 Turbo (OpenAI)",
                "provider": "openai",
                "provider_name": "OpenAI"
            },
            ...
        ]
    }
    """
    try:
        all_models = []
        
        # ä»æ•°æ®åº“åŠ è½½embedding_providersï¼ˆåŒ…å«OpenAIç­‰APIé…ç½®ï¼‰
        embedding_providers = db.query(EmbeddingProvider).filter(
            EmbeddingProvider.api_key.isnot(None)
        ).all()
        
        for ep in embedding_providers:
            if ep.provider_type == "openai" and ep.api_key and ep.api_key.strip():
                # å°†embedding providerçš„é…ç½®åŒæ­¥åˆ°multi_model_engine
                if "openai" not in multi_model_engine.api_keys:
                    multi_model_engine.set_api_key("openai", ep.api_key)
                    logger.info(f"ğŸ”„ ä»æ•°æ®åº“åŠ è½½ OpenAI APIå¯†é’¥")
        
        # éå†æ‰€æœ‰å·²é…ç½®çš„æä¾›å•†
        for provider, api_key in multi_model_engine.api_keys.items():
            provider_info = multi_model_engine.get_provider_info(provider)
            if not provider_info:
                continue
                
            provider_name = provider_info.get("name", provider)
            
            # è·å–è¯¥æä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨
            models = multi_model_engine.available_models.get(provider, [])
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨ï¼Œå°è¯•ä»APIè·å–
            if not models:
                logger.info(f"ğŸ”„ ä» {provider} è·å–æ¨¡å‹åˆ—è¡¨...")
                try:
                    verification = await multi_model_engine.verify_api_key(provider, api_key)
                    if verification["valid"] and verification["models"]:
                        models = verification["models"]
                        multi_model_engine.set_available_models(provider, models)
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å– {provider} æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
                    continue
            
            # æ ¼å¼åŒ–æ¨¡å‹åˆ—è¡¨
            for model in models:
                all_models.append({
                    "id": f"{provider}/{model}",  # å”¯ä¸€ID
                    "name": model,  # æ¨¡å‹åç§°
                    "display_name": f"{model} ({provider_name})",  # æ˜¾ç¤ºåç§°
                    "provider": provider,  # æä¾›å•†ID
                    "provider_name": provider_name  # æä¾›å•†åç§°
                })
        
        logger.info(f"ğŸ“‹ è¿”å› {len(all_models)} ä¸ªå¯ç”¨æ¨¡å‹")
        
        return {
            "models": all_models,
            "total": len(all_models)
        }
        
    except Exception as e:
        logger.error(f"è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/providers/{provider}/config")
async def remove_provider_config(provider: str):
    """
    åˆ é™¤æä¾›å•†é…ç½®
    """
    try:
        if provider in multi_model_engine.api_keys:
            del multi_model_engine.api_keys[provider]
            logger.info(f"âœ… å·²åˆ é™¤æä¾›å•†é…ç½®: {provider}")
            return {"message": "é…ç½®å·²åˆ é™¤", "provider": provider}
        else:
            raise HTTPException(status_code=404, detail="æä¾›å•†æœªé…ç½®")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
