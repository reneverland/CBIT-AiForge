"""
åº”ç”¨æ¨ç†API - OpenAIå…¼å®¹æ¥å£
ä½¿ç”¨æ··åˆæ£€ç´¢å¼•æ“æä¾›æ™ºèƒ½é—®ç­”æœåŠ¡
"""

from fastapi import APIRouter, Depends, HTTPException, Header
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from loguru import logger
from datetime import datetime
import time
import uuid

from app.models.database import get_db, Application, ApplicationKnowledgeBase, KnowledgeBase, FixedQAPair, EmbeddingProvider, RetrievalLog
from app.core.hybrid_retrieval_engine import hybrid_retrieval_engine
from app.core.multi_model_engine import multi_model_engine

router = APIRouter()


def is_casual_conversation(query: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ—¥å¸¸å¯¹è¯/é—²èŠ/å¸¸è§„é—®å€™"""
    query_lower = query.strip().lower()
    query_len = len(query_lower)
    
    # æ—¥å¸¸å¯¹è¯å…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
    casual_patterns = {
        # ä¸­æ–‡æ‰“æ‹›å‘¼
        "greetings": ["ä½ å¥½", "æ‚¨å¥½", "å—¨", "å˜¿", "å“ˆå–½", "hi", "hello", "hey"],
        # é—®å€™è¯­
        "time_greetings": ["æ—©ä¸Šå¥½", "æ™šä¸Šå¥½", "ä¸‹åˆå¥½", "æ™šå®‰", "æ—©å®‰", "ä¸­åˆå¥½"],
        # ç¤¼è²Œç”¨è¯­
        "thanks": ["è°¢è°¢", "æ„Ÿè°¢", "å¤šè°¢", "thank", "thanks", "thx"],
        # ç®€å•é—²èŠ
        "small_talk": ["æ€ä¹ˆæ ·", "åœ¨å—", "åœ¨ä¸åœ¨", "how are you", "what's up", "wassup"],
        # çŠ¶æ€è¯¢é—®
        "status": ["å¿™å—", "å¿™ä¸å¿™", "æœ‰ç©ºå—", "å¹²å˜›å‘¢", "åœ¨å¹²å˜›"],
        # å†è§
        "goodbye": ["å†è§", "æ‹œæ‹œ", "bye", "goodbye", "see you"],
    }
    
    # 1. æçŸ­æŸ¥è¯¢ï¼ˆâ‰¤15å­—ç¬¦ï¼‰+ åŒ…å«ä»»ä½•æ—¥å¸¸å…³é”®è¯
    if query_len <= 15:
        for category, patterns in casual_patterns.items():
            for pattern in patterns:
                if pattern in query_lower:
                    logger.info(f"ğŸ’¬ æ£€æµ‹åˆ°æ—¥å¸¸å¯¹è¯ï¼ˆ{category}ï¼‰: {query}")
                    return True
    
    # 2. å•å­—æˆ–ä¸¤å­—æŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯"å—¨"ã€"ä½ å¥½"ç­‰ï¼‰
    if query_len <= 2:
        logger.info(f"ğŸ’¬ æ£€æµ‹åˆ°æçŸ­æŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯æ—¥å¸¸å¯¹è¯ï¼‰: {query}")
        return True
    
    # 3. å¸¸è§çš„å®Œæ•´é—®å€™å¥å¼
    greeting_sentences = [
        "ä½ å¥½å•Š", "ä½ å¥½å‘€", "æ‚¨å¥½å•Š", "åœ¨å—", "åœ¨ä¸åœ¨",
        "hello there", "hi there", "hey there",
        "how are you", "how r u", "how do you do",
        "what's up", "whats up", "sup",
    ]
    if query_lower in greeting_sentences:
        logger.info(f"ğŸ’¬ æ£€æµ‹åˆ°å®Œæ•´é—®å€™å¥å¼: {query}")
        return True
    
    return False


# Pydanticæ¨¡å‹
class ChatMessage(BaseModel):
    role: str  # user, assistant, system
    content: str


class ChatCompletionRequest(BaseModel):
    messages: List[ChatMessage]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    stream: bool = False
    skip_fixed_qa: bool = False  # æ˜¯å¦è·³è¿‡å›ºå®šQ&AåŒ¹é…ï¼ˆç”¨æˆ·é€‰æ‹©"ç»§ç»­æ€è€ƒ"æ—¶ï¼‰
    selected_qa_id: Optional[int] = None  # ç”¨æˆ·é€‰ä¸­çš„Q&A IDï¼ˆç›´æ¥è¿”å›ç­”æ¡ˆï¼‰
    force_web_search: bool = False  # æ˜¯å¦å¼ºåˆ¶å¯ç”¨è”ç½‘æœç´¢ï¼ˆç”¨æˆ·æˆæƒåï¼‰


class TestRetrievalRequest(BaseModel):
    query: str
    application_id: int


# è¾…åŠ©å‡½æ•°
def get_app_by_api_key(api_key: str, db: Session) -> Optional[Application]:
    """é€šè¿‡APIå¯†é’¥è·å–åº”ç”¨"""
    return db.query(Application).filter(Application.api_key == api_key).first()


def get_app_by_id(app_id: int, db: Session) -> Optional[Application]:
    """é€šè¿‡IDè·å–åº”ç”¨"""
    return db.query(Application).filter(Application.id == app_id).first()


async def prepare_app_context(app: Application, db: Session) -> Dict[str, Any]:
    """å‡†å¤‡åº”ç”¨ä¸Šä¸‹æ–‡ï¼ˆçŸ¥è¯†åº“ã€Q&Aç­‰ï¼‰ - v3.0é€‚é…ç‰ˆ"""
    # è·å–å®Œæ•´é…ç½®ï¼ˆå«é»˜è®¤å€¼ï¼‰
    full_config = app.get_mode_config_with_defaults()
    
    # v3.0: æ ¹æ®modeå’Œpriority_orderåˆ¤æ–­å¯ç”¨çš„åŠŸèƒ½
    priority_order = full_config.get("priority_order", [])
    enable_fixed_qa = any("fixed_qa" in item for item in priority_order)
    enable_vector_kb = "vector_kb" in priority_order
    enable_web_search = full_config.get("allow_web_search", False)
    
    # è·å–åº”ç”¨é…ç½®ï¼ˆé€‚é…v3.0ï¼‰
    app_config = {
        "id": app.id,
        "name": app.name,
        "enable_fixed_qa": enable_fixed_qa,
        "enable_vector_kb": enable_vector_kb,
        "enable_web_search": enable_web_search,
        # ç­–ç•¥æ¨¡å¼é…ç½®
        "strategy_mode": "safe_priority",  # v3.0é»˜è®¤å®‰å…¨ä¼˜å…ˆ
        "web_search_auto_threshold": full_config.get("web_search_auto_threshold", 0.50),
        "similarity_threshold_high": full_config.get("fixed_qa_threshold", 0.90),
        "similarity_threshold_low": full_config.get("recommend_threshold", 0.65),
        "retrieval_strategy": "priority",
        "top_k": full_config.get("top_k", 5),
        "fixed_qa_weight": 1.0,
        "vector_kb_weight": 1.0,
        "web_search_weight": 1.0,
        "fusion_strategy": "weighted_avg",
        "fusion_config": full_config,  # ä½¿ç”¨å®Œæ•´é…ç½®
        "web_search_domains": full_config.get("web_search_domains", []),
        "search_channels": full_config.get("search_channels", []),
        "enable_preprocessing": True,
        "enable_intent_recognition": True,
        "enable_language_detection": True,
        "enable_sensitive_filter": False,
        "sensitive_words": [],
        "enable_source_tracking": full_config.get("enable_source_tracking", True),
        "enable_citation": full_config.get("enable_citation", True),
        "system_prompt": None,
        "temperature": app.temperature,
        "max_tokens": full_config.get("max_tokens", 2000)
    }
    
    # ğŸ”‘ å…³é”®ä¿®å¤ï¼šç¡®ä¿èåˆç­–ç•¥é…ç½®ä¸­åŒ…å«fixed_qaé…ç½®
    if "fusion_config" not in app_config or not app_config["fusion_config"]:
        app_config["fusion_config"] = {}
    
    if "fixed_qa" not in app_config["fusion_config"]:
        app_config["fusion_config"]["fixed_qa"] = {}
    
    # ä»fusion_config.strategyä¸­è¯»å–é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    strategy_config = app_config["fusion_config"].get("strategy", {})
    
    # å°†qaé˜ˆå€¼é…ç½®åŒæ­¥åˆ°fixed_qaé…ç½®ä¸­
    app_config["fusion_config"]["fixed_qa"]["mode"] = strategy_config.get("fixed_qa_mode", "smart")
    app_config["fusion_config"]["fixed_qa"]["direct_match_threshold"] = strategy_config.get("qa_direct_threshold", 0.90)
    app_config["fusion_config"]["fixed_qa"]["suggest_threshold"] = strategy_config.get("qa_suggest_threshold", 0.55)  # ä½¿ç”¨ç­–ç•¥é…ç½®çš„é˜ˆå€¼
    app_config["fusion_config"]["fixed_qa"]["qa_min_threshold"] = strategy_config.get("qa_min_threshold", 0.50)  # æœ€ä½åŒ¹é…é˜ˆå€¼
    app_config["fusion_config"]["fixed_qa"]["max_suggestions"] = strategy_config.get("max_suggestions", 3)
    
    # è·å–å›ºå®šQ&Aå¯¹
    fixed_qa_pairs = []
    if app.enable_fixed_qa:
        qa_list = db.query(FixedQAPair).filter(
            FixedQAPair.application_id == app.id,
            FixedQAPair.is_active == True
        ).all()
        
        fixed_qa_pairs = [
            {
                "id": qa.id,
                "question": qa.question,
                "answer": qa.answer,
                "keywords": qa.keywords,
                "embedding_vector": qa.embedding_vector,
                "category": qa.category,
                "priority": qa.priority,
                "is_active": qa.is_active
            }
            for qa in qa_list
        ]
    
    # è·å–å…³è”çš„çŸ¥è¯†åº“
    knowledge_bases = []
    if app.enable_vector_kb:
        kb_assocs = db.query(ApplicationKnowledgeBase).filter(
            ApplicationKnowledgeBase.application_id == app.id
        ).order_by(ApplicationKnowledgeBase.priority).all()
        
        logger.info(f"ğŸ“– åº”ç”¨ [{app.name}] æŸ¥è¯¢åˆ° {len(kb_assocs)} ä¸ªçŸ¥è¯†åº“å…³è”")
        
        for assoc in kb_assocs:
            kb = db.query(KnowledgeBase).filter(KnowledgeBase.id == assoc.knowledge_base_id).first()
            if kb:
                knowledge_bases.append({
                    "id": kb.id,
                    "name": kb.name,
                    "collection_name": kb.collection_name,
                    "priority": assoc.priority,
                    "weight": assoc.weight,
                    "boost_factor": assoc.boost_factor
                })
                logger.info(f"âœ… æ·»åŠ çŸ¥è¯†åº“: {kb.name} (collection: {kb.collection_name})")
            else:
                logger.warning(f"âš ï¸ çŸ¥è¯†åº“ID {assoc.knowledge_base_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
    
    # è·å–Embeddingæä¾›å•†é…ç½®
    embedding_provider = db.query(EmbeddingProvider).filter(
        EmbeddingProvider.is_default == True
    ).first()
    
    embedding_provider_config = None
    if embedding_provider:
        embedding_provider_config = {
            "provider_type": embedding_provider.provider_type,
            "model_name": embedding_provider.model_name,
            "api_key": embedding_provider.api_key,
            "base_url": embedding_provider.base_url
        }
    
    return {
        "app_config": app_config,
        "fixed_qa_pairs": fixed_qa_pairs,
        "knowledge_bases": knowledge_bases,
        "embedding_provider_config": embedding_provider_config
    }


# APIç«¯ç‚¹
@router.post("/test-retrieval")
@router.post("/test-retrieval/")
async def test_retrieval(
    request: TestRetrievalRequest,
    db: Session = Depends(get_db)
):
    """
    æµ‹è¯•æ£€ç´¢åŠŸèƒ½ï¼ˆä¸è°ƒç”¨LLMï¼Œä»…è¿”å›æ£€ç´¢ç»“æœï¼‰
    ç”¨äºè°ƒè¯•å’ŒéªŒè¯æ£€ç´¢æ•ˆæœ
    """
    # è·å–åº”ç”¨
    app = get_app_by_id(request.application_id, db)
    if not app:
        raise HTTPException(status_code=404, detail="åº”ç”¨ä¸å­˜åœ¨")
    
    if not app.is_active:
        raise HTTPException(status_code=403, detail="åº”ç”¨æœªæ¿€æ´»")
    
    # å‡†å¤‡ä¸Šä¸‹æ–‡
    context = await prepare_app_context(app, db)
    
    # æ‰§è¡Œæ£€ç´¢
    retrieval_result = await hybrid_retrieval_engine.retrieve(
        query=request.query,
        app_config=context["app_config"],
        fixed_qa_pairs=context["fixed_qa_pairs"],
        knowledge_bases=context["knowledge_bases"],
        embedding_provider_config=context["embedding_provider_config"]
    )
    
    # è®°å½•æ£€ç´¢æ—¥å¿—
    if context["app_config"]["enable_source_tracking"]:
        log = RetrievalLog(
            application_id=app.id,
            query=request.query,
            matched_source=retrieval_result.get("matched_source"),
            confidence_score=retrieval_result.get("confidence_score"),
            references=retrieval_result.get("references"),
            final_answer=retrieval_result.get("answer"),
            retrieval_path=retrieval_result.get("retrieval_path"),
            preprocessing_info=retrieval_result.get("preprocessing_info"),
            fusion_details=retrieval_result.get("fusion_details"),
            preprocessing_time_ms=retrieval_result["timing"]["preprocessing_ms"],
            retrieval_time_ms=retrieval_result["timing"]["retrieval_ms"],
            generation_time_ms=0,
            total_time_ms=retrieval_result["timing"]["total_ms"],
            has_suggestions=retrieval_result.get("has_suggestions", False),
            suggestions=retrieval_result.get("suggestions")
        )
        db.add(log)
        db.commit()
    
    return {
        "application": {
            "id": app.id,
            "name": app.name
        },
        "retrieval_result": retrieval_result
    }


@router.post("/apps/{endpoint_path}/chat/completions")
async def app_chat_completion(
    endpoint_path: str,
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None),
    db: Session = Depends(get_db)
):
    """
    åº”ç”¨çº§èŠå¤©è¡¥å…¨APIï¼ˆOpenAIå…¼å®¹ï¼‰
    ä½¿ç”¨æ··åˆæ£€ç´¢+LLMç”Ÿæˆç­”æ¡ˆ
    """
    # éªŒè¯APIå¯†é’¥
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥")
    
    api_key = authorization.replace("Bearer ", "")
    
    # è·å–åº”ç”¨
    app = db.query(Application).filter(
        Application.endpoint_path == endpoint_path,
        Application.api_key == api_key
    ).first()
    
    if not app:
        raise HTTPException(status_code=404, detail="åº”ç”¨ä¸å­˜åœ¨æˆ–APIå¯†é’¥æ— æ•ˆ")
    
    if not app.is_active:
        raise HTTPException(status_code=403, detail="åº”ç”¨æœªæ¿€æ´»")
    
    # æå–ç”¨æˆ·æŸ¥è¯¢
    user_message = next((msg for msg in request.messages if msg.role == "user"), None)
    if not user_message:
        raise HTTPException(status_code=400, detail="è¯·æ±‚ä¸­æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯")
    
    query = user_message.content
    start_time = time.time()
    
    # ç‰¹æ®Šå¤„ç†ï¼šç”¨æˆ·ç›´æ¥é€‰æ‹©äº†æŸä¸ªQ&A
    if request.selected_qa_id:
        qa = db.query(FixedQAPair).filter(
            FixedQAPair.id == request.selected_qa_id,
            FixedQAPair.application_id == app.id
        ).first()
        
        if qa:
            # æ›´æ–°ç‚¹å‡»ç»Ÿè®¡
            qa.hit_count += 1
            qa.last_hit_at = datetime.utcnow()
            db.commit()
            
            logger.info(f"âœ… ç”¨æˆ·ç›´æ¥é€‰æ‹©å›ºå®šQ&A: {qa.question}")
            
            # ç›´æ¥è¿”å›Q&Aç­”æ¡ˆ
            return {
                "id": f"app-{app.id}-qa-{qa.id}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": app.llm_model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": qa.answer,
                        "metadata": {
                            "source": "fixed_qa",
                            "qa_id": qa.id,
                            "question": qa.question,
                            "confidence": 1.0
                        }
                    },
                    "finish_reason": "stop"
                }],
                "usage": {"total_tokens": 0},
                "cbit_metadata": {
                    "source": "fixed_qa",
                    "retrieval_time_ms": 0,
                    "total_time_ms": (time.time() - start_time) * 1000
                }
            }
    
    # å‡†å¤‡ä¸Šä¸‹æ–‡
    context = await prepare_app_context(app, db)
    
    logger.info(f"ğŸ¯ åº”ç”¨ [{app.name}] (æ¨¡å¼: {app.mode}) æ”¶åˆ°æŸ¥è¯¢: {query}")
    logger.info(f"ğŸ“Š åº”ç”¨é…ç½® - å›ºå®šQ&A: {context['app_config']['enable_fixed_qa']}, å‘é‡æ£€ç´¢: {context['app_config']['enable_vector_kb']}, è”ç½‘æœç´¢: {context['app_config']['enable_web_search']}")
    logger.info(f"ğŸ“š å…³è”çŸ¥è¯†åº“æ•°é‡: {len(context['knowledge_bases'])}")
    if context['knowledge_bases']:
        kb_names = [kb['name'] for kb in context['knowledge_bases']]
        logger.info(f"ğŸ“š çŸ¥è¯†åº“åˆ—è¡¨: {', '.join(kb_names)}")
    
    # æ‰§è¡Œæ£€ç´¢ï¼ˆå¦‚æœç”¨æˆ·é€‰æ‹©è·³è¿‡å›ºå®šQ&Aï¼Œåˆ™ä¸æ£€ç´¢å›ºå®šQ&Aï¼‰
    retrieval_start = time.time()
    
    # å¦‚æœç”¨æˆ·é€‰æ‹©"ç»§ç»­æ€è€ƒ"ï¼Œä¸´æ—¶ç¦ç”¨å›ºå®šQ&A
    original_enable_fixed_qa = context["app_config"]["enable_fixed_qa"]
    original_enable_web_search = context["app_config"]["enable_web_search"]
    
    if request.skip_fixed_qa:
        context["app_config"]["enable_fixed_qa"] = False
        context["fixed_qa_pairs"] = []
        logger.info("â­ï¸ ç”¨æˆ·é€‰æ‹©ç»§ç»­æ€è€ƒï¼Œè·³è¿‡å›ºå®šQ&Aæ£€ç´¢")
    
    # ğŸŒ å¦‚æœç”¨æˆ·æˆæƒè”ç½‘æœç´¢ï¼Œå¼ºåˆ¶å¯ç”¨
    if request.force_web_search:
        context["app_config"]["enable_web_search"] = True
        # é™ä½è”ç½‘è§¦å‘é˜ˆå€¼ï¼Œç¡®ä¿ä¼šä½¿ç”¨è”ç½‘ç»“æœ
        if "fusion_config" not in context["app_config"]:
            context["app_config"]["fusion_config"] = {}
        if "strategy" not in context["app_config"]["fusion_config"]:
            context["app_config"]["fusion_config"]["strategy"] = {}
        context["app_config"]["fusion_config"]["strategy"]["web_search_trigger_threshold"] = 0.0
        logger.info("ğŸŒ ç”¨æˆ·æˆæƒè”ç½‘æœç´¢ï¼Œå¼ºåˆ¶å¯ç”¨å¹¶é™ä½è§¦å‘é˜ˆå€¼")
    
    retrieval_result = await hybrid_retrieval_engine.retrieve(
        query=query,
        app_config=context["app_config"],
        fixed_qa_pairs=context["fixed_qa_pairs"],
        knowledge_bases=context["knowledge_bases"],
        embedding_provider_config=context["embedding_provider_config"]
    )
    retrieval_time = (time.time() - retrieval_start) * 1000
    
    # æ¢å¤åŸå§‹é…ç½®
    if request.skip_fixed_qa:
        context["app_config"]["enable_fixed_qa"] = original_enable_fixed_qa
    if request.force_web_search:
        context["app_config"]["enable_web_search"] = original_enable_web_search
    
    # âš ï¸ é‡è¦ï¼šQ&Aç¡®è®¤é€»è¾‘å¿…é¡»åœ¨é«˜ç½®ä¿¡åº¦è¿”å›ä¹‹å‰ï¼
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤Q&Aï¼ˆæ–°é€»è¾‘ï¼‰
    # ç›´æ¥ä»referencesä¸­æå–å›ºå®šQ&Aå»ºè®®
    suggested_questions_for_confirmation = []
    for ref in retrieval_result.get("references", []):
        if ref.get("source_type") == "fixed_qa":
            suggested_questions_for_confirmation.append({
                "question": ref.get("question", ""),
                "similarity": round(ref.get("similarity", 0), 4),
                "qa_id": ref.get("id")
            })
    
    logger.info(f"ğŸ” ç¡®è®¤æ£€æŸ¥: suggested_questions={len(suggested_questions_for_confirmation)}, skip_fixed_qa={request.skip_fixed_qa}, selected_qa_id={request.selected_qa_id}, force_web_search={request.force_web_search}, original_enable_fixed_qa={original_enable_fixed_qa}")
    
    # åªæœ‰åœ¨åŸå§‹å›ºå®šQ&Aå¯ç”¨ä¸”æœ‰å»ºè®®æ—¶æ‰ç¡®è®¤
    # ğŸŒ é‡è¦ï¼šå¦‚æœç”¨æˆ·å·²æˆæƒè”ç½‘æœç´¢ï¼Œè·³è¿‡Q&Aç¡®è®¤
    if suggested_questions_for_confirmation and not request.skip_fixed_qa and not request.selected_qa_id and not request.force_web_search and original_enable_fixed_qa:
        # æ£€æŸ¥æœ€é«˜ç›¸ä¼¼åº¦æ˜¯å¦è¾¾åˆ°å»ºè®®é˜ˆå€¼
        max_similarity = max([q.get('similarity', 0) for q in suggested_questions_for_confirmation]) if suggested_questions_for_confirmation else 0
        
        # ä»fusion_configè·å–å›ºå®šQ&Aé…ç½®
        fusion_config = app.fusion_config or {}
        fixed_qa_config = fusion_config.get("fixed_qa", {})
        suggest_threshold = fixed_qa_config.get("suggest_threshold", 0.75)  # é»˜è®¤0.75
        
        # å¦‚æœç›¸ä¼¼åº¦è¾¾åˆ°å»ºè®®é˜ˆå€¼ï¼Œæ˜¾ç¤ºç¡®è®¤ç•Œé¢
        if max_similarity >= suggest_threshold:
            logger.info(f"ğŸ’¡ æ£€æµ‹åˆ° {len(suggested_questions_for_confirmation)} ä¸ªç›¸å…³Q&Aï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2%}ï¼‰ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤")
            
            return {
                "id": f"app-{app.id}-confirmation",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": app.llm_model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "",  # ç©ºå†…å®¹ï¼Œè¡¨ç¤ºéœ€è¦ç”¨æˆ·ç¡®è®¤
                        "metadata": {
                            "needs_confirmation": True,
                            "source": "fixed_qa_suggestion"
                        }
                    },
                    "finish_reason": "stop"
                }],
                "usage": {"total_tokens": 0},
                "cbit_metadata": {
                    "needs_confirmation": True,
                    "suggested_questions": suggested_questions_for_confirmation[:3],  # æœ€å¤š3ä¸ª
                    "retrieval_time_ms": retrieval_time,
                    "total_time_ms": (time.time() - start_time) * 1000
                }
            }
    
    # ============ ä¼˜åŒ–çš„é˜ˆå€¼åˆ¤æ–­ç³»ç»Ÿ ============
    # è·å–é…ç½®çš„é˜ˆå€¼
    min_threshold = app.similarity_threshold_low    # æœ€ä½é˜ˆå€¼ï¼ˆå¦‚0.3ï¼‰
    high_threshold = app.similarity_threshold_high  # é«˜é˜ˆå€¼ï¼ˆå»ºè®®è®¾ç½®ä¸º0.9ï¼Œæé«˜ç½®ä¿¡åº¦æ‰ç›´æ¥è¿”å›ï¼‰
    confidence = retrieval_result.get("confidence_score", 0)
    enable_polish = app.enable_llm_polish if app.enable_llm_polish is not None else True  # é»˜è®¤å¯ç”¨æ¶¦è‰²
    
    logger.info(f"ğŸ¯ ç½®ä¿¡åº¦è¯„ä¼°: {confidence:.2%} (æœ€ä½é˜ˆå€¼: {min_threshold:.2%}, é«˜é˜ˆå€¼: {high_threshold:.2%}, LLMæ¶¦è‰²: {'âœ…' if enable_polish else 'âŒ'})")
    
    # ğŸ’¬ ç‰¹æ®Šå¤„ç†ï¼šæ—¥å¸¸å¯¹è¯æ£€æµ‹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    is_casual = is_casual_conversation(query)
    
    # å¦‚æœæ˜¯æ—¥å¸¸å¯¹è¯ï¼Œç›´æ¥ä½¿ç”¨LLMå›ç­”ï¼Œä¸æ˜¾ç¤ºä½ç½®ä¿¡æç¤º
    if is_casual:
        logger.info(f"ğŸ’¬ æ£€æµ‹åˆ°æ—¥å¸¸å¯¹è¯ï¼Œè·³è¿‡ç½®ä¿¡åº¦æ£€æŸ¥ï¼Œç›´æ¥ä½¿ç”¨LLMè‡ªç„¶å›ç­”")
        # ç»§ç»­åˆ°ä¸‹é¢çš„"åœºæ™¯1.5"å¤„ç†
    
    # åœºæ™¯1ï¼šæä½ç½®ä¿¡åº¦ - æ ¹æ®ç­–ç•¥æ¨¡å¼å¤„ç†
    # ä½†å¦‚æœæ˜¯æ—¥å¸¸å¯¹è¯ï¼Œåˆ™è·³è¿‡æ­¤é€»è¾‘
    elif confidence < min_threshold:
        # ğŸ†• è·å–ç­–ç•¥æ¨¡å¼
        strategy_mode = context["app_config"].get("strategy_mode", "safe_priority")
        
        # ğŸ›¡ï¸ å®‰å…¨ä¼˜å…ˆæ¨¡å¼ - æç¤ºç”¨æˆ·æˆæƒè”ç½‘
        if strategy_mode == "safe_priority" and context["app_config"]["enable_web_search"]:
            logger.info(f"ğŸ›¡ï¸ å®‰å…¨ä¼˜å…ˆæ¨¡å¼ + ä½ç½®ä¿¡åº¦ ({confidence:.2%} < {min_threshold:.2%})ï¼Œæç¤ºç”¨æˆ·æˆæƒè”ç½‘")
            return {
                "id": f"app-{app.id}-web-search-auth",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": app.llm_model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "âš ï¸ è¿™ä¸ªé—®é¢˜ä¸åœ¨æˆ‘çš„çŸ¥è¯†èŒƒå›´å†…ï¼Œæ˜¯å¦æ„¿æ„å¼€å¯è”ç½‘æ¨¡å—è¿›è¡Œé—®é¢˜å›ç­”ï¼Ÿ\n\nğŸ“¢ æç¤ºï¼šè”ç½‘æœç´¢ç»“æœæ¥æºäºç½‘ç»œï¼Œä»…ä¾›å‚è€ƒã€‚",
                        "web_search_prompt": True  # ç‰¹æ®Šæ ‡è®°ï¼Œå‰ç«¯å¯ä»¥æ˜¾ç¤º"å°è¯•è”ç½‘æœç´¢"æŒ‰é’®
                    },
                    "finish_reason": "web_search_auth_required"
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                },
                "metadata": {
                    "confidence": round(confidence, 4),
                    "source": "low_confidence_prompt",
                    "strategy_mode": strategy_mode,
                    "requires_web_search_auth": True
                }
            }
        
        # ğŸŒ å®æ—¶çŸ¥è¯†æ¨¡å¼ - ç›´æ¥æ‹’ç»å›ç­”
        elif strategy_mode == "realtime_knowledge":
            logger.info(f"ğŸŒ å®æ—¶çŸ¥è¯†æ¨¡å¼ + ä½ç½®ä¿¡åº¦ ({confidence:.2%} < {min_threshold:.2%})ï¼Œç›´æ¥æ‹’ç»å›ç­”")
            answer = "ğŸš« æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å‡†ç¡®å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
            source = "auto_reject"
        
        # å…¶ä»–æƒ…å†µï¼šä½¿ç”¨è‡ªå®šä¹‰å›å¤æˆ–é»˜è®¤æç¤º
        elif app.enable_custom_no_result_response and app.custom_no_result_response:
            logger.info(f"ğŸ“¢ ç½®ä¿¡åº¦è¿‡ä½ ({confidence:.2%} < {min_threshold:.2%})ï¼Œè¿”å›è‡ªå®šä¹‰å›å¤")
            answer = app.custom_no_result_response
            source = "no_result"
        else:
            # æœªé…ç½®è‡ªå®šä¹‰å›å¤ï¼Œä½¿ç”¨é»˜è®¤æç¤º
            logger.warning(f"âš ï¸ ç½®ä¿¡åº¦è¿‡ä½ä½†æœªé…ç½®è‡ªå®šä¹‰å›å¤ï¼Œä½¿ç”¨é»˜è®¤æç¤º")
            answer = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å‡†ç¡®å›ç­”è¿™ä¸ªé—®é¢˜ã€‚å»ºè®®æ‚¨è”ç³»äººå·¥å®¢æœè·å–å¸®åŠ©ã€‚"
            source = "no_result"
        tokens_used = 0
        generation_time = 0
    
    # åœºæ™¯1.5ï¼šæ—¥å¸¸å¯¹è¯ - è®©LLMè‡ªç„¶å›ç­”ï¼ˆä¸ä½¿ç”¨çŸ¥è¯†åº“ï¼‰
    if is_casual:
        logger.info(f"ğŸ’¬ æ£€æµ‹åˆ°æ—¥å¸¸å¯¹è¯ï¼Œä½¿ç”¨LLMè‡ªç„¶å›ç­”")
        generation_start = time.time()
        
        # ä½¿ç”¨ç®€å•çš„ç³»ç»Ÿpromptï¼Œä¸æ·»åŠ çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€è‡ªç„¶çš„AIåŠ©æ‰‹ï¼ŒåƒçœŸäººä¸€æ ·ä¸ç”¨æˆ·äº¤æµã€‚"
        
        enhanced_messages = [
            ChatMessage(role="system", content=system_prompt)
        ] + request.messages
        
        try:
            # ä»æ•°æ®åº“åŠ è½½AIæä¾›å•†çš„APIå¯†é’¥
            ai_provider_obj = db.query(EmbeddingProvider).filter(
                EmbeddingProvider.provider_type == app.ai_provider
            ).first()
            
            if not ai_provider_obj or not ai_provider_obj.api_key:
                raise HTTPException(
                    status_code=400, 
                    detail=f"åº”ç”¨é…ç½®çš„AIæä¾›å•† '{app.ai_provider}' æœªè®¾ç½®APIå¯†é’¥"
                )
            
            multi_model_engine.set_api_key(app.ai_provider, ai_provider_obj.api_key)
            if ai_provider_obj.base_url:
                multi_model_engine.set_custom_config(app.ai_provider, {
                    "base_url": ai_provider_obj.base_url
                })
            
            llm_response = await multi_model_engine.chat_completion(
                provider=app.ai_provider,
                model=app.llm_model,
                messages=[{"role": msg.role, "content": msg.content} for msg in enhanced_messages],
                stream=request.stream,
                temperature=request.temperature or app.temperature or 0.7,
                max_tokens=request.max_tokens or app.max_tokens or 500
            )
            
            answer = llm_response["choices"][0]["message"]["content"]
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            source = "llm_casual"
            generation_time = (time.time() - generation_start) * 1000
            logger.info(f"âœ… æ—¥å¸¸å¯¹è¯LLMç”ŸæˆæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ LLMç”Ÿæˆå¤±è´¥: {e}")
            answer = "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚ğŸ˜Š"
            source = "llm_casual"
            tokens_used = 0
            generation_time = (time.time() - generation_start) * 1000
    
    # åœºæ™¯2ï¼šæé«˜ç½®ä¿¡åº¦ + ç¦ç”¨æ¶¦è‰² = ç›´æ¥è¿”å›æ£€ç´¢ç»“æœï¼ˆæ›´åƒçœŸäººçŸ¥è¯†åˆ†äº«ï¼‰
    elif confidence >= high_threshold and not enable_polish and retrieval_result.get("answer"):
        logger.info(f"ğŸ“Š æé«˜ç½®ä¿¡åº¦ ({confidence:.2%} >= {high_threshold:.2%}) ä¸”ç¦ç”¨æ¶¦è‰²ï¼Œç›´æ¥è¿”å›æ£€ç´¢ç»“æœ")
        answer = retrieval_result["answer"]
        source = "retrieval"
        tokens_used = 0
        generation_time = 0
    
    # åœºæ™¯3ï¼šå…¶ä»–æƒ…å†µï¼ˆåŒ…æ‹¬å¯ç”¨æ¶¦è‰²æˆ–ä¸­ç­‰ç½®ä¿¡åº¦ï¼‰- LLM+çŸ¥è¯†åº“ç»“åˆè¾“å‡º
    else:
        if enable_polish:
            logger.info(f"ğŸ¤– å¯ç”¨LLMæ¶¦è‰²æ¨¡å¼ï¼Œè°ƒç”¨LLMç»“åˆçŸ¥è¯†åº“ç”Ÿæˆæ›´è‡ªç„¶çš„ç­”æ¡ˆï¼ˆç½®ä¿¡åº¦: {confidence:.2%}ï¼‰")
        else:
            logger.info(f"ğŸ¤– ä¸­ç­‰ç½®ä¿¡åº¦ ({min_threshold:.2%} <= {confidence:.2%} < {high_threshold:.2%})ï¼Œè°ƒç”¨LLMç»“åˆçŸ¥è¯†åº“ç”Ÿæˆç­”æ¡ˆ")
        generation_start = time.time()
        
        # æ„å»ºå¢å¼ºçš„prompt
        system_prompt = app.system_prompt or "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
        
        # æ·»åŠ æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        context_text = ""
        if retrieval_result.get("answer"):
            context_text = f"\n\nç›¸å…³ä¿¡æ¯ï¼š\n{retrieval_result['answer']}"
        
        if retrieval_result.get("references"):
            context_text += "\n\nå‚è€ƒæ¥æºï¼š"
            for i, ref in enumerate(retrieval_result["references"][:3], 1):
                if ref["source_type"] == "fixed_qa":
                    context_text += f"\n{i}. [å›ºå®šQ&A] {ref.get('question', '')}"
                elif ref["source_type"] == "kb":
                    context_text += f"\n{i}. [çŸ¥è¯†åº“: {ref.get('kb_name', '')}]"
        
        # æ·»åŠ æŒ‡å¯¼æ€§æç¤ºï¼Œé¿å…ç¼–é€ 
        if confidence < (min_threshold + high_threshold) / 2:  # ç½®ä¿¡åº¦åä½æ—¶
            context_text += "\n\næ³¨æ„ï¼šå¦‚æœæä¾›çš„ä¿¡æ¯ä¸è¶³ä»¥å‡†ç¡®å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®å‘ŠçŸ¥ç”¨æˆ·ä¿¡æ¯ä¸è¶³ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚"
        
        enhanced_messages = [
            ChatMessage(role="system", content=system_prompt + context_text)
        ] + request.messages
        
        # è°ƒç”¨LLMå‰ï¼Œå…ˆåŠ è½½APIå¯†é’¥
        try:
            # ä»æ•°æ®åº“åŠ è½½AIæä¾›å•†çš„APIå¯†é’¥ï¼ˆå­˜å‚¨åœ¨EmbeddingProviderè¡¨ä¸­ï¼‰
            ai_provider_obj = db.query(EmbeddingProvider).filter(
                EmbeddingProvider.provider_type == app.ai_provider
            ).first()
            
            if not ai_provider_obj or not ai_provider_obj.api_key:
                logger.error(f"âŒ åº”ç”¨é…ç½®çš„AIæä¾›å•† '{app.ai_provider}' æœªè®¾ç½®APIå¯†é’¥")
                raise HTTPException(
                    status_code=400, 
                    detail=f"åº”ç”¨é…ç½®çš„AIæä¾›å•† '{app.ai_provider}' æœªè®¾ç½®APIå¯†é’¥ï¼Œè¯·å…ˆåœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®ã€‚"
                )
            
            # è®¾ç½®APIå¯†é’¥åˆ°multi_model_engine
            multi_model_engine.set_api_key(app.ai_provider, ai_provider_obj.api_key)
            if ai_provider_obj.base_url:
                multi_model_engine.set_custom_config(app.ai_provider, {
                    "base_url": ai_provider_obj.base_url
                })
            
            logger.info(f"ğŸ”‘ å·²åŠ è½½ {app.ai_provider} APIå¯†é’¥")
            
            llm_response = await multi_model_engine.chat_completion(
                provider=app.ai_provider,
                model=app.llm_model,
                messages=[{"role": msg.role, "content": msg.content} for msg in enhanced_messages],
                stream=request.stream,
                temperature=request.temperature or app.temperature,
                max_tokens=request.max_tokens or app.max_tokens
            )
            
            answer = llm_response["choices"][0]["message"]["content"]
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            source = "llm"
            logger.info(f"âœ… LLMç”ŸæˆæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ LLMç”Ÿæˆå¤±è´¥: {e}")
            
            # LLMå¤±è´¥æ—¶çš„é™çº§ç­–ç•¥
            if app.enable_custom_no_result_response and app.custom_no_result_response:
                # æœ‰è‡ªå®šä¹‰å›å¤ï¼Œä½¿ç”¨è‡ªå®šä¹‰å›å¤
                logger.info(f"ğŸ“¢ LLMå¤±è´¥ï¼Œä½¿ç”¨è‡ªå®šä¹‰å›å¤")
                answer = app.custom_no_result_response
                source = "no_result"
            elif retrieval_result.get("answer") and confidence >= min_threshold:
                # æœ‰æ£€ç´¢ç»“æœä¸”ç½®ä¿¡åº¦ä¸æ˜¯å¤ªä½ï¼Œä½¿ç”¨æ£€ç´¢ç»“æœ
                logger.info(f"ğŸ“‹ LLMå¤±è´¥ï¼Œä½¿ç”¨æ£€ç´¢ç»“æœ")
                answer = retrieval_result["answer"]
                source = "retrieval"
            else:
                # ä»€ä¹ˆéƒ½æ²¡æœ‰ï¼Œè¿”å›é»˜è®¤æç¤º
                logger.warning(f"âš ï¸ LLMå¤±è´¥ä¸”æ— å¯ç”¨ç»“æœ")
                answer = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»äººå·¥å®¢æœã€‚"
                source = "no_result"
            
            tokens_used = 0
        
        generation_time = (time.time() - generation_start) * 1000
    
    total_time = (time.time() - start_time) * 1000
    
    # æ›´æ–°åº”ç”¨ç»Ÿè®¡
    app.total_requests += 1
    app.total_tokens += tokens_used if 'tokens_used' in locals() else 0
    db.commit()
    
    # è®°å½•æ£€ç´¢æ—¥å¿—
    if app.enable_source_tracking:
        log = RetrievalLog(
            application_id=app.id,
            query=query,
            matched_source=retrieval_result.get("matched_source"),
            confidence_score=confidence,
            references=retrieval_result.get("references"),
            final_answer=answer,
            retrieval_path=retrieval_result.get("retrieval_path"),
            preprocessing_info=retrieval_result.get("preprocessing_info"),
            fusion_details=retrieval_result.get("fusion_details"),
            preprocessing_time_ms=retrieval_result["timing"]["preprocessing_ms"],
            retrieval_time_ms=retrieval_time,
            generation_time_ms=generation_time if 'generation_time' in locals() else 0,
            total_time_ms=total_time,
            tokens_used=tokens_used if 'tokens_used' in locals() else 0,
            has_suggestions=retrieval_result.get("has_suggestions", False),
            suggestions=retrieval_result.get("suggestions")
        )
        db.add(log)
        db.commit()
    
    # æå–å›ºå®šQ&Aå»ºè®®é—®é¢˜
    suggested_questions = []
    matched_fixed_qa = False
    match_confidence = 0.0
    
    # æ£€æŸ¥æ£€ç´¢ç»“æœä¸­çš„å›ºå®šQ&A
    for result in retrieval_result.get("retrieval_path", []):
        if result.get("source") == "fixed_qa":
            matched_fixed_qa = True
            break
    
    # ä»referencesä¸­æå–å»ºè®®é—®é¢˜
    for ref in retrieval_result.get("references", []):
        if ref.get("source_type") == "fixed_qa":
            match_confidence = max(match_confidence, ref.get("similarity", 0))
            # æ£€æŸ¥æ˜¯å¦æ˜¯å»ºè®®ç±»å‹ï¼ˆå¯ä»¥ä»æ£€ç´¢ç»“æœçš„metadataä¸­è·å–ï¼‰
            suggested_questions.append({
                "question": ref.get("question", ""),
                "similarity": round(ref.get("similarity", 0), 4),
                "qa_id": ref.get("id")
            })
    
    # OpenAIå…¼å®¹æ ¼å¼å“åº”
    response = {
        "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": app.llm_model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": answer
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": tokens_used if 'tokens_used' in locals() else 0
        },
        # CBIT Forgeæ‰©å±•å­—æ®µï¼ˆä¾›å¤–éƒ¨å‰ç«¯ä½¿ç”¨ï¼‰
        "cbit_metadata": {
            "matched_fixed_qa": matched_fixed_qa,
            "match_confidence": round(match_confidence, 4),
            "suggested_questions": suggested_questions[:3],  # æœ€å¤š3ä¸ªå»ºè®®
            "retrieval_sources": {
                "fixed_qa": context["app_config"]["enable_fixed_qa"] and matched_fixed_qa,
                "vector_kb": context["app_config"]["enable_vector_kb"] and retrieval_result.get("matched_source") == "kb",
                "web_search": context["app_config"]["enable_web_search"]
            },
            "timing": {
                "retrieval_ms": round(retrieval_time, 2),
                "generation_ms": round(generation_time, 2) if 'generation_time' in locals() else 0,
                "total_ms": round(total_time, 2)
            }
        }
    }
    
    # å¦‚æœå¯ç”¨æ¥æºè¿½æº¯ï¼Œæ·»åŠ é¢å¤–çš„å†…éƒ¨metadata
    if app.enable_source_tracking:
        # ç¡®å®šä¸»è¦æ¥æºçš„æ˜¾ç¤ºåç§°
        source_display_map = {
            "retrieval": "æ£€ç´¢ç»“æœ",
            "llm": "AIç”Ÿæˆ",
            "llm_casual": "æ—¥å¸¸å¯¹è¯",  # æ–°å¢ï¼šæ—¥å¸¸å¯¹è¯æ ‡è®°
            "fallback": "å›é€€æœºåˆ¶",
            "no_result": "æœªè¾¾é˜ˆå€¼"
        }
        
        # ğŸ”‘ å…³é”®ï¼šå¦‚æœæ˜¯æ—¥å¸¸å¯¹è¯ï¼Œæ¸…ç©ºæ‰€æœ‰å¼•ç”¨ä¿¡æ¯
        if source == "llm_casual":
            logger.info("ğŸ’¬ æ—¥å¸¸å¯¹è¯ï¼šæ¸…ç©ºå¼•ç”¨æ¥æºå’Œç­–ç•¥ä¿¡æ¯")
            response["choices"][0]["message"]["metadata"] = {
                "source": source,
                "source_display": "æ—¥å¸¸å¯¹è¯",
                "retrieval_confidence": None,  # ä¸æ˜¾ç¤ºç½®ä¿¡åº¦
                "matched_source": None,
                "matched_source_display": None,
                "retrieval_path": [],
                "references": [],  # æ¸…ç©ºå¼•ç”¨
                "suggestions": [],
                "_strategy_info": None  # ä¸ä¼ é€’ç­–ç•¥ä¿¡æ¯
            }
        else:
            # éæ—¥å¸¸å¯¹è¯ï¼šæ­£å¸¸å¤„ç†
            matched_source = retrieval_result.get("matched_source")
            matched_source_display = {
                "fixed_qa": "å›ºå®šQ&A",
                "kb": "å‘é‡çŸ¥è¯†åº“",
                "web": "è”ç½‘æœç´¢",
                "tavily_answer": "Cbit AI æœç´¢",
                "tavily_web": "Cbit AI æœç´¢",
                None: "æœªåŒ¹é…"
            }.get(matched_source, matched_source or "æœªçŸ¥")
            
            # åªæœ‰éæ—¥å¸¸å¯¹è¯æ‰ä¼ é€’ç­–ç•¥ä¿¡æ¯
            strategy_info = retrieval_result.get("_strategy_info")
            
            response["choices"][0]["message"]["metadata"] = {
                "source": source,
                "source_display": source_display_map.get(source, source),
                "retrieval_confidence": confidence,
                "matched_source": matched_source,
                "matched_source_display": matched_source_display,
                "retrieval_path": retrieval_result.get("retrieval_path", []),
                "references": retrieval_result.get("references") if app.enable_citation else [],
                "suggestions": retrieval_result.get("suggestions") if retrieval_result.get("has_suggestions") else [],
                "_strategy_info": strategy_info
            }
        
        # åœ¨cbit_metadataä¸­ä¹Ÿæ·»åŠ æ›´è¯¦ç»†çš„æ¥æºä¿¡æ¯
        # ä½†å¦‚æœæ˜¯æ—¥å¸¸å¯¹è¯ï¼Œç®€åŒ–ä¿¡æ¯
        if source == "llm_casual":
            response["cbit_metadata"]["source_info"] = {
                "primary_source": "æ—¥å¸¸å¯¹è¯",
                "retrieval_source": None,
                "confidence": None,
                "confidence_level": None
            }
        else:
            response["cbit_metadata"]["source_info"] = {
                "primary_source": source_display_map.get(source, source),
                "retrieval_source": matched_source_display,
                "confidence": round(confidence, 4),
                "confidence_level": _get_confidence_level_display(confidence)
            }
    
    return response


def _get_confidence_level_display(confidence: float) -> str:
    """æ ¹æ®ç½®ä¿¡åº¦è¿”å›æ˜¾ç¤ºæ–‡æœ¬"""
    if confidence >= 0.90:
        return "æé«˜"
    elif confidence >= 0.80:
        return "é«˜"
    elif confidence >= 0.70:
        return "ä¸­ç­‰"
    elif confidence >= 0.60:
        return "è¾ƒä½"
    else:
        return "ä½"
