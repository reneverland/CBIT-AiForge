"""
æ¨ç†æœåŠ¡ API - å…¼å®¹ OpenAI æ ¼å¼
"""

from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from loguru import logger
import json
import time
from datetime import datetime

from app.models.database import get_db, Model as ModelModel, InferenceLog
from app.core.rag_engine import RAGEngine
from app.models.database import KnowledgeBase as KnowledgeBaseModel
from app.core.multi_model_engine import multi_model_engine

router = APIRouter()
rag_engine = RAGEngine()


class Message(BaseModel):
    role: str  # system, user, assistant
    content: str


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 2000
    stream: bool = False
    knowledge_base: Optional[str] = None  # çŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰ï¼‰
    n_results: int = 3  # RAG æ£€ç´¢æ•°é‡
    provider: Optional[str] = None  # AIæä¾›å•†ï¼ˆopenai, deepseekç­‰ï¼‰


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]


@router.post("/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    db: Session = Depends(get_db)
):
    """
    OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æ¥å£
    
    æ”¯æŒï¼š
    - æ™®é€šæ¨¡å‹æ¨ç†
    - RAG æ£€ç´¢å¢å¼ºï¼ˆæŒ‡å®š knowledge_baseï¼‰
    - æµå¼è¾“å‡ºï¼ˆstream=trueï¼‰
    """
    try:
        start_time = time.time()
        
        # è·å–ç”¨æˆ·æ¶ˆæ¯
        if not request.messages:
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        user_message = request.messages[-1].content
        
        # RAG æ£€ç´¢ï¼ˆå¦‚æœæŒ‡å®šäº†çŸ¥è¯†åº“ï¼‰
        context = ""
        if request.knowledge_base:
            kb = db.query(KnowledgeBaseModel).filter(
                KnowledgeBaseModel.name == request.knowledge_base
            ).first()
            
            if kb:
                results = rag_engine.query(
                    collection_name=kb.collection_name,
                    query_text=user_message,
                    n_results=request.n_results
                )
                
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([
                    f"å‚è€ƒæ–‡æ¡£ {i+1}:\n{doc}"
                    for i, doc in enumerate(results["documents"])
                ])
                
                # å°†ä¸Šä¸‹æ–‡æ’å…¥åˆ°ç³»ç»Ÿæç¤ºä¸­
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
                
                request.messages = [
                    Message(role="system", content=system_prompt),
                    Message(role="user", content=user_message)
                ]
        
        # ä½¿ç”¨å¤šæ¨¡å‹å¼•æ“è¿›è¡Œæ¨ç†
        if request.provider and request.provider in multi_model_engine.api_keys:
            # ä½¿ç”¨æŒ‡å®šçš„AIæä¾›å•†
            logger.info(f"ğŸš€ ä½¿ç”¨ {request.provider} æä¾›å•†è¿›è¡Œæ¨ç†: {request.model}")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages_dict = [{"role": msg.role, "content": msg.content} for msg in request.messages]
            
            try:
                ai_response = await multi_model_engine.chat_completion(
                    provider=request.provider,
                    model=request.model,
                    messages=messages_dict,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    stream=False
                )
                
                response_text = ai_response["choices"][0]["message"]["content"]
                logger.info(f"âœ… æ¨ç†æˆåŠŸ: {len(response_text)} å­—ç¬¦")
                
            except Exception as e:
                logger.error(f"âŒ AIæä¾›å•†æ¨ç†å¤±è´¥: {e}")
                # å›é€€åˆ°æ¨¡æ‹Ÿå“åº”
                response_text = f"""[æ¨ç†å¤±è´¥]

æŠ±æ­‰ï¼Œè°ƒç”¨ {request.provider} API æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}

è¯·æ£€æŸ¥ï¼š
1. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ
2. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
3. APIé¢åº¦æ˜¯å¦å……è¶³"""
        else:
            # æ¨¡æ‹Ÿæ¨ç†ï¼ˆç”¨äºæœ¬åœ°æ¨¡å‹æˆ–æœªé…ç½®æä¾›å•†ï¼‰
            logger.info(f"ğŸ“‹ ä½¿ç”¨æ¨¡æ‹Ÿå“åº”: {request.model}")
            response_text = f"""[cbitXForge æ¨¡æ‹Ÿå“åº”]

é—®é¢˜: {user_message}

è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„å›å¤ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ï¼š
- å¾®è°ƒåçš„æ¨¡å‹ï¼ˆå¦‚ {request.model}ï¼‰
- æˆ–åŸºåº§æ¨¡å‹è¿›è¡Œæ¨ç†

{'ä½¿ç”¨äº† RAG æ£€ç´¢å¢å¼ºï¼ŒåŸºäºç›¸å…³æ–‡æ¡£ç”Ÿæˆå›ç­”ã€‚' if request.knowledge_base else 'ç›´æ¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚'}

æç¤ºï¼šè¯·åœ¨"AIæä¾›å•†é…ç½®"é¡µé¢é…ç½®APIå¯†é’¥ï¼Œç„¶åé€‰æ‹©å¯¹åº”çš„æ¨¡å‹å³å¯è¿›è¡ŒçœŸå®æ¨ç†ã€‚

ç‰ˆæƒæ‰€æœ‰ Â© 2025 Reneverland, CBIT, CUHK"""
        
        # è®°å½•æ¨ç†æ—¥å¿—
        log = InferenceLog(
            prompt=user_message,
            response=response_text,
            tokens_used=len(user_message) + len(response_text),
            latency_ms=(time.time() - start_time) * 1000,
            metadata={
                "model": request.model,
                "provider": request.provider,
                "knowledge_base": request.knowledge_base,
                "temperature": request.temperature,
            }
        )
        db.add(log)
        db.commit()
        
        # æ„å»ºå“åº”
        response = ChatCompletionResponse(
            id=f"chatcmpl-{int(time.time())}",
            created=int(time.time()),
            model=request.model,
            choices=[
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }
            ],
            usage={
                "prompt_tokens": len(user_message) // 4,
                "completion_tokens": len(response_text) // 4,
                "total_tokens": (len(user_message) + len(response_text)) // 4
            }
        )
        
        if request.stream:
            # æµå¼è¾“å‡º
            async def generate():
                for chunk in response_text.split():
                    yield f"data: {json.dumps({'choices': [{'delta': {'content': chunk + ' '}}]})}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(generate(), media_type="text/event-stream")
        
        return response
        
    except Exception as e:
        logger.error(f"æ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/models")
async def list_available_models(db: Session = Depends(get_db)):
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ï¼ˆOpenAI å…¼å®¹ï¼‰"""
    models = db.query(ModelModel).filter(ModelModel.status == "active").all()
    
    return {
        "object": "list",
        "data": [
            {
                "id": model.name,
                "object": "model",
                "created": int(model.created_at.timestamp()) if model.created_at else 0,
                "owned_by": "cbit_forge"
            }
            for model in models
        ]
    }


@router.get("/health")
async def inference_health():
    """æ¨ç†æœåŠ¡å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "inference",
        "timestamp": datetime.utcnow().isoformat()
    }

