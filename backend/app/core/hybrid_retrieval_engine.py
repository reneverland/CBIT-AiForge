"""
æ··åˆæ£€ç´¢å¼•æ“ - æ ¸å¿ƒæ£€ç´¢ç³»ç»Ÿ
æ”¯æŒå›ºå®šQ&Aã€å‘é‡æ£€ç´¢ã€å®æ—¶æœç´¢çš„å¤šæºèåˆ
"""

from typing import List, Dict, Any, Optional, Tuple
from loguru import logger
from datetime import datetime
import time

from app.core.embedding_engine import embedding_engine
from app.core.rag_engine import rag_engine
from app.core.accurate_priority_strategy import accurate_priority_strategy


class HybridRetrievalEngine:
    """
    æ··åˆæ£€ç´¢å¼•æ“
    å®ç°ä¼˜å…ˆçº§è·¯ç”±ã€åŒé˜ˆå€¼ç­–ç•¥ã€æƒé‡èåˆç­‰é«˜çº§æ£€ç´¢åŠŸèƒ½
    """
    
    def __init__(self):
        pass
    
    async def retrieve(
        self,
        query: str,
        app_config: Dict[str, Any],
        fixed_qa_pairs: List[Dict[str, Any]],
        knowledge_bases: List[Dict[str, Any]],
        embedding_provider_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ··åˆæ£€ç´¢ä¸»å…¥å£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            app_config: åº”ç”¨é…ç½®
            fixed_qa_pairs: å›ºå®šQ&Aå¯¹åˆ—è¡¨
            knowledge_bases: çŸ¥è¯†åº“åˆ—è¡¨
            embedding_provider_config: Embeddingæä¾›å•†é…ç½®
        
        Returns:
            æ£€ç´¢ç»“æœï¼ŒåŒ…å«åŒ¹é…æ¥æºã€å†…å®¹ã€ç›¸ä¼¼åº¦ç­‰ä¿¡æ¯
        """
        start_time = time.time()
        
        # é¢„å¤„ç†
        preprocessing_start = time.time()
        preprocessing_info = await self._preprocess_query(query, app_config)
        preprocessing_time = (time.time() - preprocessing_start) * 1000
        
        # æ£€ç´¢è·¯å¾„è¿½è¸ª
        retrieval_path = []
        all_results = []
        
        # 1. å›ºå®šQ&Aæ£€ç´¢ï¼ˆç¬¬ä¸€ä¼˜å…ˆçº§ï¼‰
        if app_config.get("enable_fixed_qa", False) and fixed_qa_pairs:
            retrieval_start = time.time()
            fixed_qa_results = await self._search_fixed_qa(
                query,
                fixed_qa_pairs,
                app_config,
                embedding_provider_config
            )
            retrieval_time = (time.time() - retrieval_start) * 1000
            
            if fixed_qa_results:
                retrieval_path.append({
                    "source": "fixed_qa",
                    "time_ms": retrieval_time,
                    "results_count": len(fixed_qa_results)
                })
                
                for result in fixed_qa_results:
                    result["source"] = "fixed_qa"
                    result["weighted_score"] = result["similarity"] * app_config.get("fixed_qa_weight", 1.0)
                    all_results.append(result)
        
        # 2. å‘é‡çŸ¥è¯†åº“æ£€ç´¢
        if app_config.get("enable_vector_kb", False) and knowledge_bases:
            retrieval_start = time.time()
            kb_results = await self._search_knowledge_bases(
                query,
                knowledge_bases,
                app_config,
                embedding_provider_config
            )
            retrieval_time = (time.time() - retrieval_start) * 1000
            
            if kb_results:
                retrieval_path.append({
                    "source": "vector_kb",
                    "time_ms": retrieval_time,
                    "results_count": len(kb_results)
                })
                
                for result in kb_results:
                    result["source"] = "kb"
                    result["weighted_score"] = result["similarity"] * app_config.get("vector_kb_weight", 1.0)
                    all_results.append(result)
        
        # 3. æ™ºèƒ½è”ç½‘æœç´¢ï¼ˆæ ¹æ®ç­–ç•¥æ¨¡å¼å’Œé˜ˆå€¼è§¦å‘ï¼‰
        should_trigger_web_search = False
        if app_config.get("enable_web_search", False):
            # è·å–è§¦å‘é˜ˆå€¼ï¼ˆä»fusion_configæˆ–ç›´æ¥ä»app_configï¼‰
            web_search_trigger_threshold = 0.70  # é»˜è®¤å€¼
            if app_config.get("fusion_config") and app_config["fusion_config"].get("strategy"):
                web_search_trigger_threshold = app_config["fusion_config"]["strategy"].get("web_search_trigger_threshold", 0.70)
            
            # è·å–ç­–ç•¥æ¨¡å¼
            strategy_mode = app_config.get("strategy_mode", "safe_priority")
            
            # ğŸ”‘ å…³é”®ï¼šå¦‚æœé˜ˆå€¼è®¾ç½®ä¸º0.0ï¼Œè¯´æ˜æ˜¯å¼ºåˆ¶è”ç½‘æœç´¢ï¼ˆforce_web_searchï¼Œç”¨æˆ·æ˜ç¡®æˆæƒï¼‰
            if web_search_trigger_threshold == 0.0:
                should_trigger_web_search = True
                logger.info(f"ğŸŒ ç”¨æˆ·æ˜ç¡®æˆæƒå¼ºåˆ¶è”ç½‘æœç´¢ï¼ˆé˜ˆå€¼=0.0ï¼‰")
            else:
                # æ£€æŸ¥ç°æœ‰ç»“æœçš„æœ€é«˜ç½®ä¿¡åº¦
                max_confidence = 0.0
                if all_results:
                    max_confidence = max(result.get("similarity", 0) for result in all_results)
                    logger.info(f"ğŸ“Š å½“å‰æœ€é«˜ç½®ä¿¡åº¦: {max_confidence:.2f}, ç­–ç•¥æ¨¡å¼: {strategy_mode}")
                
                # ğŸ›¡ï¸ å®‰å…¨ä¼˜å…ˆæ¨¡å¼ï¼šä¸è‡ªåŠ¨è§¦å‘è”ç½‘ï¼Œç•™ç»™ app_inference æç¤ºç”¨æˆ·æˆæƒ
                if strategy_mode == "safe_priority":
                    logger.info(f"ğŸ›¡ï¸ å®‰å…¨ä¼˜å…ˆæ¨¡å¼ï¼Œä¸è‡ªåŠ¨è§¦å‘è”ç½‘ï¼ˆå°†åœ¨ä½ç½®ä¿¡åº¦æ—¶æç¤ºç”¨æˆ·æˆæƒï¼‰")
                    should_trigger_web_search = False
                
                # ğŸŒ å®æ—¶çŸ¥è¯†æ¨¡å¼ï¼šæ ¹æ®è‡ªåŠ¨è”ç½‘é˜ˆå€¼è§¦å‘
                elif strategy_mode == "realtime_knowledge":
                    web_search_auto_threshold = app_config.get("web_search_auto_threshold", 0.50)
                    if max_confidence < web_search_auto_threshold:
                        should_trigger_web_search = True
                        logger.info(f"ğŸŒ å®æ—¶çŸ¥è¯†æ¨¡å¼ï¼Œè‡ªåŠ¨è§¦å‘è”ç½‘ (æœ€é«˜ç½®ä¿¡åº¦ {max_confidence:.2f} < è‡ªåŠ¨é˜ˆå€¼ {web_search_auto_threshold})")
                    else:
                        logger.info(f"âœ‹ å®æ—¶çŸ¥è¯†æ¨¡å¼ï¼Œè·³è¿‡è”ç½‘ (æœ€é«˜ç½®ä¿¡åº¦ {max_confidence:.2f} >= è‡ªåŠ¨é˜ˆå€¼ {web_search_auto_threshold})")
                
                # å…¼å®¹æ—§é…ç½®ï¼šå¦‚æœæ²¡æœ‰è®¾ç½®ç­–ç•¥æ¨¡å¼ï¼Œä½¿ç”¨æ—§é€»è¾‘
                else:
                    if max_confidence < web_search_trigger_threshold:
                        should_trigger_web_search = True
                        logger.info(f"ğŸŒ è§¦å‘è”ç½‘æœç´¢ (æœ€é«˜ç½®ä¿¡åº¦ {max_confidence:.2f} < é˜ˆå€¼ {web_search_trigger_threshold})")
                    else:
                        logger.info(f"âœ‹ è·³è¿‡è”ç½‘æœç´¢ (æœ€é«˜ç½®ä¿¡åº¦ {max_confidence:.2f} >= é˜ˆå€¼ {web_search_trigger_threshold})")
        
        if should_trigger_web_search:
            retrieval_start = time.time()
            web_results = await self._search_web(
                query,
                app_config
            )
            retrieval_time = (time.time() - retrieval_start) * 1000
            
            if web_results:
                retrieval_path.append({
                    "source": "web_search",
                    "time_ms": retrieval_time,
                    "results_count": len(web_results)
                })
                
                for result in web_results:
                    # ä¿ç•™åŸå§‹çš„sourceï¼ˆtavily_answer/tavily_webï¼‰ï¼Œä½†æ·»åŠ webæ ‡è®°ç”¨äºåˆ†ç±»
                    if "source" not in result:
                        result["source"] = "web"
                    result["search_type"] = "web"  # æ·»åŠ æœç´¢ç±»å‹æ ‡è®°
                    
                    # ç¡®ä¿æœ‰similarityå­—æ®µï¼ˆè”ç½‘æœç´¢å¯èƒ½ä½¿ç”¨relevanceï¼‰
                    if "similarity" not in result and "relevance" in result:
                        result["similarity"] = result["relevance"]
                    elif "similarity" not in result:
                        result["similarity"] = 0.5  # é»˜è®¤ä¸­ç­‰ç›¸ä¼¼åº¦
                    
                    result["weighted_score"] = result.get("similarity", 0) * app_config.get("web_search_weight", 0.6)
                    all_results.append(result)
            else:
                retrieval_path.append({
                    "source": "web_search",
                    "time_ms": retrieval_time,
                    "results_count": 0,
                    "status": "æœªé…ç½®æˆ–æš‚ä¸å¯ç”¨"
                })
                logger.warning("âš ï¸ è”ç½‘æœç´¢å·²å¯ç”¨ä½†æœªè¿”å›ç»“æœï¼ˆå¯èƒ½æœªé…ç½®æœç´¢å¼•æ“ï¼‰")
        
        # è®¡ç®—æ€»æ£€ç´¢æ—¶é—´
        retrieval_time_total = (time.time() - start_time) * 1000
        
        # åº”ç”¨èåˆç­–ç•¥
        fusion_start = time.time()
        final_result = await self._apply_fusion_strategy(
            all_results,
            app_config
        )
        fusion_time = (time.time() - fusion_start) * 1000
        
        # æ€»æ—¶é—´
        total_time = (time.time() - start_time) * 1000
        
        # æå–ç­–ç•¥ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        strategy_info = final_result.get("_strategy_info") if final_result else None
        
        return {
            "query": query,
            "matched_source": final_result.get("source") if final_result else None,
            "confidence_score": final_result.get("similarity") if final_result else 0.0,  # ä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦ï¼Œä¸æ˜¯åŠ æƒåˆ†æ•°
            "weighted_score": final_result.get("weighted_score") if final_result else 0.0,  # ä¿ç•™åŠ æƒåˆ†æ•°ä¾›å†…éƒ¨ä½¿ç”¨
            "raw_score": final_result.get("similarity") if final_result else 0.0,
            "_strategy_info": strategy_info,  # ä¼ é€’ç­–ç•¥ä¿¡æ¯åˆ°ä¸Šå±‚
            "answer": final_result.get("answer") if final_result else None,
            "references": self._extract_references(all_results, app_config),
            "suggestions": self._generate_suggestions(all_results, app_config),
            "has_suggestions": len(self._generate_suggestions(all_results, app_config)) > 0,
            "retrieval_path": retrieval_path,
            "preprocessing_info": preprocessing_info,
            "fusion_details": {
                "strategy": app_config.get("fusion_strategy", "weighted_avg"),
                "total_candidates": len(all_results),
                "selected": final_result is not None
            },
            "timing": {
                "preprocessing_ms": round(preprocessing_time, 2),
                "retrieval_ms": round(retrieval_time_total, 2),
                "fusion_ms": round(fusion_time, 2),
                "total_ms": round(total_time, 2)
            }
        }
    
    async def _preprocess_query(
        self,
        query: str,
        app_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """é¢„å¤„ç†æŸ¥è¯¢"""
        info = {
            "original_query": query,
            "processed_query": query,
            "detected_language": "unknown",
            "detected_intent": "unknown",
            "is_filtered": False
        }
        
        if not app_config.get("enable_preprocessing", True):
            return info
        
        # è¯­è¨€æ£€æµ‹
        if app_config.get("enable_language_detection", True):
            # ç®€å•çš„è¯­è¨€æ£€æµ‹ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„å®ç°ï¼‰
            if any('\u4e00' <= char <= '\u9fff' for char in query):
                info["detected_language"] = "zh"
            else:
                info["detected_language"] = "en"
        
        # æ„å›¾è¯†åˆ«
        if app_config.get("enable_intent_recognition", True):
            # ç®€å•çš„æ„å›¾è¯†åˆ«ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºMLæ¨¡å‹ï¼‰
            if any(word in query.lower() for word in ["how", "ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ"]):
                info["detected_intent"] = "question"
            elif any(word in query.lower() for word in ["help", "å¸®åŠ©", "é—®é¢˜"]):
                info["detected_intent"] = "help"
            else:
                info["detected_intent"] = "general"
        
        # æ•æ„Ÿè¯è¿‡æ»¤
        if app_config.get("enable_sensitive_filter", False):
            sensitive_words = app_config.get("sensitive_words", [])
            for word in sensitive_words:
                if word.lower() in query.lower():
                    info["is_filtered"] = True
                    break
        
        return info
    
    async def _search_fixed_qa(
        self,
        query: str,
        fixed_qa_pairs: List[Dict[str, Any]],
        app_config: Dict[str, Any],
        embedding_provider_config: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """æœç´¢å›ºå®šQ&Aå¯¹ï¼ˆæ”¯æŒæ™ºèƒ½åŒ¹é…æ¨¡å¼ï¼‰"""
        if not fixed_qa_pairs or not embedding_provider_config:
            return []
        
        try:
            # ä»fusion_configä¸­è·å–å›ºå®šQ&Aé…ç½®
            fusion_config = app_config.get("fusion_config", {})
            fixed_qa_config = fusion_config.get("fixed_qa", {})
            
            # è·å–é…ç½®å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨fusion_configä¸­çš„é…ç½®
            mode = fixed_qa_config.get("mode", "smart")
            direct_threshold = fixed_qa_config.get("direct_match_threshold", 0.90)
            suggest_threshold = fixed_qa_config.get("suggest_threshold", 0.70)
            qa_min_threshold = fixed_qa_config.get("qa_min_threshold", 0.50)  # æœ€ä½åŒ¹é…é˜ˆå€¼
            max_suggestions = fixed_qa_config.get("max_suggestions", 3)
            
            logger.info(f"ğŸ” å›ºå®šQ&Aé…ç½® - æ¨¡å¼: {mode}, ç›´æ¥é˜ˆå€¼: {direct_threshold}, å»ºè®®é˜ˆå€¼: {suggest_threshold}, æœ€ä½é˜ˆå€¼: {qa_min_threshold}")
            
            # ğŸš€ é—®é¢˜æ‰©å±•ï¼šç”Ÿæˆå¤šä¸ªåŒä¹‰é—®æ³•ä»¥æé«˜åŒ¹é…ç‡
            from app.core.qa_expansion import qa_expansion
            expanded_queries = qa_expansion.expand_question(query)
            
            if len(expanded_queries) > 1:
                logger.info(f"ğŸ“ é—®é¢˜æ‰©å±•: '{query}' -> {expanded_queries}")
            
            # ä¸ºæ‰€æœ‰æ‰©å±•é—®é¢˜ç”Ÿæˆå‘é‡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
            if len(expanded_queries) == 1:
                query_vectors = [await embedding_engine.embed_text(query, embedding_provider_config)]
            else:
                query_vectors = await embedding_engine.embed_texts(expanded_queries, embedding_provider_config)
            
            # ä½¿ç”¨åŸå§‹é—®é¢˜çš„å‘é‡ä½œä¸ºä¸»å‘é‡
            primary_query_vector = query_vectors[0]
            
            # è®¡ç®—æ‰€æœ‰Q&Aå¯¹çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å¤šä¸ªæŸ¥è¯¢å‘é‡å–æœ€é«˜åˆ†ï¼‰
            all_matches = []
            for qa in fixed_qa_pairs:
                if qa.get("embedding_vector") and qa.get("is_active", True):
                    # è®¡ç®—ä¸æ‰€æœ‰æ‰©å±•é—®é¢˜çš„ç›¸ä¼¼åº¦ï¼Œå–æœ€é«˜å€¼
                    max_similarity = 0.0
                    for query_vec in query_vectors:
                        similarity = embedding_engine.compute_similarity(
                            query_vec,
                            qa["embedding_vector"]
                        )
                        max_similarity = max(max_similarity, similarity)
                    
                    # ğŸ¯ å…³é”®è¯åŠ æˆï¼šå¦‚æœé—®é¢˜åŒ…å«å…³é”®è¯ï¼Œç»™äºˆå°å¹…åŠ æˆ
                    keyword_boost = 0.0
                    qa_question_lower = qa["question"].lower()
                    
                    # æå–æŸ¥è¯¢å…³é”®è¯
                    keywords = qa_expansion.extract_keywords(query)
                    matching_keywords = sum(1 for kw in keywords if kw.lower() in qa_question_lower)
                    
                    if matching_keywords > 0:
                        keyword_boost = min(0.05 * matching_keywords, 0.15)  # æœ€å¤šåŠ 15%
                        logger.debug(f"ğŸ”‘ å…³é”®è¯åŒ¹é…: {matching_keywords}ä¸ª -> +{keyword_boost:.2%} åŠ æˆ")
                    
                    final_similarity = min(max_similarity + keyword_boost, 1.0)  # ä¸è¶…è¿‡1.0
                    
                    # âœ… åº”ç”¨æœ€ä½é˜ˆå€¼è¿‡æ»¤
                    if final_similarity >= qa_min_threshold:
                        all_matches.append({
                            "id": qa["id"],
                            "question": qa["question"],
                            "answer": qa["answer"],
                            "category": qa.get("category"),
                            "similarity": final_similarity,
                            "priority": qa.get("priority", 0),
                            "_raw_similarity": max_similarity,  # ä¿å­˜åŸå§‹ç›¸ä¼¼åº¦ç”¨äºè°ƒè¯•
                            "_keyword_boost": keyword_boost
                        })
                        logger.debug(f"âœ… Q&Aé€šè¿‡æœ€ä½é˜ˆå€¼: '{qa['question'][:30]}...' ç›¸ä¼¼åº¦={final_similarity:.2f} >= {qa_min_threshold:.2f}")
                    else:
                        logger.debug(f"âŒ Q&Aè¢«æœ€ä½é˜ˆå€¼è¿‡æ»¤: '{qa['question'][:30]}...' ç›¸ä¼¼åº¦={final_similarity:.2f} < {qa_min_threshold:.2f}")
            
            # æŒ‰ç›¸ä¼¼åº¦å’Œä¼˜å…ˆçº§æ’åº
            all_matches.sort(key=lambda x: (x["similarity"], x["priority"]), reverse=True)
            
            # æ ¹æ®æ¨¡å¼è¿”å›ç»“æœ
            results = []
            
            if mode == "smart":
                # æ™ºèƒ½æ¨¡å¼ï¼šæ ¹æ®ç›¸ä¼¼åº¦è‡ªåŠ¨å†³å®š
                for match in all_matches:
                    if match["similarity"] >= direct_threshold:
                        # é«˜ç›¸ä¼¼åº¦ï¼šæ ‡è®°ä¸ºç›´æ¥å›ç­”
                        match["match_type"] = "direct"
                        results.append(match)
                    elif match["similarity"] >= suggest_threshold:
                        # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šæ ‡è®°ä¸ºå»ºè®®
                        match["match_type"] = "suggest"
                        results.append(match)
                        if len([r for r in results if r.get("match_type") == "suggest"]) >= max_suggestions:
                            break
            
            elif mode == "suggest":
                # å»ºè®®æ¨¡å¼ï¼šå§‹ç»ˆè¿”å›å»ºè®®åˆ—è¡¨
                for match in all_matches[:max_suggestions]:
                    if match["similarity"] >= suggest_threshold:
                        match["match_type"] = "suggest"
                        results.append(match)
            
            elif mode == "strict":
                # ä¸¥æ ¼æ¨¡å¼ï¼šåªæœ‰æé«˜ç›¸ä¼¼åº¦æ‰ä½¿ç”¨
                for match in all_matches:
                    if match["similarity"] >= direct_threshold:
                        match["match_type"] = "direct"
                        results.append(match)
                    if len(results) >= 1:  # ä¸¥æ ¼æ¨¡å¼åªè¿”å›æœ€ä½³åŒ¹é…
                        break
            
            logger.info(f"âœ… å›ºå®šQ&AåŒ¹é…ç»“æœ: {len(results)}ä¸ªï¼Œå…¶ä¸­ç›´æ¥:{len([r for r in results if r.get('match_type')=='direct'])}ä¸ªï¼Œå»ºè®®:{len([r for r in results if r.get('match_type')=='suggest'])}ä¸ª")
            
            return results
            
        except Exception as e:
            logger.error(f"å›ºå®šQ&Aæœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_knowledge_bases(
        self,
        query: str,
        knowledge_bases: List[Dict[str, Any]],
        app_config: Dict[str, Any],
        embedding_provider_config: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """æœç´¢å‘é‡çŸ¥è¯†åº“"""
        if not knowledge_bases:
            return []
        
        all_results = []
        
        # éœ€è¦è·å–å‘é‡æ•°æ®åº“é…ç½®
        from app.models.database import SessionLocal, VectorDBProvider
        from app.core.rag_engine import RAGEngine
        
        db = SessionLocal()
        try:
            # è·å–é»˜è®¤çš„å‘é‡æ•°æ®åº“æä¾›å•†é…ç½®
            vector_db_provider = db.query(VectorDBProvider).filter(
                VectorDBProvider.is_default == True
            ).first()
            
            if not vector_db_provider:
                logger.warning("æœªæ‰¾åˆ°é»˜è®¤å‘é‡æ•°æ®åº“æä¾›å•†é…ç½®")
                return []
            
            vector_db_config = {
                "name": vector_db_provider.name,
                "provider_type": vector_db_provider.provider_type,
                "host": vector_db_provider.host,
                "port": vector_db_provider.port
            }
            
            # æ·»åŠ  API keyï¼ˆå¦‚æœæœ‰ï¼‰
            if vector_db_provider.api_key:
                vector_db_config["api_key"] = vector_db_provider.api_key
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ HTTPS
            if vector_db_provider.provider_type == "qdrant" and ('qdrant.io' in (vector_db_provider.host or '') or vector_db_provider.port in [443, 6334]):
                vector_db_config["https"] = True
            
            # åˆ›å»ºå¸¦é…ç½®çš„RAGå¼•æ“å®ä¾‹
            rag = RAGEngine(
                embedding_provider_config=embedding_provider_config,
                vector_db_provider_config=vector_db_config
            )
            
            # ä»fusion_configä¸­è·å–å‘é‡æ£€ç´¢é…ç½®
            fusion_config = app_config.get("fusion_config", {})
            vector_retrieval_config = fusion_config.get("vector_retrieval", {})
            
            # è·å–æ£€ç´¢å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨fusion_configä¸­çš„é…ç½®
            min_similarity_score = vector_retrieval_config.get("min_similarity_score", 
                                                               app_config.get("similarity_threshold_low", 0.75))
            max_results = vector_retrieval_config.get("max_results", 
                                                      app_config.get("top_k", 5))
            rerank_enabled = vector_retrieval_config.get("rerank_enabled", False)
            hybrid_search_enabled = vector_retrieval_config.get("hybrid_search_enabled", False)
            
            logger.info(f"ğŸ” å‘é‡æ£€ç´¢é…ç½® - æœ€å°ç›¸ä¼¼åº¦: {min_similarity_score}, æœ€å¤§ç»“æœæ•°: {max_results}, "
                       f"é‡æ’åº: {rerank_enabled}, æ··åˆæœç´¢: {hybrid_search_enabled}")
            
            for kb in knowledge_bases:
                try:
                    # ä½¿ç”¨RAGå¼•æ“æŸ¥è¯¢
                    results = await rag.query(
                        collection_name=kb["collection_name"],
                        query_text=query,
                        n_results=max_results
                    )
                    
                    if results and results.get("documents"):
                        for i, doc in enumerate(results["documents"]):
                            # è®¡ç®—åŠ æƒåˆ†æ•°
                            # å‘é‡æ•°æ®åº“è¿”å›çš„distanceéœ€è¦è½¬æ¢ä¸ºsimilarity
                            distance = results["distances"][i] if results.get("distances") else 0
                            similarity = 1.0 / (1.0 + distance)  # ç®€å•çš„è·ç¦»è½¬ç›¸ä¼¼åº¦
                            
                            # åº”ç”¨çŸ¥è¯†åº“æƒé‡
                            kb_weight = kb.get("weight", 1.0)
                            boost_factor = kb.get("boost_factor", 1.0)
                            
                            weighted_similarity = similarity * kb_weight * boost_factor
                            
                            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤ - ä¼˜å…ˆä½¿ç”¨æ–°çš„èåˆç­–ç•¥é…ç½®
                            fusion_config = app_config.get("fusion_config", {})
                            strategy_config = fusion_config.get("strategy", {})
                            kb_min_threshold = strategy_config.get("kb_min_threshold", min_similarity_score)
                            
                            # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºé˜ˆå€¼åº”ç”¨è¿‡ç¨‹
                            logger.info(f"ğŸ¯ çŸ¥è¯†åº“ [{kb['name']}] æ–‡æ¡£ç›¸ä¼¼åº¦: {similarity:.4f}, é˜ˆå€¼: {kb_min_threshold:.4f}")
                            
                            if similarity >= kb_min_threshold:
                                all_results.append({
                                    "kb_id": kb["id"],
                                    "kb_name": kb["name"],
                                    "text": doc,
                                    "metadata": results["metadatas"][i] if results.get("metadatas") else {},
                                    "similarity": similarity,
                                    "weighted_similarity": weighted_similarity,
                                    "kb_weight": kb_weight,
                                    "answer": doc  # å¯¹äºçŸ¥è¯†åº“ï¼Œæ–‡æœ¬å†…å®¹å³ä¸ºç­”æ¡ˆ
                                })
                                logger.info(f"âœ… çŸ¥è¯†åº“ [{kb['name']}] æ–‡æ¡£é€šè¿‡é˜ˆå€¼æ£€æŸ¥ï¼Œç›¸ä¼¼åº¦: {similarity:.4f} >= {kb_min_threshold:.4f}")
                            else:
                                logger.info(f"âŒ çŸ¥è¯†åº“ [{kb['name']}] æ–‡æ¡£è¢«é˜ˆå€¼è¿‡æ»¤ï¼Œç›¸ä¼¼åº¦ {similarity:.4f} < {kb_min_threshold:.4f}")
                
                except Exception as e:
                    logger.error(f"æœç´¢çŸ¥è¯†åº“ {kb.get('name')} å¤±è´¥: {e}", exc_info=True)
                    continue
            
            # æŒ‰åŠ æƒç›¸ä¼¼åº¦æ’åº
            all_results.sort(key=lambda x: x["weighted_similarity"], reverse=True)
            
            # é™åˆ¶è¿”å›æ•°é‡
            return all_results[:max_results]
            
        finally:
            db.close()
    
    async def _kb_priority_strategy(
        self,
        results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        çŸ¥è¯†åº“ä¼˜å…ˆç­–ç•¥ï¼ˆæ¨èé¢„è®¾ï¼‰
        - çŸ¥è¯†åº“æ˜¯ä¸»åŠ›ï¼Œæœ‰ä¼˜å…ˆæƒ
        - è”ç½‘æœç´¢æ˜¯é”¦ä¸Šæ·»èŠ±ï¼Œåœ¨çŸ¥è¯†åº“ä¸è¶³æ—¶è¡¥å……
        """
        if not results:
            return None
        
        # è·å–ç­–ç•¥é…ç½®ï¼ˆä¼˜å…ˆæ–°é…ç½®ï¼Œå…¼å®¹æ—§é…ç½®ï¼‰
        fusion_config = app_config.get("fusion_config", {})
        strategy_config = fusion_config.get("strategy", {})
        
        # çŸ¥è¯†åº“ä¼˜å…ˆç­–ç•¥å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
        kb_absolute_priority_threshold = strategy_config.get("kb_absolute_priority_threshold", 0.85)
        kb_priority_threshold = strategy_config.get("kb_priority_threshold", 0.70)
        kb_priority_bonus = strategy_config.get("kb_priority_bonus", 0.15)
        
        # åˆ†ç¦»ä¸åŒæ¥æºçš„ç»“æœ
        fixed_qa_results = [r for r in results if r.get("source") == "fixed_qa"]
        kb_results = [r for r in results if r.get("source") == "kb"]
        web_results = [r for r in results if r.get("source") in ["web", "tavily_answer", "tavily_web"]]
        
        # è·å–å„æ¥æºçš„æœ€ä½³ç»“æœ
        best_fixed_qa = max(fixed_qa_results, key=lambda x: x.get("similarity", 0)) if fixed_qa_results else None
        best_kb = max(kb_results, key=lambda x: x.get("similarity", 0)) if kb_results else None
        best_web = max(web_results, key=lambda x: x.get("similarity", 0)) if web_results else None
        
        # ç­–ç•¥0: å›ºå®šQ&Aä¼˜å…ˆï¼ˆå¦‚æœå­˜åœ¨é«˜è´¨é‡åŒ¹é…ï¼‰
        if best_fixed_qa and best_fixed_qa.get("similarity", 0) >= 0.90:
            logger.info(f"ğŸ’ å›ºå®šQ&Aé«˜åŒ¹é… {best_fixed_qa.get('similarity', 0):.1%}ï¼Œç›´æ¥é‡‡ç”¨")
            return best_fixed_qa
        
        # ç­–ç•¥1: çŸ¥è¯†åº“é«˜ç½®ä¿¡åº¦ï¼ˆâ‰¥85%ï¼‰â†’ ç›´æ¥ä½¿ç”¨çŸ¥è¯†åº“
        if best_kb and best_kb.get("similarity", 0) >= kb_absolute_priority_threshold:
            kb_score = best_kb.get("similarity", 0)
            logger.info(f"âœ… çŸ¥è¯†åº“é«˜ç½®ä¿¡åº¦ {kb_score:.1%}ï¼ˆâ‰¥{kb_absolute_priority_threshold:.0%}ï¼‰ï¼Œç›´æ¥é‡‡ç”¨")
            return best_kb
        
        # ç­–ç•¥2: çŸ¥è¯†åº“ä¸­ç­‰ç½®ä¿¡åº¦ï¼ˆ70-85%ï¼‰â†’ çŸ¥è¯†åº“ä¼˜å…ˆï¼Œä½†ç»™è”ç½‘æœºä¼š
        if best_kb and best_kb.get("similarity", 0) >= kb_priority_threshold:
            kb_score = best_kb.get("similarity", 0)
            
            if best_web:
                web_score = best_web.get("similarity", 0)
                # è”ç½‘æœç´¢éœ€è¦æ˜æ˜¾æ›´å¥½ï¼ˆè¶…è¿‡çŸ¥è¯†åº“ + bonusï¼‰
                required_web_score = kb_score + kb_priority_bonus
                
                if web_score > required_web_score:
                    logger.info(
                        f"ğŸŒ è”ç½‘æœç´¢æ˜¾è‘—æ›´å¥½ "
                        f"(Web {web_score:.1%} > KB {kb_score:.1%} + {kb_priority_bonus:.0%} = {required_web_score:.1%})"
                    )
                    return best_web
                else:
                    logger.info(
                        f"ğŸ“š çŸ¥è¯†åº“ä¼˜å…ˆ "
                        f"(KB {kb_score:.1%} vs Web {web_score:.1%}ï¼Œå·®è·æœªè¾¾ {kb_priority_bonus:.0%})"
                    )
                    return best_kb
            else:
                logger.info(f"ğŸ“š çŸ¥è¯†åº“ä¸­ç­‰ç½®ä¿¡åº¦ {kb_score:.1%}ï¼Œæ— è”ç½‘ç»“æœï¼Œé‡‡ç”¨çŸ¥è¯†åº“")
                return best_kb
        
        # ç­–ç•¥3: çŸ¥è¯†åº“ä½ç½®ä¿¡åº¦ï¼ˆ<70%ï¼‰â†’ å…¬å¹³ç«äº‰
        kb_info = f"{best_kb.get('similarity', 0):.1%}" if best_kb else "æ— "
        logger.info(
            f"âš–ï¸ çŸ¥è¯†åº“ç½®ä¿¡åº¦è¾ƒä½ (KB: {kb_info}), æŒ‰åŠ æƒåˆ†æ•°å…¬å¹³ç«äº‰"
        )
        
        # æŒ‰åŠ æƒåˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€ä½³ç»“æœ
        results_sorted = sorted(results, key=lambda x: x.get("weighted_score", 0), reverse=True)
        best_result = results_sorted[0] if results_sorted else None
        
        if best_result:
            source_display = {
                "fixed_qa": "å›ºå®šQ&A",
                "kb": "çŸ¥è¯†åº“",
                "tavily_answer": "Cbit AIæœç´¢",
                "tavily_web": "Cbit AIæœç´¢",
                "web": "è”ç½‘æœç´¢"
            }.get(best_result.get("source"), best_result.get("source"))
            
            logger.info(
                f"ğŸ† å…¬å¹³ç«äº‰ç»“æœ: {source_display} "
                f"(ç›¸ä¼¼åº¦ {best_result.get('similarity', 0):.1%}, "
                f"åŠ æƒåˆ†æ•° {best_result.get('weighted_score', 0):.2f})"
            )
        
        return best_result
    
    async def _apply_fusion_strategy(
        self,
        results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """åº”ç”¨èåˆç­–ç•¥"""
        if not results:
            return None
        
        fusion_strategy = app_config.get("fusion_strategy", "weighted_avg")
        threshold_high = app_config.get("similarity_threshold_high", 0.90)
        threshold_low = app_config.get("similarity_threshold_low", 0.75)
        
        # å‡†ç¡®ä¼˜å…ˆç­–ç•¥ï¼ˆæœ€ä¸¥æ ¼ï¼‰
        if fusion_strategy == "accurate_priority":
            strategy_result = await accurate_priority_strategy.apply_strategy(results, app_config)
            # Aæ¡£æˆ–Bæ¡£ï¼šè¿”å›final_resultï¼Œä½†é™„åŠ ç­–ç•¥ä¿¡æ¯
            final = strategy_result.get("final_result")
            if final:
                # ğŸ”‘ å…³é”®ï¼šåˆ›å»ºstrategy_infoå‰¯æœ¬ï¼Œé¿å…å¾ªç¯å¼•ç”¨
                strategy_info_for_response = {
                    "tier": strategy_result.get("tier"),
                    "confidence_level": strategy_result.get("confidence_level"),
                    "citations": strategy_result.get("citations", []),
                    "explanation": strategy_result.get("explanation"),
                    "web_search_option": strategy_result.get("web_search_option", False),
                    "custom_message": strategy_result.get("custom_message")
                }
                final["_strategy_info"] = strategy_info_for_response
                return final
            # Cæ¡£ï¼šæ²¡æœ‰final_resultï¼Œä½†ä»éœ€ä¼ é€’ç­–ç•¥ä¿¡æ¯
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç»“æœæ¥æºå¸¦ç­–ç•¥ä¿¡æ¯
            strategy_info_for_response = {
                "tier": strategy_result.get("tier"),
                "confidence_level": strategy_result.get("confidence_level"),
                "citations": strategy_result.get("citations", []),
                "explanation": strategy_result.get("explanation"),
                "web_search_option": strategy_result.get("web_search_option", False),
                "custom_message": strategy_result.get("custom_message")
            }
            return {
                "source": "no_result",
                "answer": None,
                "similarity": 0.0,
                "weighted_score": 0.0,
                "_strategy_info": strategy_info_for_response
            }
        
        # çŸ¥è¯†åº“ä¼˜å…ˆç­–ç•¥ï¼ˆæ¨èé¢„è®¾ï¼‰
        if fusion_strategy == "kb_priority":
            final = await self._kb_priority_strategy(results, app_config)
            # ä¸ºæ‰€æœ‰ç­–ç•¥æ·»åŠ ç»Ÿä¸€çš„citations
            if final:
                final["_strategy_info"] = self._generate_unified_strategy_info(final, results, app_config)
            return final
        
        # ä¼˜å…ˆçº§è·¯ç”±ç­–ç•¥
        if fusion_strategy == "priority":
            # æŒ‰æ¥æºä¼˜å…ˆçº§ï¼šfixed_qa > kb > web > llm
            for source_type in ["fixed_qa", "kb", "web"]:
                for result in results:
                    if result.get("source") == source_type:
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é«˜é˜ˆå€¼
                        if result.get("similarity", 0) >= threshold_high:
                            result["_strategy_info"] = self._generate_unified_strategy_info(result, results, app_config)
                            return result
            
            # å¦‚æœæ²¡æœ‰è¾¾åˆ°é«˜é˜ˆå€¼ï¼Œè¿”å›æœ€é«˜åˆ†çš„
            final = results[0] if results else None
            if final:
                final["_strategy_info"] = self._generate_unified_strategy_info(final, results, app_config)
            return final
        
        # åŠ æƒå¹³å‡ç­–ç•¥
        elif fusion_strategy == "weighted_avg":
            # æŒ‰åŠ æƒåˆ†æ•°æ’åº
            results_sorted = sorted(results, key=lambda x: x.get("weighted_score", 0), reverse=True)
            final = results_sorted[0] if results_sorted else None
            if final:
                final["_strategy_info"] = self._generate_unified_strategy_info(final, results, app_config)
            return final
        
        # æœ€å¤§å€¼ä¼˜å…ˆç­–ç•¥
        elif fusion_strategy == "max_score":
            # è¿”å›åŸå§‹ç›¸ä¼¼åº¦æœ€é«˜çš„
            results_sorted = sorted(results, key=lambda x: x.get("similarity", 0), reverse=True)
            final = results_sorted[0] if results_sorted else None
            if final:
                final["_strategy_info"] = self._generate_unified_strategy_info(final, results, app_config)
            return final
        
        # æŠ•ç¥¨æœºåˆ¶
        elif fusion_strategy == "voting":
            # ç»Ÿè®¡æ¯ä¸ªç­”æ¡ˆçš„ç¥¨æ•°ï¼ˆåŸºäºæƒé‡ï¼‰
            answer_votes: Dict[str, float] = {}
            answer_results: Dict[str, Dict] = {}
            
            for result in results:
                answer = result.get("answer", "")
                weight = result.get("weighted_score", 0)
                
                if answer in answer_votes:
                    answer_votes[answer] += weight
                else:
                    answer_votes[answer] = weight
                    answer_results[answer] = result
            
            # è¿”å›ç¥¨æ•°æœ€é«˜çš„
            if answer_votes:
                winning_answer = max(answer_votes, key=answer_votes.get)
                final = answer_results[winning_answer]
                if final:
                    final["_strategy_info"] = self._generate_unified_strategy_info(final, results, app_config)
                return final
            return None
        
        # å¤šæºèåˆ
        elif fusion_strategy == "multi_source_fusion":
            # ç»¼åˆæ‰€æœ‰é«˜è´¨é‡æ¥æº
            high_quality_results = [
                r for r in results 
                if r.get("similarity", 0) >= threshold_low
            ]
            
            if not high_quality_results:
                return results[0] if results else None
            
            # è¿”å›åŠ æƒåˆ†æ•°æœ€é«˜çš„
            return max(high_quality_results, key=lambda x: x.get("weighted_score", 0))
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
        final = results[0] if results else None
        if final:
            final["_strategy_info"] = self._generate_unified_strategy_info(final, results, app_config)
        return final
    
    def _generate_unified_strategy_info(
        self,
        final_result: Dict[str, Any],
        all_results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ä¸ºæ‰€æœ‰èåˆç­–ç•¥ç”Ÿæˆç»Ÿä¸€çš„ç­–ç•¥ä¿¡æ¯å’Œcitationsï¼ˆä»¿OpenAIæ ¼å¼ï¼‰
        ç¡®ä¿æ‰€æœ‰é¢„è®¾éƒ½ä½¿ç”¨ä¸€è‡´çš„å¼•ç”¨æ¥æºæ˜¾ç¤ºæ ·å¼
        """
        # ç”Ÿæˆcitationsï¼šä»¥final_resultä¸ºä¸»ï¼Œè¡¥å……å…¶ä»–é«˜ç›¸å…³ç»“æœ
        relevant_results = self._get_relevant_results_for_citations(final_result, all_results)
        citations = self._generate_unified_citations(relevant_results)
        
        # æ ¹æ®ç›¸ä¼¼åº¦åˆ¤æ–­ç½®ä¿¡åº¦ç­‰çº§
        similarity = final_result.get("similarity", 0)
        if similarity >= 0.85:
            confidence_level = "high"
            tier = "A"
        elif similarity >= 0.70:
            confidence_level = "moderate"
            tier = "B"
        else:
            confidence_level = "low"
            tier = "C"
        
        # ç”Ÿæˆç®€å•çš„è§£é‡Š
        source_type = final_result.get("source", "")
        source_name_map = {
            "fixed_qa": "å›ºå®šQ&A",
            "kb": "çŸ¥è¯†åº“",
            "web": "è”ç½‘æœç´¢",
            "tavily_answer": "è”ç½‘æœç´¢",
            "tavily_web": "è”ç½‘æœç´¢"
        }
        source_display = source_name_map.get(source_type, "çŸ¥è¯†æº")
        
        # ğŸ”‘ æ ¹æ®æ¥æºç±»å‹ä½¿ç”¨ä¸åŒçš„åº¦é‡è¯
        # è”ç½‘æœç´¢ä½¿ç”¨"ç›¸å…³åº¦"ï¼ŒçŸ¥è¯†åº“/Q&Aä½¿ç”¨"ç›¸ä¼¼åº¦"
        if source_type in ["web", "tavily_answer", "tavily_web"]:
            # è”ç½‘æœç´¢ï¼šåªæ˜¾ç¤ºæ¥æºé“¾æ¥ï¼Œä¸æ˜¾ç¤ºç›¸å…³åº¦ç™¾åˆ†æ¯”
            explanation = f"åŸºäº{source_display}æ£€ç´¢ç»“æœ"
            
            # å°è¯•è·å–ç¬¬ä¸€ä¸ªå¼•ç”¨çš„é“¾æ¥ä½œä¸ºä¸»è¦æ¥æº
            if citations and len(citations) > 0:
                first_citation = citations[0]
                url = first_citation.get("url")
                source_name = first_citation.get("source_name", "")
                if url:
                    explanation += f" | ä¸»è¦æ¥æº: {source_name} | é“¾æ¥: {url}"
                elif source_name:
                    explanation += f" | ä¸»è¦æ¥æº: {source_name}"
        else:
            # çŸ¥è¯†åº“/Q&Aï¼šä½¿ç”¨ç›¸ä¼¼åº¦
            metric_name = "ç›¸ä¼¼åº¦"
            explanation = f"åŸºäº{source_display}æ£€ç´¢ç»“æœï¼ˆ{metric_name} {similarity*100:.1f}%ï¼‰"
        
        return {
            "tier": tier,
            "confidence_level": confidence_level,
            "citations": citations,
            "explanation": explanation,
            "web_search_option": False,  # éaccurate_priorityç­–ç•¥ä¸æ˜¾ç¤ºè”ç½‘æŒ‰é’®
            "custom_message": None
        }
    
    def _get_relevant_results_for_citations(
        self,
        final_result: Dict[str, Any],
        all_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½é€‰æ‹©ä¸æœ€ç»ˆå›ç­”ç›¸å…³çš„ç»“æœä½œä¸ºå¼•ç”¨æ¥æº
        
        ç­–ç•¥ï¼š
        1. å¦‚æœæœ€ç»ˆå›ç­”æ¥è‡ªè”ç½‘æœç´¢ï¼Œåªæ˜¾ç¤ºè”ç½‘æœç´¢çš„å¼•ç”¨
        2. å¦‚æœæ¥è‡ªçŸ¥è¯†åº“ï¼Œä¼˜å…ˆæ˜¾ç¤ºçŸ¥è¯†åº“çš„é«˜ç›¸å…³ç»“æœ
        3. æœ€å¤šè¿”å›3ä¸ªç»“æœ
        4. æŒ‰ç›¸ä¼¼åº¦/ç›¸å…³åº¦é™åºæ’åˆ—
        """
        relevant_results = []
        final_source = final_result.get("source", "")
        
        # ğŸ”‘ å…³é”®ï¼šå¦‚æœæœ€ç»ˆå›ç­”æ¥è‡ªè”ç½‘æœç´¢ï¼Œåªé€‰æ‹©è”ç½‘æœç´¢çš„å¼•ç”¨
        if final_source in ["web", "tavily_answer", "tavily_web"]:
            logger.info(f"ğŸŒ å›ç­”æ¥è‡ªè”ç½‘æœç´¢ï¼Œåªé€‰æ‹©è”ç½‘æœç´¢çš„å¼•ç”¨æ¥æº")
            
            # æ”¶é›†æ‰€æœ‰è”ç½‘æœç´¢ç»“æœ
            web_results = [
                r for r in all_results 
                if r.get("source") in ["web", "tavily_answer", "tavily_web"]
            ]
            
            # æŒ‰ç›¸å…³åº¦æ’åºï¼ˆtavilyä½¿ç”¨relevanceï¼Œwebä½¿ç”¨similarityï¼‰
            web_results_sorted = sorted(
                web_results,
                key=lambda x: x.get("relevance", x.get("similarity", 0)),
                reverse=True
            )
            
            # æœ€å¤šè¿”å›3ä¸ªè”ç½‘ç»“æœ
            relevant_results = web_results_sorted[:3]
            
            logger.info(f"ğŸ“š é€‰æ‹©äº† {len(relevant_results)} ä¸ªè”ç½‘æœç´¢å¼•ç”¨")
            for idx, res in enumerate(relevant_results, 1):
                relevance = res.get("relevance", res.get("similarity", 0))
                title = res.get("title", "")[:50]
                url = res.get("url", "")
                logger.info(f"  [{idx}] {res.get('source')} - ç›¸å…³åº¦: {relevance:.2%} - {title}")
                if url:
                    logger.info(f"       é“¾æ¥: {url}")
            
            return relevant_results
        
        # å¦‚æœä¸æ˜¯è”ç½‘æœç´¢ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆçŸ¥è¯†åº“ã€Q&Aç­‰ï¼‰
        # 1. é¦–å…ˆæ·»åŠ  final_resultï¼ˆå¦‚æœå®ƒæœ‰å®é™…å†…å®¹ä¸”ä¸æ˜¯ "no_result"ï¼‰
        if final_result.get("source") != "no_result":
            # ç¡®ä¿ final_result æœ‰æ–‡æœ¬å†…å®¹
            has_content = (
                final_result.get("text") or 
                final_result.get("answer") or 
                final_result.get("content")
            )
            if has_content:
                relevant_results.append(final_result)
        
        # 2. ä» all_results ä¸­é€‰æ‹©å…¶ä»–é«˜ç›¸å…³ç»“æœï¼ˆåŒæºç±»å‹ä¼˜å…ˆï¼‰
        final_source_type = final_result.get("source", "")
        
        # ä¼˜å…ˆé€‰æ‹©åŒæºç±»å‹çš„ç»“æœ
        same_source_results = []
        other_results = []
        
        for result in all_results:
            # è·³è¿‡ final_resultï¼ˆå·²æ·»åŠ ï¼‰
            if result is final_result:
                continue
            
            # è·³è¿‡è”ç½‘æœç´¢ç»“æœï¼ˆå¦‚æœä¸»ç»“æœä¸æ˜¯è”ç½‘çš„ï¼‰
            if result.get("source") in ["web", "tavily_answer", "tavily_web"]:
                continue
            
            # è·³è¿‡ä½ç›¸ä¼¼åº¦ç»“æœ
            similarity = result.get("similarity", 0)
            if similarity < 0.6:  # æé«˜é˜ˆå€¼åˆ°60%
                continue
            
            # è·³è¿‡æ²¡æœ‰å†…å®¹çš„ç»“æœ
            has_content = (
                result.get("text") or 
                result.get("answer") or 
                result.get("content")
            )
            if not has_content:
                continue
            
            # åŒºåˆ†åŒæºå’Œå¼‚æº
            if result.get("source") == final_source_type:
                same_source_results.append(result)
            else:
                other_results.append(result)
        
        # ä¼˜å…ˆæ·»åŠ åŒæºç»“æœï¼Œå†æ·»åŠ å¼‚æºç»“æœ
        for result in same_source_results:
            relevant_results.append(result)
            if len(relevant_results) >= 3:
                break
        
        if len(relevant_results) < 3:
            for result in other_results:
                relevant_results.append(result)
                if len(relevant_results) >= 3:
                    break
        
        # 3. æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆä¿è¯æœ€ç›¸å…³çš„åœ¨å‰é¢ï¼‰
        relevant_results = sorted(
            relevant_results, 
            key=lambda x: x.get("similarity", 0), 
            reverse=True
        )[:3]
        
        logger.info(f"ğŸ“š ä¸ºå›ç­”é€‰æ‹©äº† {len(relevant_results)} ä¸ªå¼•ç”¨æ¥æº")
        for idx, res in enumerate(relevant_results, 1):
            logger.info(f"  [{idx}] {res.get('source')} - ç›¸ä¼¼åº¦: {res.get('similarity', 0):.2%}")
        
        return relevant_results
    
    def _generate_unified_citations(
        self,
        results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„citationsï¼ˆä»¿OpenAIæ ¼å¼ï¼‰
        ä¸accurate_priorityç­–ç•¥ä½¿ç”¨ç›¸åŒçš„æ ¼å¼
        """
        citations = []
        
        for idx, result in enumerate(results, 1):  # ä½¿ç”¨ä¼ å…¥çš„resultsï¼Œä¸å†é™åˆ¶[:3]
            source_type = result.get("source", "")
            
            if source_type == "kb":
                # çŸ¥è¯†åº“æ¥æº
                text = result.get("text", "")
                doc_title = text[:30] + "..." if len(text) > 30 else text
                
                citation = {
                    "id": idx,
                    "type": "kb",
                    "label": "KB",
                    "title": doc_title,
                    "source_name": result.get("kb_name", "çŸ¥è¯†åº“"),
                    "date": None,
                    "snippet": text,
                    "url": result.get("url"),
                    "_internal_score": result.get("similarity", 0)
                }
            elif source_type == "fixed_qa":
                # å›ºå®šQ&Aæ¥æº
                citation = {
                    "id": idx,
                    "type": "qa",
                    "label": "KB",
                    "title": result.get("question", "")[:30],
                    "source_name": "å›ºå®šQ&A",
                    "date": None,
                    "snippet": result.get("answer", ""),
                    "url": None,
                    "_internal_score": result.get("similarity", 0)
                }
            elif source_type in ["web", "tavily_answer", "tavily_web"]:
                # è”ç½‘æœç´¢æ¥æº
                if source_type == "tavily_answer":
                    citation = {
                        "id": idx,
                        "type": "web",
                        "label": "Web",
                        "title": "AIç»¼åˆç­”æ¡ˆ",
                        "source_name": "ç½‘ç»œæœç´¢",
                        "date": None,
                        "snippet": result.get("answer", result.get("content", "")),
                        "url": None,
                        "_internal_score": result.get("relevance", result.get("similarity", 0))
                    }
                else:
                    url = result.get("url", "")
                    domain = url.split("//")[-1].split("/")[0] if url else "ç½‘é¡µ"
                    citation = {
                        "id": idx,
                        "type": "web",
                        "label": "Web",
                        "title": result.get("title", "ç½‘é¡µ")[:40],
                        "source_name": domain,
                        "date": None,
                        "snippet": result.get("content", ""),
                        "url": url,
                        "_internal_score": result.get("relevance", result.get("similarity", 0))
                    }
            else:
                # å…¶ä»–æ¥æº
                citation = {
                    "id": idx,
                    "type": "other",
                    "label": "æ¥æº",
                    "title": "å…¶ä»–æ¥æº",
                    "source_name": source_type,
                    "date": None,
                    "snippet": result.get("text", result.get("content", "")),
                    "url": None,
                    "_internal_score": result.get("similarity", 0)
                }
            
            citations.append(citation)
        
        return citations
    
    def _extract_references(
        self,
        results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æå–å¼•ç”¨ä¿¡æ¯ï¼Œæä¾›æ¸…æ™°çš„æ¥æºæ ‡æ³¨"""
        if not app_config.get("enable_citation", True):
            return []
        
        references = []
        
        # æŒ‰æ¥æºç±»å‹åˆ†ç»„ï¼Œç¡®ä¿æ¯ç§ç±»å‹éƒ½æœ‰ä»£è¡¨
        sources_by_type = {
            "fixed_qa": [],
            "kb": [],
            "web": []  # åŒ…æ‹¬ tavily_answer, tavily_web, web
        }
        
        for result in results:
            source_type = result.get("source")
            if source_type == "fixed_qa":
                sources_by_type["fixed_qa"].append(result)
            elif source_type == "kb":
                sources_by_type["kb"].append(result)
            elif source_type in ["web", "tavily_answer", "tavily_web"]:
                sources_by_type["web"].append(result)
        
        # ä»æ¯ç§ç±»å‹ä¸­å–å‰2ä¸ªï¼Œç¡®ä¿è”ç½‘æœç´¢ç»“æœä¹Ÿè¢«åŒ…å«
        selected_results = []
        for source_type in ["fixed_qa", "kb", "web"]:
            sources = sources_by_type[source_type]
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            sources.sort(key=lambda x: x.get("similarity", 0), reverse=True)
            selected_results.extend(sources[:2])  # æ¯ç§ç±»å‹æœ€å¤š2ä¸ª
        
        # å¤„ç†é€‰ä¸­çš„ç»“æœ
        for result in selected_results:
            source_type = result.get("source")
            
            # æ ‡å‡†åŒ–æ¥æºç±»å‹æ˜¾ç¤ºåç§°
            source_display_map = {
                "fixed_qa": "å›ºå®šQ&A",
                "kb": "çŸ¥è¯†åº“",
                "web": "è”ç½‘æœç´¢",
                "llm": "AIæ¨ç†",
                "fallback": "å›é€€æœºåˆ¶"
            }
            
            ref = {
                "source_type": source_type,
                "source_display": source_display_map.get(source_type, source_type),
                "similarity": round(result.get("similarity", 0), 4),
                "weighted_score": round(result.get("weighted_score", 0), 4),
                "confidence_level": self._get_confidence_level(result.get("similarity", 0))
            }
            
            if source_type == "fixed_qa":
                ref.update({
                    "id": result.get("id"),
                    "question": result.get("question"),
                    "category": result.get("category"),
                    "text": result.get("answer", ""),
                    "source": "fixed_qa",
                    "source_detail": f"å›ºå®šQ&A - {result.get('category', 'é»˜è®¤')}ç±»åˆ«"
                })
            elif source_type == "kb":
                kb_name = result.get("kb_name", "æœªçŸ¥çŸ¥è¯†åº“")
                ref.update({
                    "kb_id": result.get("kb_id"),
                    "kb_name": kb_name,
                    "document_id": result.get("document_id"),
                    "text": result.get("text", ""),
                    "text_snippet": result.get("text", "")[:200] + ("..." if len(result.get("text", "")) > 200 else ""),
                    "source": "kb",
                    "source_detail": f"çŸ¥è¯†åº“ã€Œ{kb_name}ã€"
                })
            elif source_type == "web" or source_type == "tavily_answer" or source_type == "tavily_web":
                # å¤„ç†è”ç½‘æœç´¢ç»“æœï¼ˆåŒ…æ‹¬Tavilyï¼‰
                ref.update({
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "source": source_type,  # ä¿ç•™åŸå§‹sourceç±»å‹
                    "content": result.get("content", ""),
                    "source_detail": "Cbit AI æœç´¢" if "tavily" in source_type else f"äº’è”ç½‘æœç´¢ - {result.get('search_engine', 'Web')}"
                })
            
            references.append(ref)
        
        return references
    
    def _get_confidence_level(self, similarity: float) -> str:
        """æ ¹æ®ç›¸ä¼¼åº¦è¿”å›ç½®ä¿¡åº¦ç­‰çº§"""
        if similarity >= 0.90:
            return "æé«˜"
        elif similarity >= 0.80:
            return "é«˜"
        elif similarity >= 0.70:
            return "ä¸­ç­‰"
        elif similarity >= 0.60:
            return "è¾ƒä½"
        else:
            return "ä½"
    
    def _generate_suggestions(
        self,
        results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆå»ºè®®ï¼ˆå½“ç›¸ä¼¼åº¦åœ¨ä½é«˜é˜ˆå€¼ä¹‹é—´æ—¶ï¼‰"""
        # ä½¿ç”¨æ–°çš„èåˆç­–ç•¥é…ç½®
        fusion_config = app_config.get("fusion_config", {})
        strategy_config = fusion_config.get("strategy", {})
        
        # è·å–é˜ˆå€¼ï¼ˆä¼˜å…ˆæ–°é…ç½®ï¼Œå…¼å®¹æ—§é…ç½®ï¼‰
        qa_suggest_threshold = strategy_config.get("qa_suggest_threshold", 0.55)  # æ›´å®½æ¾çš„å»ºè®®é˜ˆå€¼
        qa_direct_threshold = strategy_config.get("qa_direct_threshold", 0.90)
        kb_context_threshold = strategy_config.get("kb_context_threshold", 0.60)
        kb_high_confidence_threshold = strategy_config.get("kb_high_confidence_threshold", 0.85)
        
        # å…¼å®¹æ—§é…ç½®
        if not strategy_config:
            qa_suggest_threshold = app_config.get("similarity_threshold_low", 0.75)
            qa_direct_threshold = app_config.get("similarity_threshold_high", 0.90)
            kb_context_threshold = qa_suggest_threshold
            kb_high_confidence_threshold = qa_direct_threshold
        
        suggestions = []
        
        # æ‰¾åˆ°ç›¸ä¼¼åº¦åœ¨å»ºè®®åŒºé—´çš„ç»“æœ
        for result in results:
            similarity = result.get("similarity", 0)
            source = result.get("source")
            
            # å›ºå®šQ&Aï¼šåœ¨å»ºè®®é˜ˆå€¼å’Œç›´æ¥å›ç­”é˜ˆå€¼ä¹‹é—´
            if source == "fixed_qa" and qa_suggest_threshold <= similarity < qa_direct_threshold:
                suggestions.append(f"æ‚¨æ˜¯å¦æƒ³é—®ï¼š{result.get('question', '')}")
            # çŸ¥è¯†åº“ï¼šåœ¨ä¸Šä¸‹æ–‡é˜ˆå€¼å’Œé«˜ç½®ä¿¡åº¦é˜ˆå€¼ä¹‹é—´
            elif source == "kb" and kb_context_threshold <= similarity < kb_high_confidence_threshold:
                suggestions.append(f"ç›¸å…³å†…å®¹ï¼š{result.get('text', '')[:100]}...")
        
        return suggestions[:3]  # æœ€å¤šè¿”å›3æ¡å»ºè®®
    
    async def _search_web(
        self,
        query: str,
        app_config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        è”ç½‘æœç´¢åŠŸèƒ½
        æ”¯æŒTavily AI Searchã€Googleã€Serperç­‰æœç´¢å¼•æ“
        """
        try:
            search_channels = app_config.get("search_channels", [])
            
            if not search_channels:
                logger.info("â„¹ï¸ æœªé…ç½®æœç´¢æ¸ é“ï¼Œè·³è¿‡è”ç½‘æœç´¢")
                return []
            
            logger.info(f"ğŸŒ è”ç½‘æœç´¢å·²é…ç½®æ¸ é“: {search_channels}")
            
            # ä»æ•°æ®åº“åŠ è½½æœç´¢æä¾›å•†é…ç½®
            from app.models.database import get_db, SearchProvider
            db = next(get_db())
            
            # å°è¯•ä½¿ç”¨é…ç½®çš„æœç´¢æä¾›å•†
            results = []
            for channel in search_channels:
                # æŸ¥æ‰¾å¯¹åº”çš„æœç´¢æä¾›å•†
                provider = db.query(SearchProvider).filter(
                    SearchProvider.provider_type == channel,
                    SearchProvider.status == "active"
                ).first()
                
                if not provider:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¿€æ´»çš„ {channel} æœç´¢æä¾›å•†")
                    continue
                
                # æ ¹æ®æä¾›å•†ç±»å‹è°ƒç”¨ç›¸åº”çš„æœç´¢
                if channel == "tavily":
                    results = await self._search_with_tavily(query, provider, app_config)
                    if results:
                        logger.info(f"âœ… Tavilyæœç´¢è¿”å› {len(results)} æ¡ç»“æœ")
                        break  # æˆåŠŸè·å–ç»“æœï¼Œé€€å‡ºå¾ªç¯
                
                elif channel == "serper":
                    logger.info("âš ï¸ Serperæœç´¢é›†æˆå¼€å‘ä¸­...")
                    # TODO: å®ç°Serperæœç´¢
                
                elif channel == "google":
                    logger.info("âš ï¸ Googleæœç´¢é›†æˆå¼€å‘ä¸­...")
                    # TODO: å®ç°Googleæœç´¢
                
                elif channel == "serpapi":
                    logger.info("âš ï¸ SerpAPIæœç´¢é›†æˆå¼€å‘ä¸­...")
                    # TODO: å®ç°SerpAPIæœç´¢
            
            if not results:
                logger.warning("âš ï¸ æ‰€æœ‰æœç´¢æ¸ é“å‡æœªè¿”å›ç»“æœ")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ è”ç½‘æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # è¿”å›ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„ç‰¹æ®Šç»“æœï¼Œä»¥ä¾¿å‰ç«¯èƒ½æ˜¾ç¤º
            return [{
                "source": "search_error",
                "error": str(e),
                "content": f"è”ç½‘æœç´¢å¤±è´¥: {str(e)}"
            }]
    
    async def _search_with_tavily(
        self,
        query: str,
        provider: Any,
        app_config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Tavilyæ‰§è¡Œæœç´¢"""
        try:
            from app.core.tavily_search import TavilySearch
            
            # ğŸ¯ ä¼˜åŒ–æœç´¢é…ç½®ï¼Œè·å–æ›´å¤šç»“æœ
            max_results = max(app_config.get("top_k", 5), 8)  # è‡³å°‘8æ¡ç»“æœ
            search_depth = provider.config.get("search_depth", "basic") if provider.config else "basic"
            
            logger.info(f"ğŸ” Tavilyæœç´¢å‚æ•°: query='{query}', max_results={max_results}, depth={search_depth}")
            
            # åˆ›å»ºTavilyå®¢æˆ·ç«¯
            tavily = TavilySearch(provider.api_key)
            
            # æ‰§è¡Œæœç´¢
            search_results = await tavily.search(
                query=query,
                max_results=max_results,
                search_depth=search_depth,
                include_answer=True,
                include_raw_content=False  # ä¸éœ€è¦å®Œæ•´ç½‘é¡µå†…å®¹ï¼ŒåŠ å¿«é€Ÿåº¦
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = tavily.format_results_for_rag(search_results)
            
            if formatted_results:
                logger.info(f"âœ… TavilyæˆåŠŸè¿”å› {len(formatted_results)} æ¡æ ¼å¼åŒ–ç»“æœ")
                # è®°å½•æ‰€æœ‰ç»“æœçš„æ ‡é¢˜ã€æ¥æºå’Œç›¸å…³åº¦
                for idx, result in enumerate(formatted_results, 1):
                    source_type = result.get('source', 'unknown')
                    title = result.get('title', 'N/A')[:60]
                    relevance = result.get('relevance', result.get('similarity', 0))
                    logger.info(f"  [{idx}] ({source_type}) {title} (ç›¸å…³åº¦: {relevance:.2%})")
            else:
                logger.warning("âš ï¸ Tavilyæœç´¢æœªè¿”å›ä»»ä½•æ ¼å¼åŒ–ç»“æœ")
            
            # æ›´æ–°ä½¿ç”¨é‡
            from datetime import date
            if provider.last_reset_date != date.today():
                provider.current_usage = 0
                provider.last_reset_date = date.today()
            
            provider.current_usage += 1
            
            from app.models.database import get_db
            db = next(get_db())
            db.commit()
            
            return formatted_results
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Tavilyæœç´¢å¤±è´¥: {error_msg}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # è¿”å›é”™è¯¯ä¿¡æ¯ä»¥ä¾¿å‰ç«¯æ˜¾ç¤º
            return [{
                "source": "tavily_error",
                "error": error_msg,
                "content": f"Tavilyæœç´¢å¤±è´¥: {error_msg}",
                "title": "æœç´¢å¤±è´¥",
                "url": "",
                "relevance": 0,
                "similarity": 0
            }]


# å…¨å±€å®ä¾‹
hybrid_retrieval_engine = HybridRetrievalEngine()
