"""
æ–‡æ¡£å¤„ç†æ¨¡å— - æ”¯æŒå¤šç§æ ¼å¼æ–‡æ¡£çš„è§£æå’Œæ¸…æ´—
"""

from pathlib import Path
from typing import List, Dict, Any
import fitz  # PyMuPDF
from docx import Document as DocxDocument
import openpyxl
from loguru import logger
import re


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    @staticmethod
    def process_pdf(file_path: Path) -> str:
        """å¤„ç† PDF æ–‡ä»¶"""
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return DocumentProcessor.clean_text(text)
        except Exception as e:
            logger.error(f"PDF å¤„ç†å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def process_docx(file_path: Path, preserve_structure: bool = True) -> str:
        """
        å¤„ç† Word æ–‡æ¡£
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            preserve_structure: æ˜¯å¦ä¿ç•™æ®µè½ç»“æ„ï¼ˆé»˜è®¤Trueï¼Œé€‚åˆFAQç­‰ç»“æ„åŒ–æ–‡æ¡£ï¼‰
        """
        try:
            doc = DocxDocument(file_path)
            # ä¿ç•™æ®µè½ç»“æ„ï¼Œæ¯ä¸ªæ®µè½ç”¨åŒæ¢è¡Œåˆ†éš”
            text = "\n\n".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])
            return DocumentProcessor.clean_text(text, preserve_structure=preserve_structure)
        except Exception as e:
            logger.error(f"DOCX å¤„ç†å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def process_excel(file_path: Path) -> str:
        """å¤„ç† Excel æ–‡ä»¶"""
        try:
            wb = openpyxl.load_workbook(file_path, read_only=True)
            text = ""
            for sheet in wb.worksheets:
                for row in sheet.iter_rows(values_only=True):
                    row_text = " | ".join([str(cell) if cell else "" for cell in row])
                    text += row_text + "\n"
            wb.close()
            return DocumentProcessor.clean_text(text)
        except Exception as e:
            logger.error(f"Excel å¤„ç†å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def process_txt(file_path: Path) -> str:
        """å¤„ç†çº¯æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
            return DocumentProcessor.clean_text(text)
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(file_path, "r", encoding="gbk") as f:
                    text = f.read()
                return DocumentProcessor.clean_text(text)
            except Exception as e:
                logger.error(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
                raise
    
    @staticmethod
    def process_markdown(file_path: Path) -> str:
        """å¤„ç† Markdown æ–‡ä»¶"""
        return DocumentProcessor.process_txt(file_path)
    
    @staticmethod
    def clean_text(text: str, preserve_structure: bool = False) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            preserve_structure: æ˜¯å¦ä¿ç•™æ®µè½ç»“æ„ï¼ˆç”¨äºFAQç­‰ç»“æ„åŒ–æ–‡æ¡£ï¼‰
        """
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        if preserve_structure:
            # ä¿ç•™æ®µè½ç»“æ„ï¼Œåªè§„èŒƒåŒ–æ¢è¡Œ
            text = re.sub(r'\n{3,}', '\n\n', text)  # å¤šä¸ªæ¢è¡Œå˜æˆä¸¤ä¸ª
            text = re.sub(r'[ \t]+', ' ', text)  # åªæ¸…ç†è¡Œå†…ç©ºæ ¼
        else:
            # æ ‡å‡†æ¸…æ´—
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    @staticmethod
    def chunk_text_faq(text: str) -> List[str]:
        """
        FAQæ¨¡å¼æ™ºèƒ½åˆ†å— - è¯†åˆ«é—®ç­”å¯¹ç»“æ„
        
        æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. ç¼–å·æ ¼å¼ï¼š"æ•°å­—) é—®é¢˜ï¼Ÿ" æˆ– "æ•°å­—ï¼‰ é—®é¢˜ï¼Ÿ"
        2. æ®µè½æ ¼å¼ï¼šä»¥"ï¼Ÿ"ç»“å°¾çš„æ®µè½ä½œä¸ºé—®é¢˜ï¼Œåç»­æ®µè½ä½œä¸ºç­”æ¡ˆ
        """
        chunks = []
        
        # æ–¹æ³•1ï¼šå°è¯•ç¼–å·æ ¼å¼
        qa_pattern = r'(?:^|\n)(\d+[)ï¼‰])\s*(.+?)(?=\n\d+[)ï¼‰]|\Z)'
        matches = list(re.finditer(qa_pattern, text, re.DOTALL | re.MULTILINE))
        
        if matches and len(matches) >= 5:  # è‡³å°‘5ä¸ªç¼–å·é—®é¢˜æ‰è®¤ä¸ºæ˜¯ç¼–å·æ ¼å¼
            logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ç¼–å·FAQæ ¼å¼ï¼Œè¯†åˆ«åˆ° {len(matches)} ä¸ªé—®ç­”å¯¹")
            
            for match in matches:
                number = match.group(1)
                content = match.group(2).strip()
                lines = content.split('\n', 1)
                question = lines[0].strip()
                answer = lines[1].strip() if len(lines) > 1 else ""
                
                qa_text = f"Q{number} {question}\n\nA: {answer}" if answer else f"Q{number} {question}"
                chunks.append(qa_text)
                logger.debug(f"  æå–QAå¯¹ {number}: {question[:50]}...")
        else:
            # æ–¹æ³•2ï¼šæ®µè½æ ¼å¼ - è¯†åˆ«é—®å¥å’Œç­”æ¡ˆ
            paragraphs = text.split('\n\n')
            i = 0
            qa_count = 0
            
            while i < len(paragraphs):
                para = paragraphs[i].strip()
                
                # è·³è¿‡æ ‡é¢˜æ®µè½ï¼ˆé€šå¸¸å¾ˆçŸ­ï¼Œæˆ–åŒ…å«"ä¸€ã€"ç­‰æ ‡è®°ï¼‰
                if len(para) < 10 or re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]', para) or para.endswith('ç›¸å…³'):
                    i += 1
                    continue
                
                # è¯†åˆ«é—®å¥ï¼ˆä»¥ï¼Ÿç»“å°¾ï¼Œä¸”é•¿åº¦é€‚ä¸­ï¼‰
                if 'ï¼Ÿ' in para and 10 < len(para) < 200:
                    question = para.strip()
                    answer_parts = []
                    
                    # æ”¶é›†ç­”æ¡ˆæ®µè½ï¼ˆç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜æˆ–æ ‡é¢˜ï¼‰
                    j = i + 1
                    while j < len(paragraphs):
                        next_para = paragraphs[j].strip()
                        
                        # é‡åˆ°æ–°é—®é¢˜æˆ–æ ‡é¢˜åˆ™åœæ­¢
                        if ('ï¼Ÿ' in next_para and len(next_para) < 200) or \
                           re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]', next_para) or \
                           next_para.endswith('ç›¸å…³'):
                            break
                        
                        if next_para:
                            answer_parts.append(next_para)
                        j += 1
                    
                    # æ„å»ºQAå¯¹
                    answer = '\n\n'.join(answer_parts) if answer_parts else ""
                    qa_count += 1
                    qa_text = f"Q{qa_count}: {question}\n\nA: {answer}" if answer else f"Q{qa_count}: {question}"
                    chunks.append(qa_text)
                    
                    logger.debug(f"  æå–QAå¯¹ {qa_count}: {question[:50]}...")
                    i = j
                else:
                    i += 1
            
            if qa_count > 0:
                logger.info(f"ğŸ¯ æ£€æµ‹åˆ°æ®µè½FAQæ ¼å¼ï¼Œè¯†åˆ«åˆ° {qa_count} ä¸ªé—®ç­”å¯¹")
            else:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°FAQæ ¼å¼ï¼Œå°†ä½¿ç”¨æ ‡å‡†åˆ†å—")
                return []
        
        return chunks
    
    @staticmethod
    def chunk_text(text: str, chunk_size: int = 512, overlap: int = 50, mode: str = "auto") -> List[str]:
        """
        æ™ºèƒ½åˆ†å—æ–‡æœ¬
        
        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆè¯æ•°ï¼‰
            overlap: é‡å å¤§å°
            mode: åˆ†å—æ¨¡å¼
                - "auto": è‡ªåŠ¨æ£€æµ‹ï¼ˆä¼˜å…ˆFAQï¼Œç„¶åæ ‡å‡†ï¼‰
                - "faq": å¼ºåˆ¶ä½¿ç”¨FAQæ¨¡å¼
                - "standard": æ ‡å‡†æ®µè½åˆ†å—
        """
        # æ¨¡å¼1ï¼šFAQæ¨¡å¼ï¼ˆä¼˜å…ˆï¼‰
        if mode in ["auto", "faq"]:
            faq_chunks = DocumentProcessor.chunk_text_faq(text)
            if faq_chunks:
                logger.info(f"âœ… ä½¿ç”¨FAQæ¨¡å¼ï¼Œç”Ÿæˆ {len(faq_chunks)} ä¸ªé—®ç­”å¯¹")
                return faq_chunks
            elif mode == "faq":
                logger.warning("âš ï¸ å¼ºåˆ¶FAQæ¨¡å¼ä½†æœªæ£€æµ‹åˆ°æ ¼å¼ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å¼")
        
        # æ¨¡å¼2ï¼šæ ‡å‡†æ®µè½åˆ†å—
        # å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n+', text)
        chunks = []
        current_chunk = ""
        current_words = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            para_words = len(para.split())
            
            # å¦‚æœå½“å‰æ®µè½å¤ªå¤§ï¼Œå•ç‹¬åˆ†å—
            if para_words > chunk_size * 1.5:
                # å…ˆä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_words = 0
                
                # å¤§æ®µè½æŒ‰å¥å­åˆ†å—
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', para)
                temp_chunk = ""
                temp_words = 0
                
                for sent in sentences:
                    sent = sent.strip()
                    if not sent:
                        continue
                    sent_words = len(sent.split())
                    
                    if temp_words + sent_words > chunk_size:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = sent + "ã€‚"
                        temp_words = sent_words
                    else:
                        temp_chunk += sent + "ã€‚"
                        temp_words += sent_words
                
                if temp_chunk:
                    chunks.append(temp_chunk.strip())
            
            # å¦‚æœåŠ ä¸Šè¿™ä¸ªæ®µè½ä¼šè¶…è¿‡chunk_sizeï¼Œå…ˆä¿å­˜å½“å‰chunk
            elif current_words + para_words > chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para
                current_words = para_words
            else:
                current_chunk += ("\n\n" if current_chunk else "") + para
                current_words += para_words
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # ç¡®ä¿æ¯ä¸ªchunkä¸ä¼šå¤ªå°
        final_chunks = []
        for chunk in chunks:
            if len(chunk.split()) >= 50 or not final_chunks:  # è‡³å°‘50è¯
                final_chunks.append(chunk)
            else:
                # åˆå¹¶åˆ°ä¸Šä¸€ä¸ªchunk
                final_chunks[-1] += "\n\n" + chunk
        
        return final_chunks
    
    @classmethod
    def process_document(cls, file_path: Path, chunk_mode: str = "auto") -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            chunk_mode: åˆ†å—æ¨¡å¼ ("auto", "faq", "standard")
        
        Returns:
            {
                "text": str,  # æ¸…æ´—åçš„å®Œæ•´æ–‡æœ¬
                "chunks": List[str],  # åˆ†å—åçš„æ–‡æœ¬
                "metadata": dict  # å…ƒæ•°æ®
                "chunk_mode": str  # å®é™…ä½¿ç”¨çš„åˆ†å—æ¨¡å¼
            }
        """
        file_ext = file_path.suffix.lower()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ä¸åŒçš„å¤„ç†å™¨
        processors = {
            ".pdf": cls.process_pdf,
            ".docx": cls.process_docx,
            ".doc": cls.process_docx,
            ".xlsx": cls.process_excel,
            ".xls": cls.process_excel,
            ".txt": cls.process_txt,
            ".md": cls.process_markdown,
            ".markdown": cls.process_markdown,
        }
        
        processor = processors.get(file_ext)
        if not processor:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        
        # æå–æ–‡æœ¬
        text = processor(file_path)
        
        logger.info(f"ğŸ“„ æ–‡æ¡£æå–å®Œæˆ: {file_path.name}, æ€»å­—ç¬¦æ•°: {len(text)}")
        logger.info(f"ğŸ”§ ä½¿ç”¨åˆ†å—æ¨¡å¼: {chunk_mode}")
        
        # æ™ºèƒ½åˆ†å—
        from app.core.config import settings
        chunks = cls.chunk_text(text, settings.CHUNK_SIZE, settings.CHUNK_OVERLAP, mode=chunk_mode)
        
        # ç¡®å®šå®é™…ä½¿ç”¨çš„æ¨¡å¼
        actual_mode = "faq" if (chunk_mode in ["auto", "faq"] and chunks and chunks[0].startswith("Q")) else "standard"
        
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: åˆ†å—æ•°={len(chunks)}, å®é™…æ¨¡å¼={actual_mode}")
        
        return {
            "text": text,
            "chunks": chunks,
            "metadata": {
                "filename": file_path.name,
                "file_type": file_ext,
                "char_count": len(text),
                "chunk_count": len(chunks),
                "chunk_mode": actual_mode,
            }
        }

