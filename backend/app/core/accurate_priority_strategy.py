"""
å‡†ç¡®ä¼˜å…ˆç­–ç•¥
æ ¸å¿ƒç†å¿µï¼šå‡†ç¡® > å…¨é¢ï¼Œå®å¯ä¸ç­”ï¼Œä¸å¯ä¹±ç­”
"""

from typing import List, Dict, Any, Optional, Tuple
from loguru import logger
from datetime import datetime
from collections import defaultdict


class AccuratePriorityStrategy:
    """å‡†ç¡®ä¼˜å…ˆç­–ç•¥å®ç°"""
    
    # é»˜è®¤ä¸‰æ¡£é˜ˆå€¼ï¼ˆå¦‚æœé…ç½®ä¸­æ²¡æœ‰åˆ™ä½¿ç”¨è¿™äº›é»˜è®¤å€¼ï¼‰
    DEFAULT_HIGH_CONFIDENCE_THRESHOLD = 0.82  # Aæ¡£ï¼šå¼ºç½®ä¿¡
    DEFAULT_MEDIUM_CONFIDENCE_THRESHOLD = 0.70  # Bæ¡£ï¼šä¸­ç­‰ç½®ä¿¡
    # Cæ¡£ï¼š< 0.70 ä½ç½®ä¿¡ï¼Œæ”¾å¼ƒä½œç­”
    
    # è”ç½‘è¦†ç›–KBçš„æ¡ä»¶
    WEB_CONSENSUS_THRESHOLD = 0.88  # å¤šæºå…±è¯†åº¦
    WEB_ADVANTAGE_THRESHOLD = 0.15  # å¿…é¡»æ˜¾è‘—ä¼˜äºKB
    
    def __init__(self):
        pass
    
    async def apply_strategy(
        self,
        results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        åº”ç”¨å‡†ç¡®ä¼˜å…ˆç­–ç•¥
        
        Returns:
            {
                "tier": "A" | "B" | "C",
                "final_result": Dict,
                "confidence_level": "é«˜" | "ä¸­" | "ä½",
                "citations": List[Dict],  # ç¼–å·å¼•ç”¨
                "explanation": str,  # ä¸ºä½•è¿™æ ·å›ç­”
                "web_search_option": bool,  # æ˜¯å¦æ˜¾ç¤ºè”ç½‘é€‰é¡¹
                "custom_message": str  # Cæ¡£çš„è‡ªå®šä¹‰æç¤º
            }
        """
        if not results:
            return self._create_c_tier_response(app_config, "æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯")
        
        # ğŸ”‘ ä»é…ç½®ä¸­è¯»å–é˜ˆå€¼ï¼ˆå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        strategy_config = app_config.get("fusion_config", {}).get("strategy", {})
        high_threshold = strategy_config.get("kb_high_confidence_threshold", self.DEFAULT_HIGH_CONFIDENCE_THRESHOLD)
        medium_threshold = strategy_config.get("kb_context_threshold", self.DEFAULT_MEDIUM_CONFIDENCE_THRESHOLD)
        
        logger.info(f"ğŸ“Š ä½¿ç”¨é˜ˆå€¼é…ç½®: Aæ¡£â‰¥{high_threshold:.0%}, Bæ¡£â‰¥{medium_threshold:.0%}")
        
        # åˆ†ç¦»ä¸åŒæ¥æº
        kb_results = [r for r in results if r.get("source") == "kb"]
        fixed_qa_results = [r for r in results if r.get("source") == "fixed_qa"]
        web_results = [r for r in results if r.get("source") in ["web", "tavily_answer", "tavily_web"]]
        
        # è·å–æœ€ä½³ç»“æœ
        best_kb = max(kb_results, key=lambda x: x.get("similarity", 0)) if kb_results else None
        best_fixed_qa = max(fixed_qa_results, key=lambda x: x.get("similarity", 0)) if fixed_qa_results else None
        best_web = max(web_results, key=lambda x: x.get("similarity", 0)) if web_results else None
        
        # ä¼˜å…ˆæ£€æŸ¥å›ºå®šQ&Aï¼ˆä½¿ç”¨é…ç½®çš„é˜ˆå€¼ï¼‰
        qa_high_threshold = strategy_config.get("qa_direct_threshold", 0.90)
        if best_fixed_qa and best_fixed_qa.get("similarity", 0) >= qa_high_threshold:
            logger.info(f"ğŸ’ å›ºå®šQ&Aé«˜åŒ¹é… {best_fixed_qa.get('similarity', 0):.1%}ï¼ˆâ‰¥{qa_high_threshold:.0%}ï¼‰ï¼ŒAæ¡£å¤„ç†")
            return self._create_a_tier_response(best_fixed_qa, [best_fixed_qa], "fixed_qa")
        
        # åˆ¤å®šçŸ¥è¯†åº“ç½®ä¿¡åº¦æ¡£æ¬¡
        kb_similarity = best_kb.get("similarity", 0) if best_kb else 0
        
        # Aæ¡£ï¼šå¼ºç½®ä¿¡ï¼ˆä½¿ç”¨é…ç½®çš„é«˜é˜ˆå€¼ï¼‰
        if kb_similarity >= high_threshold:
            logger.info(f"âœ… Aæ¡£ï¼šçŸ¥è¯†åº“å¼ºç½®ä¿¡ {kb_similarity:.1%}ï¼ˆâ‰¥{high_threshold:.0%}ï¼‰")
            return self._create_a_tier_response(best_kb, kb_results[:3], "kb")
        
        # Bæ¡£ï¼šä¸­ç­‰ç½®ä¿¡ï¼ˆä½¿ç”¨é…ç½®çš„ä¸­ç­‰é˜ˆå€¼ï¼‰
        elif kb_similarity >= medium_threshold:
            logger.info(f"âš ï¸ Bæ¡£ï¼šçŸ¥è¯†åº“ä¸­ç­‰ç½®ä¿¡ {kb_similarity:.1%}ï¼ˆ{medium_threshold:.0%}-{high_threshold:.0%}ï¼‰")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è”ç½‘ç»“æœå¯ä»¥è¦†ç›–
            if web_results:
                should_use_web, reason = self._should_use_web_over_kb(
                    best_kb, web_results, app_config
                )
                if should_use_web:
                    logger.info(f"ğŸŒ è”ç½‘è¦†ç›–KB: {reason}")
                    return self._create_b_tier_response_with_web(
                        best_kb, web_results, kb_results, reason
                    )
            
            # é»˜è®¤ï¼šä¿å®ˆå›ç­” + è”ç½‘é€‰é¡¹
            return self._create_b_tier_response(best_kb, kb_results[:3])
        
        # Cæ¡£ï¼šä½ç½®ä¿¡ï¼ˆ< 0.70ï¼‰
        else:
            logger.info(f"ğŸ™‡ Cæ¡£ï¼šçŸ¥è¯†åº“ä½ç½®ä¿¡ {kb_similarity:.1%}ï¼ˆ<{self.MEDIUM_CONFIDENCE_THRESHOLD:.0%}ï¼‰")
            
            # ğŸ”‘ å…³é”®ï¼šæ£€æŸ¥æ˜¯å¦æœ‰è”ç½‘æœç´¢ç»“æœå¯ç”¨
            if web_results:
                best_web = max(web_results, key=lambda x: x.get("relevance", x.get("similarity", 0)))
                web_relevance = best_web.get("relevance", best_web.get("similarity", 0))
                
                # å¦‚æœè”ç½‘ç»“æœç›¸å…³åº¦åˆç†ï¼ˆâ‰¥0.60ï¼‰ï¼Œä½¿ç”¨è”ç½‘ç»“æœ
                if web_relevance >= 0.60:
                    logger.info(f"ğŸŒ Cæ¡£ä½¿ç”¨è”ç½‘ç»“æœ: {best_web.get('title', 'N/A')[:50]} (ç›¸å…³åº¦: {web_relevance:.1%})")
                    citations = self._generate_numbered_citations(web_results[:3])
                    
                    # ç”Ÿæˆå¸¦é“¾æ¥çš„è¯´æ˜ï¼ˆä¸æ˜¾ç¤ºç›¸å…³åº¦ç™¾åˆ†æ¯”ï¼‰
                    explanation = f"çŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ï¼Œå·²ä¸ºæ‚¨ä»ç½‘ç»œæ£€ç´¢åˆ°ç›¸å…³å†…å®¹"
                    
                    # æ·»åŠ ä¸»è¦æ¥æºé“¾æ¥
                    if citations and len(citations) > 0:
                        first_citation = citations[0]
                        url = first_citation.get("url")
                        source_name = first_citation.get("source_name", "")
                        if url:
                            explanation += f" | ä¸»è¦æ¥æº: {source_name} | é“¾æ¥: {url}"
                        elif source_name:
                            explanation += f" | ä¸»è¦æ¥æº: {source_name}"
                    
                    return {
                        "tier": "C",
                        "final_result": best_web,
                        "confidence_level": "ä¸­",  # æœ‰è”ç½‘æ”¯æŒï¼Œæå‡åˆ°"ä¸­"
                        "citations": citations,
                        "explanation": explanation,
                        "web_search_option": False,  # å·²ç»ä½¿ç”¨äº†webä¿¡æ¯
                        "custom_message": "â„¹ï¸ ä»¥ä¸‹ä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢"
                    }
            
            # å¦‚æœæ²¡æœ‰è”ç½‘ç»“æœæˆ–ç›¸å…³åº¦å¤ªä½ï¼Œæ‰æ”¾å¼ƒä½œç­”
            logger.info("âŒ çŸ¥è¯†åº“å’Œè”ç½‘å‡æ— å……åˆ†ä¿¡æ¯ï¼Œæ”¾å¼ƒä½œç­”")
            custom_message = app_config.get("custom_no_result_response", "")
            return self._create_c_tier_response(app_config, custom_message or "æœªæ‰¾åˆ°è¶³å¤Ÿè¯æ®")
    
    def _create_a_tier_response(
        self,
        best_result: Dict[str, Any],
        all_results: List[Dict[str, Any]],
        source_type: str
    ) -> Dict[str, Any]:
        """åˆ›å»ºAæ¡£å“åº”ï¼šä»…ç”¨çŸ¥è¯†åº“ï¼Œä¸è§¦å‘è”ç½‘"""
        citations = self._generate_numbered_citations(all_results[:3])
        
        return {
            "tier": "A",
            "final_result": best_result,
            "confidence_level": "é«˜",
            "citations": citations,
            "explanation": self._generate_explanation(citations, "strong"),
            "web_search_option": False,  # Aæ¡£ä¸æä¾›è”ç½‘é€‰é¡¹
            "custom_message": None
        }
    
    def _create_b_tier_response(
        self,
        best_result: Dict[str, Any],
        all_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆ›å»ºBæ¡£å“åº”ï¼šä¿å®ˆå›ç­” + è”ç½‘é€‰é¡¹"""
        citations = self._generate_numbered_citations(all_results[:3])
        
        return {
            "tier": "B",
            "final_result": best_result,
            "confidence_level": "ä¸­",
            "citations": citations,
            "explanation": self._generate_explanation(citations, "moderate"),
            "web_search_option": True,  # Bæ¡£æä¾›è”ç½‘é€‰é¡¹
            "custom_message": "âš ï¸ ç°æœ‰èµ„æ–™ä¸å¤Ÿå……åˆ†ï¼Œéƒ¨åˆ†ä¿¡æ¯å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ ¸éªŒã€‚"
        }
    
    def _create_b_tier_response_with_web(
        self,
        kb_result: Dict[str, Any],
        web_results: List[Dict[str, Any]],
        kb_results: List[Dict[str, Any]],
        reason: str
    ) -> Dict[str, Any]:
        """åˆ›å»ºBæ¡£å“åº”ï¼ˆå«è”ç½‘ä¿¡æ¯ï¼‰"""
        # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šåªä½¿ç”¨è”ç½‘æœç´¢çš„å¼•ç”¨ï¼ˆä¸æ··å…¥çŸ¥è¯†åº“å¼•ç”¨ï¼‰
        # å› ä¸ºæœ€ç»ˆå›ç­”æ¥è‡ªè”ç½‘æœç´¢ï¼Œå¼•ç”¨åº”è¯¥åªæ˜¾ç¤ºè”ç½‘æ¥æº
        citations = self._generate_numbered_citations(web_results[:3])
        
        # ä½¿ç”¨webç»“æœä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
        best_web = max(web_results, key=lambda x: x.get("relevance", x.get("similarity", 0)))
        
        # ç”Ÿæˆå¸¦é“¾æ¥çš„è¯´æ˜ï¼ˆä¸æ˜¾ç¤ºç›¸å…³åº¦ç™¾åˆ†æ¯”ï¼‰
        explanation = f"çŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ï¼Œå·²ä»ç½‘ç»œæ£€ç´¢åˆ°ç›¸å…³å†…å®¹"
        
        # æ·»åŠ ä¸»è¦æ¥æºé“¾æ¥
        if citations and len(citations) > 0:
            first_citation = citations[0]
            url = first_citation.get("url")
            source_name = first_citation.get("source_name", "")
            if url:
                explanation += f" | ä¸»è¦æ¥æº: {source_name} | é“¾æ¥: {url}"
            elif source_name:
                explanation += f" | ä¸»è¦æ¥æº: {source_name}"
        
        return {
            "tier": "B",
            "final_result": best_web,
            "confidence_level": "ä¸­",
            "citations": citations,
            "explanation": explanation,
            "web_search_option": False,  # å·²ç»ä½¿ç”¨äº†webä¿¡æ¯
            "custom_message": "â„¹ï¸ ä»¥ä¸‹ä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢"
        }
    
    def _create_c_tier_response(
        self,
        app_config: Dict[str, Any],
        default_message: str
    ) -> Dict[str, Any]:
        """åˆ›å»ºCæ¡£å“åº”ï¼šä¸ä½œç­” + è‡ªå®šä¹‰æç¤º"""
        custom_msg = app_config.get("custom_no_result_response", default_message)
        
        return {
            "tier": "C",
            "final_result": None,
            "confidence_level": "ä½",
            "citations": [],
            "explanation": "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°è¶³å¤Ÿå……åˆ†çš„ç›¸å…³ä¿¡æ¯ã€‚",
            "web_search_option": True,  # Cæ¡£æä¾›è”ç½‘é€‰é¡¹
            "custom_message": custom_msg or "ğŸ™‡ æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°å…³äºæ­¤é—®é¢˜çš„å……åˆ†ä¿¡æ¯ã€‚\n\næ‚¨å¯ä»¥ï¼š\n- å°è¯•æ¢ä¸ªé—®æ³•\n- æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯\n- è”ç³»ç®¡ç†å‘˜è¡¥å……ç›¸å…³èµ„æ–™"
        }
    
    def _should_use_web_over_kb(
        self,
        kb_result: Dict[str, Any],
        web_results: List[Dict[str, Any]],
        app_config: Dict[str, Any]
    ) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”¨è”ç½‘ä¿¡æ¯è¦†ç›–çŸ¥è¯†åº“
        
        å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
        1. web_consensus_score â‰¥ 0.88
        2. web_score - kb_score â‰¥ 0.15
        3. æœ‰æ—¶é—´æˆ³è¯æ®è¡¨æ˜KBè¿‡æœŸ
        """
        # æ£€æŸ¥å¤šæºä¸€è‡´æ€§
        consensus_score, consensus_facts = self._check_web_consensus(web_results)
        
        if consensus_score < self.WEB_CONSENSUS_THRESHOLD:
            return False, f"ç½‘ç»œå…±è¯†åº¦ä¸è¶³ ({consensus_score:.1%} < {self.WEB_CONSENSUS_THRESHOLD:.0%})"
        
        # æ£€æŸ¥ä¼˜åŠ¿
        best_web = max(web_results, key=lambda x: x.get("similarity", 0))
        web_score = best_web.get("similarity", 0)
        kb_score = kb_result.get("similarity", 0)
        advantage = web_score - kb_score
        
        if advantage < self.WEB_ADVANTAGE_THRESHOLD:
            return False, f"ç½‘ç»œä¼˜åŠ¿ä¸æ˜¾è‘— ({advantage:.1%} < {self.WEB_ADVANTAGE_THRESHOLD:.0%})"
        
        # æ£€æŸ¥æ—¶é—´æˆ³è¯æ®
        has_timestamp_evidence = self._check_timestamp_evidence(kb_result, web_results)
        
        if not has_timestamp_evidence:
            return False, "æ— æ˜ç¡®çš„æ—¶é—´æˆ³è¯æ®è¡¨æ˜çŸ¥è¯†åº“è¿‡æœŸ"
        
        return True, f"ç½‘ç»œå¤šæºä¸€è‡´ ({consensus_score:.1%})ï¼Œæ˜¾è‘—ä¼˜äºçŸ¥è¯†åº“ (+{advantage:.1%})ï¼Œä¸”æœ‰æ–°è¯æ®"
    
    def _check_web_consensus(
        self,
        web_results: List[Dict[str, Any]]
    ) -> Tuple[float, List[str]]:
        """
        æ£€æŸ¥ç½‘ç»œæ¥æºçš„å¤šæºä¸€è‡´æ€§
        
        Returns:
            (consensus_score, consensus_facts)
        """
        if len(web_results) < 2:
            return 0.0, []
        
        # æŒ‰åŸŸååˆ†ç»„
        sources_by_domain = defaultdict(list)
        for result in web_results:
            url = result.get("url", "")
            if url:
                # ç®€å•æå–åŸŸå
                domain = self._extract_domain(url)
                sources_by_domain[domain].append(result)
        
        # è‡³å°‘éœ€è¦2ä¸ªä¸åŒåŸŸå
        unique_domains = len(sources_by_domain)
        if unique_domains < 2:
            logger.info(f"âš ï¸ ç½‘ç»œæ¥æºä¸è¶³ï¼šåªæœ‰ {unique_domains} ä¸ªç‹¬ç«‹åŸŸå")
            return 0.0, []
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦ä½œä¸ºå…±è¯†åˆ†æ•°
        avg_similarity = sum(r.get("similarity", 0) for r in web_results) / len(web_results)
        
        # æ£€æŸ¥ç»“æœçš„ä¸€è‡´æ€§ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨ç›¸ä¼¼åº¦æ ‡å‡†å·®ï¼‰
        similarities = [r.get("similarity", 0) for r in web_results]
        std_dev = self._calculate_std_dev(similarities)
        
        # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
        consistency_factor = max(0, 1 - std_dev * 2)
        
        consensus_score = avg_similarity * consistency_factor
        
        logger.info(f"ğŸ§­ ç½‘ç»œå…±è¯†åˆ†æï¼š{unique_domains}ä¸ªç‹¬ç«‹æºï¼Œå¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.1%}ï¼Œä¸€è‡´æ€§ {consistency_factor:.1%}ï¼Œå…±è¯†åˆ†æ•° {consensus_score:.1%}")
        
        return consensus_score, []
    
    def _check_timestamp_evidence(
        self,
        kb_result: Dict[str, Any],
        web_results: List[Dict[str, Any]]
    ) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æˆ³è¯æ®è¡¨æ˜KBè¿‡æœŸæˆ–å­˜åœ¨çŸ›ç›¾
        
        æ£€æŸ¥ä»¥ä¸‹æƒ…å†µï¼š
        1. è”ç½‘ç»“æœåŒ…å«æ˜ç¡®çš„æ—¶é—´æ ‡è®°ï¼ˆæ—¥æœŸã€æ—¶é—´ç­‰ï¼‰
        2. è”ç½‘ç»“æœä¸­æœ‰æƒå¨æ¥æºï¼ˆå®˜æ–¹ç½‘ç«™ã€æ–°é—»åª’ä½“ç­‰ï¼‰
        3. è”ç½‘ç»“æœçš„æ ‡é¢˜æˆ–å†…å®¹æš—ç¤ºæ˜¯æœ€æ–°æ›´æ–°
        4. Tavily AI ç»¼åˆç­”æ¡ˆï¼ˆé€šå¸¸åŸºäºå¤šä¸ªæœ€æ–°æ¥æºï¼‰
        """
        timestamp_indicators = [
            '2024', '2025',  # å¹´ä»½æ ‡è®°
            'æœ€æ–°', 'æ›´æ–°', 'æ–°ç‰ˆ', 'ç°åœ¨',  # ä¸­æ–‡æ—¶æ•ˆè¯
            'latest', 'update', 'new', 'current', 'recent',  # è‹±æ–‡æ—¶æ•ˆè¯
            'å®˜æ–¹', 'å…¬å‘Š', 'é€šçŸ¥',  # æƒå¨å‘å¸ƒ
            'official', 'announcement', 'notice'
        ]
        
        authoritative_domains = [
            '.gov.', '.edu.',  # æ”¿åºœå’Œæ•™è‚²æœºæ„
            'news', 'press',  # æ–°é—»åª’ä½“
            'blog', 'official'  # å®˜æ–¹åšå®¢
        ]
        
        evidence_score = 0
        reasons = []
        
        for result in web_results:
            # 1. Tavily AI ç»¼åˆç­”æ¡ˆï¼ˆæœ€é«˜æƒé‡ï¼‰
            if result.get("source") == "tavily_answer":
                evidence_score += 3
                reasons.append("Tavily AIç»¼åˆç­”æ¡ˆï¼ˆåŸºäºå¤šæºæœ€æ–°ä¿¡æ¯ï¼‰")
                continue
            
            # 2. æ£€æŸ¥æ ‡é¢˜å’Œå†…å®¹ä¸­çš„æ—¶é—´æ ‡è®°
            title = result.get("title", "").lower()
            content = result.get("content", "").lower()
            combined_text = f"{title} {content}"
            
            for indicator in timestamp_indicators:
                if indicator.lower() in combined_text:
                    evidence_score += 0.5
                    reasons.append(f"åŒ…å«æ—¶æ•ˆæ ‡è®°: {indicator}")
                    break  # æ¯ä¸ªç»“æœåªè®¡åˆ†ä¸€æ¬¡
            
            # 3. æ£€æŸ¥æ˜¯å¦æ¥è‡ªæƒå¨åŸŸå
            url = result.get("url", "").lower()
            for domain in authoritative_domains:
                if domain in url:
                    evidence_score += 1
                    reasons.append(f"æƒå¨æ¥æº: {domain}")
                    break
        
        # åˆ¤å®šé˜ˆå€¼ï¼šè‡³å°‘éœ€è¦ 2 åˆ†
        # - Tavilyç­”æ¡ˆï¼š3åˆ†ï¼ˆç›´æ¥é€šè¿‡ï¼‰
        # - æƒå¨åŸŸå+æ—¶æ•ˆæ ‡è®°ï¼š1.5åˆ†
        # - 2ä¸ªæƒå¨åŸŸåï¼š2åˆ†ï¼ˆé€šè¿‡ï¼‰
        has_evidence = evidence_score >= 2.0
        
        if has_evidence:
            logger.info(f"âœ… å‘ç°æ—¶é—´æˆ³è¯æ®ï¼ˆå¾—åˆ†: {evidence_score:.1f}ï¼‰: {', '.join(reasons[:3])}")
        else:
            logger.info(f"âŒ æ—¶é—´æˆ³è¯æ®ä¸è¶³ï¼ˆå¾—åˆ†: {evidence_score:.1f} < 2.0ï¼‰")
        
        return has_evidence
    
    def _generate_numbered_citations(
        self,
        results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆç¼–å·å¼•ç”¨ï¼ˆä»¿ OpenAI æ ¼å¼ï¼‰
        
        æ ¼å¼:
        [1] KB â€” æ–‡æ¡£é¢˜åï¼ˆç®€ï¼‰ Â· çŸ¥è¯†åº“å Â· YYYY-MM-DD
        [2] Web â€” é¡µé¢æ ‡é¢˜ Â· YYYY-MM-DD
        
        æ³¨æ„ï¼šsimilarity ä»…ç”¨äºå†…éƒ¨åˆ¤å®šï¼Œä¸å‘ç”¨æˆ·å±•ç¤º
        """
        citations = []
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºä¼ å…¥çš„ç»“æœ
        logger.info(f"ğŸ“š ç”Ÿæˆå¼•ç”¨æ¥æºï¼Œå…± {len(results)} ä¸ªç»“æœï¼š")
        for i, r in enumerate(results[:3], 1):
            source = r.get("source", "unknown")
            sim = r.get("similarity", r.get("relevance", 0))
            title = r.get("question", r.get("title", r.get("text", "")))[:50]
            logger.info(f"  [{i}] {source} - ç›¸ä¼¼åº¦: {sim:.2%} - {title}")
        
        for idx, result in enumerate(results[:3], 1):  # æœ€å¤š3æ¡ï¼Œä¼˜å…ˆä¸åŒæ¥æº/æ—¶é—´
            source_type = result.get("source", "")
            
            if source_type == "kb":
                # æˆªå–æ–‡æ¡£æ ‡é¢˜ï¼ˆå–ç¬¬ä¸€å¥æˆ–å‰30å­—ï¼‰
                text = result.get("text", "")
                doc_title = self._extract_title_from_text(text, max_length=30)
                
                citation = {
                    "id": idx,
                    "type": "kb",
                    "label": "KB",  # æ”¹ä¸ºç®€æ´æ ‡ç­¾
                    "title": doc_title,
                    "source_name": result.get("kb_name", "çŸ¥è¯†åº“"),
                    "date": self._extract_date(result),
                    "snippet": result.get("text", ""),  # æ‚¬åœæ˜¾ç¤ºçš„æ‘˜è¦å¥
                    "url": result.get("url"),  # å¦‚æœæœ‰æ–‡æ¡£é“¾æ¥
                    # similarity ä¸ä¼ é€’ç»™å‰ç«¯ï¼Œä»…å†…éƒ¨ä½¿ç”¨
                    "_internal_score": result.get("similarity", 0)
                }
            elif source_type == "fixed_qa":
                citation = {
                    "id": idx,
                    "type": "qa",
                    "label": "KB",  # Q&A ä¹Ÿç®—çŸ¥è¯†åº“æ¥æº
                    "title": self._truncate_text(result.get("question", ""), 30),
                    "source_name": "å›ºå®šQ&A",
                    "date": self._extract_date(result),
                    "snippet": result.get("answer", ""),
                    "url": None,
                    "_internal_score": result.get("similarity", 0)
                }
            else:  # web (åŒ…æ‹¬tavily_answerå’Œtavily_web)
                # ç‰¹æ®Šå¤„ç†ï¼štavily_answer (AIç»¼åˆç­”æ¡ˆ)
                if source_type == "tavily_answer":
                    citation = {
                        "id": idx,
                        "type": "web",
                        "label": "Web",
                        "title": "AIç»¼åˆç­”æ¡ˆ",
                        "source_name": "ç½‘ç»œæœç´¢",
                        "date": self._extract_date(result),
                        "snippet": result.get("answer", result.get("content", "")),
                        "url": None,
                        "_internal_score": result.get("relevance", result.get("similarity", 0))
                    }
                else:
                    citation = {
                        "id": idx,
                        "type": "web",
                        "label": "Web",
                        "title": self._truncate_text(result.get("title", "ç½‘é¡µ"), 40),
                        "source_name": self._extract_domain(result.get("url", "")),
                        "date": self._extract_date(result),
                        "snippet": result.get("content", ""),
                        "url": result.get("url", ""),
                        "_internal_score": result.get("relevance", result.get("similarity", 0))
                    }
            
            citations.append(citation)
        
        return citations
    
    def _extract_title_from_text(self, text: str, max_length: int = 30) -> str:
        """ä»æ–‡æœ¬ä¸­æå–æ ‡é¢˜ï¼ˆå–ç¬¬ä¸€å¥æˆ–å‰Nå­—ï¼‰"""
        if not text:
            return "æ–‡æ¡£"
        
        # å°è¯•æŒ‰å¥å·åˆ†å‰²ï¼Œå–ç¬¬ä¸€å¥
        sentences = text.split('ã€‚')
        if sentences and sentences[0]:
            first_sentence = sentences[0].strip()
            if len(first_sentence) <= max_length:
                return first_sentence
            return first_sentence[:max_length] + "..."
        
        # å¦åˆ™ç›´æ¥æˆªå–
        return self._truncate_text(text, max_length)
    
    def _generate_explanation(
        self,
        citations: List[Dict[str, Any]],
        confidence_type: str
    ) -> str:
        """ç”Ÿæˆ"ä¸ºä½•è¿™æ ·å›ç­”"çš„è§£é‡Š"""
        if not citations:
            return "æœªæ‰¾åˆ°ç›¸å…³å¼•ç”¨æ¥æºã€‚"
        
        citation_ids = [f"[{c['id']}]" for c in citations]
        citation_str = "".join(citation_ids)
        
        if confidence_type == "strong":
            return f"ä¾æ®{citation_str}çš„é«˜è´¨é‡åŒ¹é…å†…å®¹å¾—å‡ºç»“è®ºï¼Œä¿¡æ¯æ¥æºå¯é ä¸”ä¸€è‡´ã€‚"
        elif confidence_type == "moderate":
            return f"ä¾æ®{citation_str}çš„éƒ¨åˆ†è¯æ®ä½œç­”ï¼Œä»…é™ˆè¿°å¯ä»¥ç¡®è®¤çš„ä¿¡æ¯ï¼Œå…¶ä»–éƒ¨åˆ†å› è¯æ®ä¸è¶³æœªä¸‹ç»“è®ºã€‚"
        else:
            return "çŸ¥è¯†åº“ä¸­ç¼ºå°‘å……åˆ†è¯æ®ï¼Œæ— æ³•ç»™å‡ºå¯é ç»“è®ºã€‚"
    
    def _extract_domain(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        if not url:
            return "æœªçŸ¥æ¥æº"
        
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc or parsed.path
            # ç§»é™¤www.
            if domain.startswith("www."):
                domain = domain[4:]
            return domain
        except:
            return url[:30]
    
    def _extract_date(self, result: Dict[str, Any]) -> str:
        """æå–æ—¥æœŸ"""
        # ä¼˜å…ˆä½¿ç”¨ç»“æœä¸­çš„æ—¥æœŸå­—æ®µ
        if "date" in result:
            return result["date"]
        
        # ä»created_atæˆ–updated_atæå–
        for date_field in ["created_at", "updated_at", "timestamp"]:
            if date_field in result:
                try:
                    date_str = str(result[date_field])
                    # ç®€å•å¤„ç†ï¼šæå–YYYY-MM-DDéƒ¨åˆ†
                    if len(date_str) >= 10:
                        return date_str[:10]
                except:
                    pass
        
        # é»˜è®¤ä½¿ç”¨ä»Šå¤©
        return datetime.now().strftime("%Y-%m-%d")
    
    def _truncate_text(self, text: str, max_length: int) -> str:
        """æˆªæ–­æ–‡æœ¬"""
        if not text:
            return ""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."
    
    def _calculate_std_dev(self, numbers: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if not numbers:
            return 0.0
        
        mean = sum(numbers) / len(numbers)
        variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)
        return variance ** 0.5


# å…¨å±€å®ä¾‹
accurate_priority_strategy = AccuratePriorityStrategy()

